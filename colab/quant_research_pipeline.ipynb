{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Market Quantitative Research Pipeline (v2)\n",
    "\n",
    "**Institutional-grade**, multi-market (US + KOSPI + KOSDAQ), cost-aware, overfitting-controlled\n",
    "framework for systematic strategy discovery.\n",
    "\n",
    "## Key Enhancements over v1\n",
    "- **Multi-market**: US large-cap, KOSPI, KOSDAQ with FX-adjusted returns\n",
    "- **Dynamic universe**: point-in-time universe construction per market\n",
    "- **Multi-horizon**: 5-day, 21-day, 63-day forward returns\n",
    "- **Transaction costs**: cost-adjusted forward returns (commission + slippage)\n",
    "- **Turnover control**: strategy turnover estimation and penalty\n",
    "- **Regime analysis**: bull/bear regime stability testing\n",
    "- **Multiple testing correction**: FDR (Benjamini-Hochberg)\n",
    "- **Strengthened overfitting control**: higher thresholds, bootstrap min-samples guard\n",
    "- **Cross-market portfolio**: diversification benefit analysis\n",
    "\n",
    "## Pipeline Steps\n",
    "1. Data Layer — Per-market OHLCV download (yfinance / pykrx)\n",
    "2. Feature Engine — Momentum, volatility, regime + multi-horizon cost-adjusted returns\n",
    "3. Candidate Generator — Decile, decision tree, logistic rank (per market, per horizon)\n",
    "4. Edge Evaluation — Net-of-cost metrics per candidate\n",
    "5. Walk-Forward Validation — Rolling train/test with embargo & turnover estimation\n",
    "6. Overfitting Control — Stability, bootstrap CI, FDR correction\n",
    "7. Turnover & Cost Filtering — Reject high-turnover strategies\n",
    "8. Strategy Scoring — Turnover-penalised composite score + correlation clustering\n",
    "9. Regime Analysis — Bull/bear performance consistency\n",
    "10. Portfolio Combination — Per-market & cross-market equal-weight portfolios\n",
    "\n",
    "**Resume-safe:** every step checkpoints to Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q yfinance pandas numpy scikit-learn scipy matplotlib pyarrow joblib pykrx finance-datareader statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import gc\n",
    "import logging\n",
    "import warnings\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import List, Optional, Dict, Tuple\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PipelineConfig:\n",
    "    \"\"\"Central configuration — 16 parameter groups.\"\"\"\n",
    "\n",
    "    # --- 1. Markets ---\n",
    "    markets: List[str] = field(default_factory=lambda: [\"US\", \"KOSPI\", \"KOSDAQ\"])\n",
    "    base_currency: str = \"USD\"\n",
    "    apply_fx_conversion: bool = True\n",
    "\n",
    "    us_tickers: List[str] = field(default_factory=lambda: [\n",
    "        \"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"NVDA\", \"META\", \"TSLA\", \"BRK-B\",\n",
    "        \"JPM\", \"JNJ\", \"V\", \"PG\", \"UNH\", \"HD\", \"MA\", \"DIS\", \"BAC\", \"NFLX\",\n",
    "        \"ADBE\", \"CRM\", \"XOM\", \"VZ\", \"KO\", \"INTC\", \"PEP\", \"ABT\", \"CSCO\",\n",
    "        \"COST\", \"MRK\", \"WMT\", \"AVGO\", \"ACN\", \"CVX\", \"NKE\", \"LLY\", \"MCD\",\n",
    "        \"QCOM\", \"UPS\", \"BMY\", \"LIN\", \"NEE\", \"ORCL\", \"RTX\", \"HON\", \"TXN\",\n",
    "        \"AMD\", \"PYPL\", \"CMCSA\", \"TMO\", \"DHR\",\n",
    "    ])\n",
    "\n",
    "    kospi_tickers: List[str] = field(default_factory=lambda: [\n",
    "        \"005930\", \"000660\", \"373220\", \"207940\", \"005380\", \"000270\",\n",
    "        \"035420\", \"006400\", \"105560\", \"051910\", \"005490\", \"034730\",\n",
    "        \"068270\", \"055550\", \"035720\", \"086790\", \"012330\", \"003550\",\n",
    "        \"028260\", \"033780\", \"000810\", \"032830\", \"017670\", \"010950\",\n",
    "        \"316140\", \"066570\", \"009150\", \"018260\", \"011200\", \"034020\",\n",
    "    ])\n",
    "\n",
    "    kosdaq_tickers: List[str] = field(default_factory=lambda: [\n",
    "        \"247540\", \"086520\", \"028300\", \"196170\", \"403870\", \"035900\",\n",
    "        \"263750\", \"293490\", \"053800\", \"112040\", \"041510\", \"145020\",\n",
    "        \"257720\", \"036930\", \"058470\", \"950160\", \"383310\", \"322000\",\n",
    "        \"214150\", \"108320\",\n",
    "    ])\n",
    "\n",
    "    market_index: Dict[str, str] = field(default_factory=lambda: {\n",
    "        \"US\": \"SPY\", \"KOSPI\": \"^KS11\", \"KOSDAQ\": \"^KQ11\",\n",
    "    })\n",
    "\n",
    "    # --- 2. Dynamic Universe ---\n",
    "    use_dynamic_universe: bool = True\n",
    "    rebuild_universe_each_fold: bool = False\n",
    "    min_market_cap: float = 0\n",
    "    min_avg_volume: float = 100000\n",
    "    min_listing_days: int = 252\n",
    "    include_delisted: bool = False\n",
    "    max_universe_size: int = 50\n",
    "\n",
    "    # --- 3. Features ---\n",
    "    momentum_windows: List[int] = field(default_factory=lambda: [5, 20, 60, 120])\n",
    "    volatility_windows: List[int] = field(default_factory=lambda: [20, 60])\n",
    "    regime_window: int = 60\n",
    "    adaptive_quantiles: bool = True\n",
    "    min_bin_size: int = 20\n",
    "\n",
    "    # --- 4. Forward Returns ---\n",
    "    forward_days_list: List[int] = field(default_factory=lambda: [5, 21, 63])\n",
    "    avoid_overlapping_labels: bool = True\n",
    "\n",
    "    # --- 5. Candidates ---\n",
    "    max_candidates_total: int = 3000\n",
    "    max_candidates_per_feature_pair: int = 200\n",
    "    min_sample_size: int = 300\n",
    "\n",
    "    # --- 6. Trees ---\n",
    "    n_trees: int = 20\n",
    "    tree_feature_subsample: float = 0.5\n",
    "    tree_max_depth: int = 2\n",
    "    tree_min_samples_leaf: int = 500\n",
    "\n",
    "    # --- 7. Walk-Forward ---\n",
    "    wf_train_years: int = 3\n",
    "    wf_test_months: int = 12\n",
    "    wf_step_months: int = 6\n",
    "    wf_embargo_days: int = 5\n",
    "    wf_min_folds: int = 4\n",
    "\n",
    "    # --- 8. Overfitting ---\n",
    "    min_stability: float = 0.5\n",
    "    min_sharpe: float = 0.5\n",
    "    min_win_rate: float = 0.52\n",
    "    baseline_winrate_adjusted: bool = True\n",
    "    compute_lift_against_market: bool = True\n",
    "\n",
    "    # --- 9. Bootstrap ---\n",
    "    bootstrap_n: int = 1000\n",
    "    bootstrap_ci: float = 0.95\n",
    "    bootstrap_min_samples: int = 200\n",
    "\n",
    "    # --- 10. Transaction Costs ---\n",
    "    transaction_cost_bps: float = 5.0\n",
    "    slippage_bps: float = 2.0\n",
    "\n",
    "    # --- 11. Turnover ---\n",
    "    penalty_turnover: float = 0.1\n",
    "    max_turnover: float = 3.0\n",
    "    max_position_pct_of_adv: float = 0.05\n",
    "\n",
    "    # --- 12. Correlation Filter ---\n",
    "    max_strategy_correlation: float = 0.85\n",
    "    cluster_strategies: bool = True\n",
    "\n",
    "    # --- 13. Portfolio ---\n",
    "    portfolio_max_strategies: int = 10\n",
    "    portfolio_weight_method: str = \"equal\"\n",
    "\n",
    "    # --- 14. Regime ---\n",
    "    evaluate_by_regime: bool = True\n",
    "    regime_split_method: str = \"market_momentum\"\n",
    "    min_regime_performance_ratio: float = 0.7\n",
    "\n",
    "    # --- 15. Multiple Testing ---\n",
    "    apply_multiple_testing_correction: bool = True\n",
    "    mtc_method: str = \"fdr\"\n",
    "\n",
    "    # --- 16. Memory ---\n",
    "    use_float32: bool = True\n",
    "    max_ram_gb: float = 28\n",
    "    batch_feature_size: int = 5\n",
    "    gc_every_n_candidates: int = 100\n",
    "\n",
    "    # --- Scoring weights ---\n",
    "    w_stability: float = 0.30\n",
    "    w_sharpe: float = 0.30\n",
    "    w_lift: float = 0.20\n",
    "    w_sample: float = 0.20\n",
    "\n",
    "    # --- Paths ---\n",
    "    drive_root: str = \"/content/drive/MyDrive/quant_pipeline_v2\"\n",
    "    data_period: str = \"10y\"\n",
    "    seed: int = 42\n",
    "    logistic_top_pct: float = 0.20\n",
    "\n",
    "    # --- Per-market directory helpers ---\n",
    "    def data_dir(self, market: str) -> str:\n",
    "        return os.path.join(self.drive_root, \"data\", market)\n",
    "\n",
    "    def features_dir(self, market: str) -> str:\n",
    "        return os.path.join(self.drive_root, \"features\", market)\n",
    "\n",
    "    def candidates_dir(self, market: str) -> str:\n",
    "        return os.path.join(self.drive_root, \"candidates\", market)\n",
    "\n",
    "    def evaluation_dir(self, market: str) -> str:\n",
    "        return os.path.join(self.drive_root, \"evaluation\", market)\n",
    "\n",
    "    def walkforward_dir(self, market: str) -> str:\n",
    "        return os.path.join(self.drive_root, \"walkforward\", market)\n",
    "\n",
    "    @property\n",
    "    def global_eval_dir(self) -> str:\n",
    "        return os.path.join(self.drive_root, \"evaluation\", \"_global\")\n",
    "\n",
    "    @property\n",
    "    def logs_dir(self) -> str:\n",
    "        return os.path.join(self.drive_root, \"logs\")\n",
    "\n",
    "    @property\n",
    "    def state_path(self) -> str:\n",
    "        return os.path.join(self.drive_root, \"state.json\")\n",
    "\n",
    "    @property\n",
    "    def total_cost_bps(self) -> float:\n",
    "        return self.transaction_cost_bps + self.slippage_bps\n",
    "\n",
    "\n",
    "CFG = PipelineConfig()\n",
    "print(\"Config created.  Drive root:\", CFG.drive_root)\n",
    "print(\"Markets:\", CFG.markets)\n",
    "print(\"Forward horizons:\", CFG.forward_days_list)\n",
    "print(\"Cost (round-trip bps): %.1f\" % (CFG.total_cost_bps * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Persistence & Resume System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRIVE_MOUNTED = False\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', timeout_ms=60000)\n",
    "    DRIVE_MOUNTED = True\n",
    "    print(\"Google Drive mounted.\")\n",
    "except Exception as e:\n",
    "    print(\"Drive mount failed: %s\" % str(e)[:80])\n",
    "    print(\"Using local storage (data lost on disconnect).\")\n",
    "    CFG.drive_root = \"/content/quant_pipeline_v2\"\n",
    "\n",
    "# Create directories for all markets\n",
    "for mkt in CFG.markets:\n",
    "    for d in [CFG.data_dir(mkt), CFG.features_dir(mkt), CFG.candidates_dir(mkt),\n",
    "              CFG.evaluation_dir(mkt), CFG.walkforward_dir(mkt)]:\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "os.makedirs(CFG.global_eval_dir, exist_ok=True)\n",
    "os.makedirs(CFG.logs_dir, exist_ok=True)\n",
    "print(\"Directories ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgressTracker:\n",
    "    \"\"\"JSON-based checkpoint system for resumable execution.\"\"\"\n",
    "\n",
    "    def __init__(self, state_path: str):\n",
    "        self.state_path = state_path\n",
    "        self.state = self._load()\n",
    "\n",
    "    def _load(self) -> dict:\n",
    "        if os.path.exists(self.state_path):\n",
    "            with open(self.state_path, 'r') as f:\n",
    "                return json.load(f)\n",
    "        return {\"completed_steps\": {}, \"metadata\": {}}\n",
    "\n",
    "    def _save(self):\n",
    "        with open(self.state_path, 'w') as f:\n",
    "            json.dump(self.state, f, indent=2, default=str)\n",
    "\n",
    "    def is_completed(self, step_name: str) -> bool:\n",
    "        return self.state[\"completed_steps\"].get(step_name, False)\n",
    "\n",
    "    def mark_completed(self, step_name: str, metadata: dict = None):\n",
    "        self.state[\"completed_steps\"][step_name] = True\n",
    "        if metadata:\n",
    "            self.state[\"metadata\"][step_name] = metadata\n",
    "        self._save()\n",
    "        print(\"  [CHECKPOINT] %s completed.\" % step_name)\n",
    "\n",
    "    def get_metadata(self, step_name: str) -> dict:\n",
    "        return self.state[\"metadata\"].get(step_name, {})\n",
    "\n",
    "    def reset(self, step_name: str = None):\n",
    "        if step_name:\n",
    "            self.state[\"completed_steps\"].pop(step_name, None)\n",
    "            self.state[\"metadata\"].pop(step_name, None)\n",
    "        else:\n",
    "            self.state = {\"completed_steps\": {}, \"metadata\": {}}\n",
    "        self._save()\n",
    "\n",
    "    def summary(self):\n",
    "        completed = [k for k, v in self.state[\"completed_steps\"].items() if v]\n",
    "        print(\"=== Progress Summary ===\")\n",
    "        if completed:\n",
    "            for s in completed:\n",
    "                print(\"  [DONE] %s\" % s)\n",
    "        else:\n",
    "            print(\"  No steps completed yet.\")\n",
    "\n",
    "\n",
    "tracker = ProgressTracker(CFG.state_path)\n",
    "tracker.summary()\n",
    "\n",
    "# Uncomment to force re-run from scratch:\n",
    "# tracker.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "log_file = os.path.join(CFG.logs_dir, \"pipeline.log\")\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\",\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),\n",
    "        logging.FileHandler(log_file, mode='a'),\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(\"pipeline\")\n",
    "logger.info(\"Pipeline v2 started. Log: %s\" % log_file)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Market Configuration Registry\n",
    "\n",
    "Defines per-market data sources, index tickers, currencies, and download helpers.\n",
    "- **US**: yfinance, SPY, USD\n",
    "- **KOSPI**: pykrx (primary) / FinanceDataReader (fallback), ^KS11, KRW\n",
    "- **KOSDAQ**: pykrx / FinanceDataReader, ^KQ11, KRW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARKET_REGISTRY = {\n",
    "    \"US\": {\n",
    "        \"data_source\": \"yfinance\",\n",
    "        \"index_ticker\": \"SPY\",\n",
    "        \"currency\": \"USD\",\n",
    "        \"pykrx_market\": None,\n",
    "    },\n",
    "    \"KOSPI\": {\n",
    "        \"data_source\": \"pykrx\",\n",
    "        \"index_ticker\": \"^KS11\",\n",
    "        \"currency\": \"KRW\",\n",
    "        \"pykrx_market\": \"KOSPI\",\n",
    "    },\n",
    "    \"KOSDAQ\": {\n",
    "        \"data_source\": \"pykrx\",\n",
    "        \"index_ticker\": \"^KQ11\",\n",
    "        \"currency\": \"KRW\",\n",
    "        \"pykrx_market\": \"KOSDAQ\",\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def get_static_tickers(market):\n",
    "    \"\"\"Return static fallback ticker list for a market.\"\"\"\n",
    "    if market == \"US\":\n",
    "        return list(CFG.us_tickers)\n",
    "    elif market == \"KOSPI\":\n",
    "        return list(CFG.kospi_tickers)\n",
    "    elif market == \"KOSDAQ\":\n",
    "        return list(CFG.kosdaq_tickers)\n",
    "    return []\n",
    "\n",
    "\n",
    "def download_fx_rates(period=\"10y\"):\n",
    "    \"\"\"Download USD/KRW exchange rate via yfinance.\"\"\"\n",
    "    import yfinance as yf\n",
    "    try:\n",
    "        fx = yf.download(\"USDKRW=X\", period=period, progress=False, auto_adjust=True)\n",
    "        if isinstance(fx.columns, pd.MultiIndex):\n",
    "            fx.columns = fx.columns.get_level_values(0)\n",
    "        fx = fx[[\"Close\"]].copy()\n",
    "        fx.columns = [\"usdkrw\"]\n",
    "        fx.index = pd.to_datetime(fx.index).tz_localize(None)\n",
    "        return fx\n",
    "    except Exception as e:\n",
    "        logger.warning(\"FX download failed: %s\" % str(e)[:60])\n",
    "        return pd.DataFrame(columns=[\"usdkrw\"])\n",
    "\n",
    "\n",
    "def convert_krw_to_usd(df_krw, fx_df):\n",
    "    \"\"\"Convert KRW-denominated OHLCV to USD using FX rates.\"\"\"\n",
    "    if fx_df.empty:\n",
    "        logger.warning(\"No FX data — returning KRW prices.\")\n",
    "        return df_krw\n",
    "    fx_aligned = fx_df[\"usdkrw\"].reindex(df_krw.index.get_level_values(0), method=\"ffill\")\n",
    "    fx_aligned.index = df_krw.index\n",
    "    for col in [\"open\", \"high\", \"low\", \"close\"]:\n",
    "        if col in df_krw.columns:\n",
    "            df_krw[col] = df_krw[col] / fx_aligned\n",
    "    return df_krw\n",
    "\n",
    "\n",
    "print(\"Market registry ready. Markets:\", list(MARKET_REGISTRY.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dynamic Universe Builder\n",
    "\n",
    "Constructs a point-in-time stock universe per market. For Korean markets uses\n",
    "`pykrx.stock.get_market_ticker_list` with market-cap / volume filters. Falls\n",
    "back to a static ticker list when the API is unavailable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_universe(market, ref_date=None, config=None):\n",
    "    \"\"\"Build a point-in-time ticker universe for *market*.\n",
    "\n",
    "    Returns a list of ticker strings.\n",
    "    \"\"\"\n",
    "    if config is None:\n",
    "        config = CFG\n",
    "    registry = MARKET_REGISTRY[market]\n",
    "\n",
    "    # --- Korean markets: try pykrx dynamic universe ---\n",
    "    if registry[\"data_source\"] == \"pykrx\" and config.use_dynamic_universe:\n",
    "        try:\n",
    "            from pykrx import stock\n",
    "            if ref_date is None:\n",
    "                import datetime\n",
    "                ref_date = datetime.datetime.now()\n",
    "            date_str = pd.Timestamp(ref_date).strftime(\"%Y%m%d\")\n",
    "\n",
    "            tickers = stock.get_market_ticker_list(date_str, market=registry[\"pykrx_market\"])\n",
    "            if not tickers:\n",
    "                raise ValueError(\"Empty ticker list from pykrx\")\n",
    "\n",
    "            # Apply market-cap / volume filter\n",
    "            try:\n",
    "                cap_df = stock.get_market_cap_by_ticker(date_str, market=registry[\"pykrx_market\"])\n",
    "                if not cap_df.empty:\n",
    "                    if config.min_avg_volume > 0:\n",
    "                        cap_df = cap_df[cap_df.iloc[:, -1] >= config.min_avg_volume]\n",
    "                    cap_df = cap_df.sort_values(cap_df.columns[0], ascending=False)\n",
    "                    tickers = cap_df.head(config.max_universe_size).index.tolist()\n",
    "            except Exception:\n",
    "                tickers = tickers[:config.max_universe_size]\n",
    "\n",
    "            logger.info(\"Dynamic universe for %s: %d tickers (ref %s)\" % (\n",
    "                market, len(tickers), date_str))\n",
    "            return tickers\n",
    "        except Exception as e:\n",
    "            logger.warning(\"Dynamic universe failed for %s: %s — using static list\" % (\n",
    "                market, str(e)[:60]))\n",
    "\n",
    "    # --- Fallback: static list ---\n",
    "    static = get_static_tickers(market)\n",
    "    logger.info(\"Static universe for %s: %d tickers\" % (market, len(static)))\n",
    "    return static\n",
    "\n",
    "\n",
    "print(\"Universe builder ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Layer\n",
    "\n",
    "Downloads OHLCV data per market:\n",
    "- **US**: yfinance\n",
    "- **KOSPI / KOSDAQ**: pykrx (primary), FinanceDataReader (fallback)\n",
    "\n",
    "Korean prices are optionally converted to USD. Market indices and FX rates\n",
    "are downloaded separately.\n",
    "\n",
    "**Safeguards:** No lookahead bias — forward returns computed later in the\n",
    "Feature Engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "# Storage for loaded data (populated either from cache or download)\n",
    "ohlcv_data = {}        # market -> DataFrame (MultiIndex: date, ticker)\n",
    "market_indices = {}    # market -> DataFrame (index: date, col: close)\n",
    "fx_rates = pd.DataFrame()\n",
    "\n",
    "# Download FX rates once (needed for KR markets)\n",
    "fx_path = os.path.join(CFG.drive_root, \"data\", \"fx_usdkrw.parquet\")\n",
    "if os.path.exists(fx_path):\n",
    "    fx_rates = pd.read_parquet(fx_path)\n",
    "    print(\"FX rates loaded: %d rows\" % len(fx_rates))\n",
    "else:\n",
    "    if any(MARKET_REGISTRY[m][\"currency\"] == \"KRW\" for m in CFG.markets):\n",
    "        fx_rates = download_fx_rates(CFG.data_period)\n",
    "        if not fx_rates.empty:\n",
    "            fx_rates.to_parquet(fx_path)\n",
    "            print(\"FX rates downloaded: %d rows\" % len(fx_rates))\n",
    "\n",
    "# --- Per-market download ---\n",
    "for market in CFG.markets:\n",
    "    STEP = \"data_load_%s\" % market\n",
    "    processed_path = os.path.join(CFG.data_dir(market), \"processed.parquet\")\n",
    "    idx_path = os.path.join(CFG.data_dir(market), \"market_index.parquet\")\n",
    "    registry = MARKET_REGISTRY[market]\n",
    "\n",
    "    if tracker.is_completed(STEP):\n",
    "        logger.info(\"[SKIP] %s — loading cache\" % STEP)\n",
    "        ohlcv_data[market] = pd.read_parquet(processed_path)\n",
    "        market_indices[market] = pd.read_parquet(idx_path)\n",
    "        print(\"%s loaded: %s\" % (market, str(ohlcv_data[market].shape)))\n",
    "        continue\n",
    "\n",
    "    logger.info(\"[RUN] %s\" % STEP)\n",
    "    t0 = time.time()\n",
    "    tickers = build_universe(market)\n",
    "    all_dfs = []\n",
    "    failed = []\n",
    "\n",
    "    if registry[\"data_source\"] == \"yfinance\":\n",
    "        for i, ticker in enumerate(tickers):\n",
    "            if (i + 1) % 10 == 0 or i == 0:\n",
    "                print(\"  [%d/%d] %s\" % (i + 1, len(tickers), ticker))\n",
    "            try:\n",
    "                df = yf.download(ticker, period=CFG.data_period, progress=False, auto_adjust=True)\n",
    "                if df.empty or len(df) < 252:\n",
    "                    failed.append(ticker)\n",
    "                    continue\n",
    "                if isinstance(df.columns, pd.MultiIndex):\n",
    "                    df.columns = df.columns.get_level_values(0)\n",
    "                df = df[[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]].copy()\n",
    "                df.columns = [\"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
    "                df.index = pd.to_datetime(df.index).tz_localize(None)\n",
    "                df[\"ticker\"] = ticker\n",
    "                all_dfs.append(df)\n",
    "            except Exception as e:\n",
    "                failed.append(ticker)\n",
    "\n",
    "    elif registry[\"data_source\"] == \"pykrx\":\n",
    "        import datetime as _dt\n",
    "        end_str = _dt.datetime.now().strftime(\"%Y%m%d\")\n",
    "        start_str = (_dt.datetime.now() - _dt.timedelta(days=365 * 10)).strftime(\"%Y%m%d\")\n",
    "\n",
    "        for i, ticker in enumerate(tickers):\n",
    "            if (i + 1) % 10 == 0 or i == 0:\n",
    "                print(\"  [%d/%d] %s\" % (i + 1, len(tickers), ticker))\n",
    "            try:\n",
    "                from pykrx import stock as pykrx_stock\n",
    "                df = pykrx_stock.get_market_ohlcv_by_date(start_str, end_str, ticker)\n",
    "                if df.empty or len(df) < 252:\n",
    "                    raise ValueError(\"Insufficient data\")\n",
    "                rename_map = {}\n",
    "                for c in df.columns:\n",
    "                    cl = c.strip()\n",
    "                    if cl in (\"시가\", \"Open\"):\n",
    "                        rename_map[c] = \"open\"\n",
    "                    elif cl in (\"고가\", \"High\"):\n",
    "                        rename_map[c] = \"high\"\n",
    "                    elif cl in (\"저가\", \"Low\"):\n",
    "                        rename_map[c] = \"low\"\n",
    "                    elif cl in (\"종가\", \"Close\"):\n",
    "                        rename_map[c] = \"close\"\n",
    "                    elif cl in (\"거래량\", \"Volume\"):\n",
    "                        rename_map[c] = \"volume\"\n",
    "                df = df.rename(columns=rename_map)\n",
    "                df = df[[\"open\", \"high\", \"low\", \"close\", \"volume\"]].copy()\n",
    "                df.index = pd.to_datetime(df.index).tz_localize(None)\n",
    "                df[\"ticker\"] = ticker\n",
    "                all_dfs.append(df)\n",
    "            except Exception as e1:\n",
    "                # Fallback: FinanceDataReader\n",
    "                try:\n",
    "                    import FinanceDataReader as fdr\n",
    "                    df2 = fdr.DataReader(ticker, start_str[:4] + \"-\" + start_str[4:6] + \"-\" + start_str[6:])\n",
    "                    if df2.empty or len(df2) < 252:\n",
    "                        failed.append(ticker)\n",
    "                        continue\n",
    "                    df2 = df2.rename(columns={\n",
    "                        \"Open\": \"open\", \"High\": \"high\", \"Low\": \"low\",\n",
    "                        \"Close\": \"close\", \"Volume\": \"volume\",\n",
    "                    })\n",
    "                    df2 = df2[[\"open\", \"high\", \"low\", \"close\", \"volume\"]].copy()\n",
    "                    df2.index = pd.to_datetime(df2.index).tz_localize(None)\n",
    "                    df2[\"ticker\"] = ticker\n",
    "                    all_dfs.append(df2)\n",
    "                except Exception:\n",
    "                    failed.append(ticker)\n",
    "\n",
    "    if not all_dfs:\n",
    "        logger.warning(\"No data for market %s — skipping.\" % market)\n",
    "        continue\n",
    "\n",
    "    panel = pd.concat(all_dfs)\n",
    "    panel = panel.set_index([panel.index, \"ticker\"])\n",
    "    panel.index.names = [\"date\", \"ticker\"]\n",
    "    panel = panel.sort_index()\n",
    "\n",
    "    # FX conversion for KRW markets\n",
    "    if registry[\"currency\"] == \"KRW\" and CFG.apply_fx_conversion and not fx_rates.empty:\n",
    "        panel = convert_krw_to_usd(panel, fx_rates)\n",
    "        logger.info(\"FX conversion applied for %s\" % market)\n",
    "\n",
    "    # float32 for memory\n",
    "    if CFG.use_float32:\n",
    "        for col in [\"open\", \"high\", \"low\", \"close\"]:\n",
    "            panel[col] = panel[col].astype(np.float32)\n",
    "        panel[\"volume\"] = panel[\"volume\"].astype(np.float64)\n",
    "\n",
    "    panel.to_parquet(processed_path)\n",
    "    ohlcv_data[market] = panel\n",
    "\n",
    "    # Market index\n",
    "    idx_ticker = registry[\"index_ticker\"]\n",
    "    try:\n",
    "        idx_df = yf.download(idx_ticker, period=CFG.data_period, progress=False, auto_adjust=True)\n",
    "        if isinstance(idx_df.columns, pd.MultiIndex):\n",
    "            idx_df.columns = idx_df.columns.get_level_values(0)\n",
    "        idx_df = idx_df[[\"Close\"]].copy()\n",
    "        idx_df.columns = [\"close\"]\n",
    "        idx_df.index = pd.to_datetime(idx_df.index).tz_localize(None)\n",
    "        if registry[\"currency\"] == \"KRW\" and CFG.apply_fx_conversion and not fx_rates.empty:\n",
    "            fx_al = fx_rates[\"usdkrw\"].reindex(idx_df.index, method=\"ffill\")\n",
    "            idx_df[\"close\"] = idx_df[\"close\"] / fx_al\n",
    "    except Exception as e:\n",
    "        logger.warning(\"Index download failed for %s: %s\" % (market, str(e)[:60]))\n",
    "        idx_df = pd.DataFrame(columns=[\"close\"])\n",
    "    idx_df.to_parquet(idx_path)\n",
    "    market_indices[market] = idx_df\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    meta = {\"n_tickers\": len(all_dfs), \"failed\": failed, \"rows\": len(panel), \"time_sec\": elapsed}\n",
    "    tracker.mark_completed(STEP, meta)\n",
    "    print(\"%s downloaded: %d/%d tickers in %.0fs\" % (market, len(all_dfs), len(tickers), elapsed))\n",
    "    if failed:\n",
    "        print(\"  Failed: %s\" % failed[:10])\n",
    "    gc.collect()\n",
    "\n",
    "# Summary\n",
    "for m in CFG.markets:\n",
    "    if m in ohlcv_data:\n",
    "        p = ohlcv_data[m]\n",
    "        tks = p.index.get_level_values(1).unique()\n",
    "        dts = p.index.get_level_values(0).unique()\n",
    "        print(\"%s — %s | tickers %d | %s to %s\" % (\n",
    "            m, str(p.shape), len(tks), dts.min().date(), dts.max().date()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engine\n",
    "\n",
    "Generates momentum, volatility, and regime features per market.\n",
    "\n",
    "**Enhancements:**\n",
    "- Multi-horizon forward returns: 5d, 21d, 63d\n",
    "- Cost-adjusted forward returns: `net_return = raw_return - 2 * total_cost_bps / 10000`\n",
    "- Adaptive quantile binning (quintiles if universe < 100, else deciles)\n",
    "- float32 downcasting for memory efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_momentum_features(close: pd.Series, windows: list) -> pd.DataFrame:\n",
    "    \"\"\"Momentum (return) features for each window.\"\"\"\n",
    "    feats = {}\n",
    "    for w in windows:\n",
    "        feats[\"mom_%dd\" % w] = close.pct_change(w)\n",
    "    return pd.DataFrame(feats, index=close.index)\n",
    "\n",
    "\n",
    "def compute_volatility_features(close: pd.Series, windows: list) -> pd.DataFrame:\n",
    "    \"\"\"Rolling volatility features.\"\"\"\n",
    "    daily_ret = close.pct_change()\n",
    "    feats = {}\n",
    "    for w in windows:\n",
    "        feats[\"vol_%dd\" % w] = daily_ret.rolling(w).std()\n",
    "    if len(windows) >= 2:\n",
    "        short_w, long_w = windows[0], windows[-1]\n",
    "        feats[\"vol_change\"] = (\n",
    "            daily_ret.rolling(short_w).std()\n",
    "            / daily_ret.rolling(long_w).std().replace(0, np.nan) - 1.0\n",
    "        )\n",
    "    return pd.DataFrame(feats, index=close.index)\n",
    "\n",
    "\n",
    "def compute_regime_features(market_close: pd.Series, regime_window: int) -> pd.DataFrame:\n",
    "    \"\"\"Market-level regime features.\"\"\"\n",
    "    market_ret = market_close.pct_change()\n",
    "    feats = {}\n",
    "    feats[\"market_mom_%dd\" % regime_window] = market_close.pct_change(regime_window)\n",
    "    feats[\"market_vol_%dd\" % regime_window] = market_ret.rolling(regime_window).std()\n",
    "    mom = feats[\"market_mom_%dd\" % regime_window]\n",
    "    vol = feats[\"market_vol_%dd\" % regime_window]\n",
    "    vol_median = vol.rolling(252, min_periods=60).median()\n",
    "    feats[\"regime_bull\"] = ((mom > 0) & (vol < vol_median)).astype(float)\n",
    "    return pd.DataFrame(feats, index=market_close.index)\n",
    "\n",
    "\n",
    "def adaptive_n_bins(universe_size, config):\n",
    "    \"\"\"Return number of bins based on universe size.\"\"\"\n",
    "    if not config.adaptive_quantiles:\n",
    "        return 10\n",
    "    if universe_size < 5 * config.min_bin_size:\n",
    "        return 5\n",
    "    return 10\n",
    "\n",
    "\n",
    "def to_cross_sectional_deciles(feature_series, date_level, n_bins=10):\n",
    "    \"\"\"Convert feature to cross-sectional quantile ranks (0 to n_bins-1).\"\"\"\n",
    "    def rank_date(group):\n",
    "        valid = group.dropna()\n",
    "        if len(valid) < n_bins:\n",
    "            return pd.Series(np.nan, index=group.index)\n",
    "        ranks = valid.rank(method='first')\n",
    "        deciles = pd.cut(ranks, bins=n_bins, labels=False)\n",
    "        return deciles.reindex(group.index)\n",
    "    return feature_series.groupby(level=date_level).transform(rank_date)\n",
    "\n",
    "\n",
    "print(\"Feature functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_panels = {}   # market -> DataFrame\n",
    "\n",
    "for market in CFG.markets:\n",
    "    if market not in ohlcv_data:\n",
    "        continue\n",
    "\n",
    "    STEP = \"features_%s\" % market\n",
    "    fpath = os.path.join(CFG.features_dir(market), \"all_features.parquet\")\n",
    "\n",
    "    if tracker.is_completed(STEP):\n",
    "        logger.info(\"[SKIP] %s — loading\" % STEP)\n",
    "        feature_panels[market] = pd.read_parquet(fpath)\n",
    "        print(\"%s features loaded: %s\" % (market, str(feature_panels[market].shape)))\n",
    "        continue\n",
    "\n",
    "    logger.info(\"[RUN] %s\" % STEP)\n",
    "    t0 = time.time()\n",
    "    panel = ohlcv_data[market]\n",
    "    valid_tickers = panel.index.get_level_values(1).unique().tolist()\n",
    "\n",
    "    # Universe size for adaptive binning\n",
    "    n_bins = adaptive_n_bins(len(valid_tickers), CFG)\n",
    "    logger.info(\"%s: %d tickers, using %d bins\" % (market, len(valid_tickers), n_bins))\n",
    "\n",
    "    # Market index for regime features\n",
    "    mkt_close = market_indices.get(market, pd.DataFrame()).get(\"close\", pd.Series(dtype=float))\n",
    "    regime_feats = compute_regime_features(mkt_close, CFG.regime_window) if len(mkt_close) > 0 else pd.DataFrame()\n",
    "    market_ret_20d = mkt_close.pct_change(20) if len(mkt_close) > 0 else pd.Series(dtype=float)\n",
    "\n",
    "    all_features = []\n",
    "    for ticker in valid_tickers:\n",
    "        try:\n",
    "            tdata = panel.loc[(slice(None), ticker), :].droplevel(1)\n",
    "            close = tdata[\"close\"]\n",
    "\n",
    "            mom = compute_momentum_features(close, CFG.momentum_windows)\n",
    "\n",
    "            # Market-relative return\n",
    "            if len(market_ret_20d) > 0:\n",
    "                stock_20d = close.pct_change(20)\n",
    "                mkt_al = market_ret_20d.reindex(close.index, method='ffill')\n",
    "                mom[\"market_relative_20d\"] = stock_20d - mkt_al\n",
    "\n",
    "            vol = compute_volatility_features(close, CFG.volatility_windows)\n",
    "\n",
    "            reg = regime_feats.reindex(close.index, method='ffill') if len(regime_feats) > 0 else pd.DataFrame(index=close.index)\n",
    "\n",
    "            combined = pd.concat([mom, vol, reg], axis=1)\n",
    "\n",
    "            # Multi-horizon cost-adjusted forward returns\n",
    "            for fwd_days in CFG.forward_days_list:\n",
    "                raw_fwd = close.pct_change(fwd_days).shift(-fwd_days)\n",
    "                net_fwd = raw_fwd - 2 * CFG.total_cost_bps / 10000\n",
    "                combined[\"fwd_return_%dd\" % fwd_days] = net_fwd\n",
    "\n",
    "            combined[\"ticker\"] = ticker\n",
    "            combined.index.name = \"date\"\n",
    "            all_features.append(combined)\n",
    "        except Exception as e:\n",
    "            logger.warning(\"Feature error %s/%s: %s\" % (market, ticker, str(e)[:60]))\n",
    "\n",
    "    if not all_features:\n",
    "        logger.warning(\"No features for %s\" % market)\n",
    "        continue\n",
    "\n",
    "    fp = pd.concat(all_features)\n",
    "    fp = fp.reset_index().set_index([\"date\", \"ticker\"]).sort_index()\n",
    "\n",
    "    fwd_cols = [c for c in fp.columns if c.startswith(\"fwd_return_\")]\n",
    "    feat_cols = [c for c in fp.columns if c not in fwd_cols]\n",
    "    fp = fp.dropna(subset=feat_cols, how='all')\n",
    "\n",
    "    # float32\n",
    "    if CFG.use_float32:\n",
    "        for c in fp.select_dtypes(include=['float64']).columns:\n",
    "            fp[c] = fp[c].astype(np.float32)\n",
    "\n",
    "    # Cross-sectional deciles\n",
    "    logger.info(\"Computing cross-sectional deciles for %s...\" % market)\n",
    "    for col in feat_cols:\n",
    "        fp[col + \"_decile\"] = to_cross_sectional_deciles(fp[col], \"date\", n_bins)\n",
    "\n",
    "    fp.to_parquet(fpath)\n",
    "    feature_panels[market] = fp\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    tracker.mark_completed(STEP, {\n",
    "        \"n_features\": len(feat_cols), \"n_rows\": len(fp),\n",
    "        \"n_bins\": n_bins, \"time_sec\": elapsed,\n",
    "    })\n",
    "    print(\"%s features saved (%.0fs): %s\" % (market, elapsed, str(fp.shape)))\n",
    "    gc.collect()\n",
    "\n",
    "# Summary\n",
    "for m, fp in feature_panels.items():\n",
    "    fwd_cols = [c for c in fp.columns if c.startswith(\"fwd_return_\")]\n",
    "    feat_cols = [c for c in fp.columns if not c.startswith(\"fwd_return_\") and not c.endswith(\"_decile\")]\n",
    "    decile_cols = [c for c in fp.columns if c.endswith(\"_decile\")]\n",
    "    print(\"%s: %d raw features, %d decile features, horizons: %s\" % (\n",
    "        m, len(feat_cols), len(decile_cols), fwd_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Candidate Generator\n",
    "\n",
    "Three methods, run **per market** and **per forward-return horizon**:\n",
    "- **A. Decile Conditions** — single & 2-feature decile combos with explosion control\n",
    "- **B. Decision Trees** — shallow trees with feature subsampling\n",
    "- **C. Logistic Rank** — quintile strategies from logistic regression\n",
    "\n",
    "Early rejection: skip if sample < min, preliminary Sharpe < 0, or lift < 1.02x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "all_candidates_list = []   # collect across markets & horizons\n",
    "\n",
    "for market in CFG.markets:\n",
    "    if market not in feature_panels:\n",
    "        continue\n",
    "    fp = feature_panels[market]\n",
    "    fwd_cols = sorted([c for c in fp.columns if c.startswith(\"fwd_return_\")])\n",
    "    feat_cols = [c for c in fp.columns if not c.startswith(\"fwd_return_\") and not c.endswith(\"_decile\")]\n",
    "    decile_cols = [c for c in fp.columns if c.endswith(\"_decile\")]\n",
    "\n",
    "    for fwd_col in fwd_cols:\n",
    "        horizon = fwd_col  # e.g. fwd_return_21d\n",
    "        horizon_tag = fwd_col.replace(\"fwd_return_\", \"\")  # e.g. 21d\n",
    "        STEP = \"candidates_decile_%s_%s\" % (market, horizon_tag)\n",
    "        save_path = os.path.join(CFG.candidates_dir(market), \"decile_%s.parquet\" % horizon_tag)\n",
    "\n",
    "        if tracker.is_completed(STEP):\n",
    "            logger.info(\"[SKIP] %s\" % STEP)\n",
    "            df_cached = pd.read_parquet(save_path)\n",
    "            all_candidates_list.append(df_cached)\n",
    "            print(\"  Loaded %d decile candidates for %s/%s\" % (len(df_cached), market, horizon_tag))\n",
    "            continue\n",
    "\n",
    "        logger.info(\"[RUN] %s\" % STEP)\n",
    "        t0 = time.time()\n",
    "        valid = fp.dropna(subset=[fwd_col]).copy()\n",
    "        # Detect n_bins from actual decile values in data\n",
    "        if decile_cols:\n",
    "            n_bins = int(valid[decile_cols[0]].dropna().max()) + 1\n",
    "        else:\n",
    "            n_bins = 10\n",
    "\n",
    "        unconditional_mean = valid[fwd_col].mean()\n",
    "        candidates = []\n",
    "\n",
    "        # --- Single-feature conditions ---\n",
    "        for col in decile_cols:\n",
    "            for dv in range(n_bins):\n",
    "                mask = valid[col] == dv\n",
    "                n_trades = int(mask.sum())\n",
    "                if n_trades < CFG.min_sample_size:\n",
    "                    continue\n",
    "                ret = valid.loc[mask, fwd_col]\n",
    "                mr = float(ret.mean())\n",
    "                # Early rejection\n",
    "                if mr <= 0 and (unconditional_mean > 0):\n",
    "                    continue\n",
    "                wr = float((ret > 0).mean())\n",
    "                candidates.append({\n",
    "                    \"strategy_id\": \"%s_%s_%s_d%d\" % (market, horizon_tag, col, dv),\n",
    "                    \"market\": market, \"horizon\": horizon_tag,\n",
    "                    \"type\": \"single_decile\",\n",
    "                    \"features\": col,\n",
    "                    \"condition\": \"== %d\" % dv,\n",
    "                    \"n_trades\": n_trades,\n",
    "                    \"mean_return\": mr,\n",
    "                    \"win_rate\": wr,\n",
    "                })\n",
    "\n",
    "        logger.info(\"  Single decile: %d\" % len(candidates))\n",
    "\n",
    "        # --- 2-feature combos (extreme deciles only) ---\n",
    "        extreme = [0, 1, n_bins - 2, n_bins - 1] if n_bins >= 4 else list(range(n_bins))\n",
    "        n_before = len(candidates)\n",
    "        pair_count = 0\n",
    "\n",
    "        for col_a, col_b in combinations(decile_cols, 2):\n",
    "            if len(candidates) - n_before >= CFG.max_candidates_per_feature_pair * len(list(combinations(decile_cols, 2))):\n",
    "                break\n",
    "            pair_cands = 0\n",
    "            for da in extreme:\n",
    "                for db in extreme:\n",
    "                    if pair_cands >= CFG.max_candidates_per_feature_pair:\n",
    "                        break\n",
    "                    mask = (valid[col_a] == da) & (valid[col_b] == db)\n",
    "                    n_trades = int(mask.sum())\n",
    "                    if n_trades < CFG.min_sample_size:\n",
    "                        continue\n",
    "                    ret = valid.loc[mask, fwd_col]\n",
    "                    mr = float(ret.mean())\n",
    "                    std_r = float(ret.std())\n",
    "                    if std_r > 1e-8 and mr / std_r < 0:\n",
    "                        continue  # preliminary Sharpe < 0\n",
    "                    wr = float((ret > 0).mean())\n",
    "                    candidates.append({\n",
    "                        \"strategy_id\": \"%s_%s_%s_d%d_AND_%s_d%d\" % (\n",
    "                            market, horizon_tag, col_a, da, col_b, db),\n",
    "                        \"market\": market, \"horizon\": horizon_tag,\n",
    "                        \"type\": \"combo_decile\",\n",
    "                        \"features\": \"%s, %s\" % (col_a, col_b),\n",
    "                        \"condition\": \"%s==%d AND %s==%d\" % (col_a, da, col_b, db),\n",
    "                        \"n_trades\": n_trades,\n",
    "                        \"mean_return\": mr,\n",
    "                        \"win_rate\": wr,\n",
    "                    })\n",
    "                    pair_cands += 1\n",
    "            pair_count += 1\n",
    "            if pair_count % 50 == 0:\n",
    "                print(\"    %d pairs, %d candidates\" % (pair_count, len(candidates)))\n",
    "\n",
    "            # Global cap\n",
    "            if len(candidates) >= CFG.max_candidates_total:\n",
    "                logger.info(\"  Hit max_candidates_total cap\")\n",
    "                break\n",
    "\n",
    "        logger.info(\"  Combo decile: %d\" % (len(candidates) - n_before))\n",
    "        dc = pd.DataFrame(candidates)\n",
    "        dc.to_parquet(save_path)\n",
    "        all_candidates_list.append(dc)\n",
    "\n",
    "        elapsed = time.time() - t0\n",
    "        tracker.mark_completed(STEP, {\"n\": len(dc), \"time_sec\": elapsed})\n",
    "        print(\"  Decile candidates %s/%s: %d (%.0fs)\" % (market, horizon_tag, len(dc), elapsed))\n",
    "        gc.collect()\n",
    "\n",
    "print(\"\\nTotal decile candidates across all markets/horizons: %d\" % sum(len(d) for d in all_candidates_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pickle\n",
    "\n",
    "for market in CFG.markets:\n",
    "    if market not in feature_panels:\n",
    "        continue\n",
    "    fp = feature_panels[market]\n",
    "    fwd_cols = sorted([c for c in fp.columns if c.startswith(\"fwd_return_\")])\n",
    "    feat_cols = [c for c in fp.columns if not c.startswith(\"fwd_return_\") and not c.endswith(\"_decile\")]\n",
    "\n",
    "    for fwd_col in fwd_cols:\n",
    "        horizon_tag = fwd_col.replace(\"fwd_return_\", \"\")\n",
    "        STEP = \"candidates_tree_%s_%s\" % (market, horizon_tag)\n",
    "        save_path = os.path.join(CFG.candidates_dir(market), \"tree_%s.parquet\" % horizon_tag)\n",
    "\n",
    "        if tracker.is_completed(STEP):\n",
    "            logger.info(\"[SKIP] %s\" % STEP)\n",
    "            all_candidates_list.append(pd.read_parquet(save_path))\n",
    "            continue\n",
    "\n",
    "        logger.info(\"[RUN] %s\" % STEP)\n",
    "        t0 = time.time()\n",
    "        valid = fp.dropna(subset=[fwd_col] + feat_cols).copy()\n",
    "        X = valid[feat_cols].values.astype(np.float32)\n",
    "        y = (valid[fwd_col].values > 0).astype(int)\n",
    "\n",
    "        tree_strats = []\n",
    "        n_feat = len(feat_cols)\n",
    "        n_sub = max(3, int(n_feat * CFG.tree_feature_subsample))\n",
    "\n",
    "        for ti in range(CFG.n_trees):\n",
    "            feat_idx = np.random.choice(n_feat, n_sub, replace=False)\n",
    "            feat_names = [feat_cols[j] for j in feat_idx]\n",
    "            X_sub = X[:, feat_idx]\n",
    "            sample_idx = np.random.choice(len(X_sub), min(len(X_sub), 50000), replace=False)\n",
    "\n",
    "            tree = DecisionTreeClassifier(\n",
    "                max_depth=CFG.tree_max_depth,\n",
    "                min_samples_leaf=CFG.tree_min_samples_leaf,\n",
    "                random_state=CFG.seed + ti,\n",
    "            )\n",
    "            tree.fit(X_sub[sample_idx], y[sample_idx])\n",
    "\n",
    "            leaf_ids = tree.apply(X_sub)\n",
    "            for leaf in np.unique(leaf_ids):\n",
    "                lmask = tree.apply(X[:, feat_idx]) == leaf\n",
    "                nt = int(lmask.sum())\n",
    "                if nt < CFG.min_sample_size:\n",
    "                    continue\n",
    "                ret = valid[fwd_col].values[lmask]\n",
    "                mr = float(np.nanmean(ret))\n",
    "                if mr <= 0:\n",
    "                    continue\n",
    "                tree_strats.append({\n",
    "                    \"strategy_id\": \"%s_%s_tree_%d_leaf_%d\" % (market, horizon_tag, ti, leaf),\n",
    "                    \"market\": market, \"horizon\": horizon_tag,\n",
    "                    \"type\": \"decision_tree\",\n",
    "                    \"features\": \", \".join(feat_names[:5]),\n",
    "                    \"condition\": \"tree_%d/leaf_%d\" % (ti, leaf),\n",
    "                    \"n_trades\": nt,\n",
    "                    \"mean_return\": mr,\n",
    "                    \"win_rate\": float((ret > 0).mean()),\n",
    "                })\n",
    "\n",
    "            tp = os.path.join(CFG.candidates_dir(market), \"tree_model_%s_%d.pkl\" % (horizon_tag, ti))\n",
    "            with open(tp, 'wb') as f:\n",
    "                pickle.dump({\"tree\": tree, \"features\": feat_names, \"feat_idx\": feat_idx.tolist()}, f)\n",
    "\n",
    "            if (ti + 1) % 5 == 0:\n",
    "                print(\"    Tree %d/%d, %d candidates\" % (ti + 1, CFG.n_trees, len(tree_strats)))\n",
    "\n",
    "        tc = pd.DataFrame(tree_strats)\n",
    "        tc.to_parquet(save_path)\n",
    "        all_candidates_list.append(tc)\n",
    "\n",
    "        elapsed = time.time() - t0\n",
    "        tracker.mark_completed(STEP, {\"n\": len(tc), \"time_sec\": elapsed})\n",
    "        print(\"  Tree candidates %s/%s: %d (%.0fs)\" % (market, horizon_tag, len(tc), elapsed))\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "for market in CFG.markets:\n",
    "    if market not in feature_panels:\n",
    "        continue\n",
    "    fp = feature_panels[market]\n",
    "    fwd_cols = sorted([c for c in fp.columns if c.startswith(\"fwd_return_\")])\n",
    "    feat_cols = [c for c in fp.columns if not c.startswith(\"fwd_return_\") and not c.endswith(\"_decile\")]\n",
    "\n",
    "    for fwd_col in fwd_cols:\n",
    "        horizon_tag = fwd_col.replace(\"fwd_return_\", \"\")\n",
    "        STEP = \"candidates_logistic_%s_%s\" % (market, horizon_tag)\n",
    "        save_path = os.path.join(CFG.candidates_dir(market), \"logistic_%s.parquet\" % horizon_tag)\n",
    "\n",
    "        if tracker.is_completed(STEP):\n",
    "            logger.info(\"[SKIP] %s\" % STEP)\n",
    "            all_candidates_list.append(pd.read_parquet(save_path))\n",
    "            continue\n",
    "\n",
    "        logger.info(\"[RUN] %s\" % STEP)\n",
    "        t0 = time.time()\n",
    "        valid = fp.dropna(subset=[fwd_col] + feat_cols).copy()\n",
    "        X = valid[feat_cols].values.astype(np.float32)\n",
    "        y = (valid[fwd_col].values > 0).astype(int)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "        dates_sorted = valid.index.get_level_values(0)\n",
    "        split_date = dates_sorted.unique()[int(len(dates_sorted.unique()) * 0.8)]\n",
    "        train_mask = dates_sorted <= split_date\n",
    "\n",
    "        lr = LogisticRegression(max_iter=1000, C=0.1, penalty='l2',\n",
    "                                random_state=CFG.seed, solver='lbfgs')\n",
    "        lr.fit(X_scaled[train_mask], y[train_mask])\n",
    "\n",
    "        proba = lr.predict_proba(X_scaled)[:, 1]\n",
    "        quintile_edges = np.percentile(proba, [0, 20, 40, 60, 80, 100]).tolist()\n",
    "\n",
    "        logistic_strats = []\n",
    "        for q in range(5):\n",
    "            if q == 4:\n",
    "                qm = proba >= quintile_edges[q]\n",
    "            else:\n",
    "                qm = (proba >= quintile_edges[q]) & (proba < quintile_edges[q + 1])\n",
    "            nt = int(qm.sum())\n",
    "            if nt < CFG.min_sample_size:\n",
    "                continue\n",
    "            ret = valid[fwd_col].values[qm]\n",
    "            logistic_strats.append({\n",
    "                \"strategy_id\": \"%s_%s_logistic_q%d\" % (market, horizon_tag, q + 1),\n",
    "                \"market\": market, \"horizon\": horizon_tag,\n",
    "                \"type\": \"logistic_rank\",\n",
    "                \"features\": \"all_features\",\n",
    "                \"condition\": \"logistic_quintile_%d\" % (q + 1),\n",
    "                \"n_trades\": nt,\n",
    "                \"mean_return\": float(np.nanmean(ret)),\n",
    "                \"win_rate\": float((ret > 0).mean()),\n",
    "            })\n",
    "\n",
    "        mp = os.path.join(CFG.candidates_dir(market), \"logistic_model_%s.pkl\" % horizon_tag)\n",
    "        with open(mp, 'wb') as f:\n",
    "            pickle.dump({\"model\": lr, \"scaler\": scaler, \"features\": feat_cols,\n",
    "                         \"quintile_edges\": quintile_edges}, f)\n",
    "\n",
    "        lc = pd.DataFrame(logistic_strats)\n",
    "        lc.to_parquet(save_path)\n",
    "        all_candidates_list.append(lc)\n",
    "\n",
    "        elapsed = time.time() - t0\n",
    "        tracker.mark_completed(STEP, {\"n\": len(lc), \"time_sec\": elapsed})\n",
    "        print(\"  Logistic candidates %s/%s: %d (%.0fs)\" % (market, horizon_tag, len(lc), elapsed))\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_candidates = pd.concat(all_candidates_list, ignore_index=True) if all_candidates_list else pd.DataFrame()\n",
    "\n",
    "print(\"=== All Candidates ===\")\n",
    "print(\"Total: %d\" % len(all_candidates))\n",
    "if len(all_candidates) > 0:\n",
    "    print(\"\\nBy market:\")\n",
    "    print(all_candidates[\"market\"].value_counts())\n",
    "    print(\"\\nBy horizon:\")\n",
    "    print(all_candidates[\"horizon\"].value_counts())\n",
    "    print(\"\\nBy type:\")\n",
    "    print(all_candidates[\"type\"].value_counts())\n",
    "    print(\"\\nTop 10 by mean return:\")\n",
    "    print(all_candidates.nlargest(10, \"mean_return\")[\n",
    "        [\"strategy_id\", \"market\", \"horizon\", \"type\", \"n_trades\", \"mean_return\", \"win_rate\"]\n",
    "    ].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Edge Evaluation Engine\n",
    "\n",
    "For each candidate, compute net-of-cost metrics: win rate, Sharpe, max drawdown,\n",
    "lift, expectancy. Results saved incrementally — already-evaluated strategies\n",
    "are skipped on resume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_strategy_edge(returns):\n",
    "    \"\"\"Compute edge metrics for a strategy return array.\"\"\"\n",
    "    returns = returns[~np.isnan(returns)]\n",
    "    n = len(returns)\n",
    "    if n < 30:\n",
    "        return None\n",
    "    mean_ret = float(np.mean(returns))\n",
    "    std_ret = float(np.std(returns, ddof=1))\n",
    "    win_rate = float((returns > 0).mean())\n",
    "    avg_win = float(np.mean(returns[returns > 0])) if (returns > 0).any() else 0.0\n",
    "    avg_loss = float(np.mean(returns[returns <= 0])) if (returns <= 0).any() else 0.0\n",
    "    fwd_days_approx = 21\n",
    "    sharpe = (mean_ret / std_ret * np.sqrt(252 / max(1, fwd_days_approx))) if std_ret > 1e-8 else 0.0\n",
    "    cum = np.cumsum(returns)\n",
    "    running_max = np.maximum.accumulate(cum)\n",
    "    max_dd = float(np.min(cum - running_max)) if len(cum) > 0 else 0.0\n",
    "    expectancy = avg_win * win_rate + avg_loss * (1 - win_rate)\n",
    "    return {\n",
    "        \"n_trades\": n, \"mean_return\": mean_ret, \"std_return\": std_ret,\n",
    "        \"win_rate\": win_rate, \"avg_win\": avg_win, \"avg_loss\": avg_loss,\n",
    "        \"sharpe\": float(sharpe), \"max_drawdown\": max_dd,\n",
    "        \"expectancy\": float(expectancy),\n",
    "    }\n",
    "\n",
    "\n",
    "def build_mask(data, stype, cand_row, sid, market, horizon_tag, feat_cols_list):\n",
    "    \"\"\"Build boolean mask for a strategy on the given data slice.\"\"\"\n",
    "    if stype == \"single_decile\":\n",
    "        col = cand_row[\"features\"]\n",
    "        dv = int(cand_row[\"condition\"].split(\"== \")[1])\n",
    "        return data[col] == dv\n",
    "    elif stype == \"combo_decile\":\n",
    "        parts = cand_row[\"condition\"].split(\" AND \")\n",
    "        ca, va = parts[0].split(\"==\")\n",
    "        cb, vb = parts[1].split(\"==\")\n",
    "        return (data[ca.strip()] == int(va)) & (data[cb.strip()] == int(vb))\n",
    "    elif stype == \"decision_tree\":\n",
    "        parts = sid.split(\"_\")\n",
    "        # format: {market}_{horizon}_tree_{idx}_leaf_{id}\n",
    "        tree_num = parts[2] if parts[2].isdigit() else None\n",
    "        leaf_id = None\n",
    "        for i, p in enumerate(parts):\n",
    "            if p == \"tree\":\n",
    "                tree_num = parts[i + 1]\n",
    "            if p == \"leaf\":\n",
    "                leaf_id = int(parts[i + 1])\n",
    "        tp = os.path.join(CFG.candidates_dir(market), \"tree_model_%s_%s.pkl\" % (horizon_tag, tree_num))\n",
    "        with open(tp, 'rb') as f:\n",
    "            td = pickle.load(f)\n",
    "        missing = [fn for fn in td[\"features\"] if fn not in feat_cols_list]\n",
    "        if missing:\n",
    "            raise ValueError(\"Missing features: %s\" % missing)\n",
    "        fi = [feat_cols_list.index(fn) for fn in td[\"features\"]]\n",
    "        X = data[feat_cols_list].values[:, fi].astype(np.float32)\n",
    "        np.nan_to_num(X, copy=False)\n",
    "        return pd.Series(td[\"tree\"].apply(X) == leaf_id, index=data.index)\n",
    "    elif stype == \"logistic_rank\":\n",
    "        q_num = int(sid.split(\"_q\")[1])\n",
    "        mp = os.path.join(CFG.candidates_dir(market), \"logistic_model_%s.pkl\" % horizon_tag)\n",
    "        with open(mp, 'rb') as f:\n",
    "            ld = pickle.load(f)\n",
    "        X = data[feat_cols_list].values.astype(np.float32)\n",
    "        np.nan_to_num(X, copy=False)\n",
    "        proba = ld[\"model\"].predict_proba(ld[\"scaler\"].transform(X))[:, 1]\n",
    "        edges = ld[\"quintile_edges\"]\n",
    "        if q_num == 5:\n",
    "            return pd.Series(proba >= edges[q_num - 1], index=data.index)\n",
    "        return pd.Series((proba >= edges[q_num - 1]) & (proba < edges[q_num]), index=data.index)\n",
    "    return pd.Series(False, index=data.index)\n",
    "\n",
    "\n",
    "print(\"Edge evaluation & mask builder defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_EDGE_COLS = [\"strategy_id\", \"market\", \"horizon\", \"type\", \"n_trades\",\n",
    "              \"mean_return\", \"std_return\", \"win_rate\", \"avg_win\", \"avg_loss\",\n",
    "              \"sharpe\", \"max_drawdown\", \"expectancy\", \"lift\"]\n",
    "\n",
    "all_edge_results = []\n",
    "\n",
    "for market in CFG.markets:\n",
    "    if market not in feature_panels:\n",
    "        continue\n",
    "    fp = feature_panels[market]\n",
    "    fwd_cols = sorted([c for c in fp.columns if c.startswith(\"fwd_return_\")])\n",
    "    feat_cols = [c for c in fp.columns if not c.startswith(\"fwd_return_\") and not c.endswith(\"_decile\")]\n",
    "\n",
    "    for fwd_col in fwd_cols:\n",
    "        horizon_tag = fwd_col.replace(\"fwd_return_\", \"\")\n",
    "        STEP = \"edge_eval_%s_%s\" % (market, horizon_tag)\n",
    "        eval_path = os.path.join(CFG.evaluation_dir(market), \"edge_%s.parquet\" % horizon_tag)\n",
    "\n",
    "        if tracker.is_completed(STEP):\n",
    "            logger.info(\"[SKIP] %s\" % STEP)\n",
    "            all_edge_results.append(pd.read_parquet(eval_path))\n",
    "            continue\n",
    "\n",
    "        logger.info(\"[RUN] %s\" % STEP)\n",
    "        t0 = time.time()\n",
    "        valid = fp.dropna(subset=[fwd_col] + feat_cols).copy()\n",
    "        unconditional_mean = float(valid[fwd_col].mean())\n",
    "\n",
    "        # Filter candidates for this market/horizon\n",
    "        mh_cands = all_candidates[\n",
    "            (all_candidates[\"market\"] == market) & (all_candidates[\"horizon\"] == horizon_tag)\n",
    "        ]\n",
    "        logger.info(\"Evaluating %d candidates for %s/%s\" % (len(mh_cands), market, horizon_tag))\n",
    "\n",
    "        # Incremental resume\n",
    "        existing_ids = set()\n",
    "        eval_rows = []\n",
    "        if os.path.exists(eval_path):\n",
    "            edf = pd.read_parquet(eval_path)\n",
    "            existing_ids = set(edf[\"strategy_id\"].values)\n",
    "            eval_rows = edf.to_dict('records')\n",
    "\n",
    "        to_eval = mh_cands[~mh_cands[\"strategy_id\"].isin(existing_ids)]\n",
    "\n",
    "        for idx, (_, row) in enumerate(to_eval.iterrows()):\n",
    "            sid = row[\"strategy_id\"]\n",
    "            stype = row[\"type\"]\n",
    "            try:\n",
    "                mask = build_mask(valid, stype, row, sid, market, horizon_tag, feat_cols)\n",
    "                returns = valid.loc[mask, fwd_col].values\n",
    "                edge = evaluate_strategy_edge(returns)\n",
    "                if edge is None:\n",
    "                    continue\n",
    "                edge[\"lift\"] = edge[\"mean_return\"] - unconditional_mean\n",
    "                edge[\"strategy_id\"] = sid\n",
    "                edge[\"market\"] = market\n",
    "                edge[\"horizon\"] = horizon_tag\n",
    "                edge[\"type\"] = stype\n",
    "                eval_rows.append(edge)\n",
    "            except Exception as e:\n",
    "                logger.warning(\"Eval err %s: %s\" % (sid, str(e)[:60]))\n",
    "\n",
    "            if (idx + 1) % CFG.gc_every_n_candidates == 0:\n",
    "                pd.DataFrame(eval_rows).to_parquet(eval_path)\n",
    "                gc.collect()\n",
    "                print(\"    %d/%d evaluated, checkpoint\" % (idx + 1, len(to_eval)))\n",
    "\n",
    "        edf = pd.DataFrame(eval_rows) if eval_rows else pd.DataFrame(columns=_EDGE_COLS)\n",
    "        edf.to_parquet(eval_path)\n",
    "        all_edge_results.append(edf)\n",
    "\n",
    "        elapsed = time.time() - t0\n",
    "        tracker.mark_completed(STEP, {\"n\": len(edf), \"time_sec\": elapsed})\n",
    "        print(\"  Edge eval %s/%s: %d (%.0fs)\" % (market, horizon_tag, len(edf), elapsed))\n",
    "        gc.collect()\n",
    "\n",
    "edge_results = pd.concat(all_edge_results, ignore_index=True) if all_edge_results else pd.DataFrame(columns=_EDGE_COLS)\n",
    "print(\"\\nTotal edge results: %d\" % len(edge_results))\n",
    "if len(edge_results) > 0:\n",
    "    print(edge_results[[\"mean_return\", \"win_rate\", \"sharpe\", \"lift\"]].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Walk-Forward Validation\n",
    "\n",
    "Rolling walk-forward with embargo. Pre-filters to top candidates per market/horizon.\n",
    "Computes per-fold turnover estimates.\n",
    "\n",
    "**Controls:**\n",
    "- `wf_min_folds` enforcement\n",
    "- Embargo days between train and test\n",
    "- Per-fold checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "_WF_COLS = [\"strategy_id\", \"market\", \"horizon\", \"fold_idx\", \"n_trades\",\n",
    "            \"mean_return\", \"std_return\", \"win_rate\", \"avg_win\", \"avg_loss\",\n",
    "            \"sharpe\", \"max_drawdown\", \"expectancy\", \"turnover\",\n",
    "            \"test_start\", \"test_end\"]\n",
    "\n",
    "all_wf_results = []\n",
    "\n",
    "for market in CFG.markets:\n",
    "    if market not in feature_panels:\n",
    "        continue\n",
    "    fp = feature_panels[market]\n",
    "    fwd_cols = sorted([c for c in fp.columns if c.startswith(\"fwd_return_\")])\n",
    "    feat_cols = [c for c in fp.columns if not c.startswith(\"fwd_return_\") and not c.endswith(\"_decile\")]\n",
    "\n",
    "    for fwd_col in fwd_cols:\n",
    "        horizon_tag = fwd_col.replace(\"fwd_return_\", \"\")\n",
    "        STEP = \"walkforward_%s_%s\" % (market, horizon_tag)\n",
    "        wf_path = os.path.join(CFG.walkforward_dir(market), \"wf_%s.parquet\" % horizon_tag)\n",
    "\n",
    "        if tracker.is_completed(STEP):\n",
    "            logger.info(\"[SKIP] %s\" % STEP)\n",
    "            all_wf_results.append(pd.read_parquet(wf_path))\n",
    "            continue\n",
    "\n",
    "        logger.info(\"[RUN] %s\" % STEP)\n",
    "        t0 = time.time()\n",
    "        valid = fp.dropna(subset=[fwd_col]).copy()\n",
    "\n",
    "        # Pre-filter: top candidates for this market/horizon\n",
    "        mh_edge = edge_results[\n",
    "            (edge_results[\"market\"] == market) & (edge_results[\"horizon\"] == horizon_tag)\n",
    "        ]\n",
    "        if len(mh_edge) > 200:\n",
    "            top_ids = mh_edge.nlargest(200, \"sharpe\")[\"strategy_id\"].tolist()\n",
    "        elif len(mh_edge) > 0:\n",
    "            top_ids = mh_edge[mh_edge[\"sharpe\"] > 0.2][\"strategy_id\"].tolist()\n",
    "            if len(top_ids) < 20:\n",
    "                top_ids = mh_edge.nlargest(min(50, len(mh_edge)), \"sharpe\")[\"strategy_id\"].tolist()\n",
    "        else:\n",
    "            top_ids = []\n",
    "\n",
    "        if not top_ids:\n",
    "            logger.info(\"No candidates for WF %s/%s\" % (market, horizon_tag))\n",
    "            tracker.mark_completed(STEP, {\"n\": 0})\n",
    "            continue\n",
    "\n",
    "        logger.info(\"WF on %d candidates for %s/%s\" % (len(top_ids), market, horizon_tag))\n",
    "\n",
    "        all_dates = valid.index.get_level_values(0).unique().sort_values()\n",
    "        min_date, max_date = all_dates.min(), all_dates.max()\n",
    "\n",
    "        # Generate folds\n",
    "        folds = []\n",
    "        ts = min_date\n",
    "        while True:\n",
    "            te = ts + relativedelta(years=CFG.wf_train_years)\n",
    "            vs = te + pd.Timedelta(days=CFG.wf_embargo_days)\n",
    "            ve = vs + relativedelta(months=CFG.wf_test_months)\n",
    "            if ve > max_date:\n",
    "                break\n",
    "            folds.append((ts, te, vs, ve))\n",
    "            ts += relativedelta(months=CFG.wf_step_months)\n",
    "\n",
    "        if len(folds) < CFG.wf_min_folds:\n",
    "            logger.warning(\"Only %d folds for %s/%s (min %d) — skipping\" % (\n",
    "                len(folds), market, horizon_tag, CFG.wf_min_folds))\n",
    "            tracker.mark_completed(STEP, {\"n\": 0, \"reason\": \"insufficient_folds\"})\n",
    "            continue\n",
    "\n",
    "        logger.info(\"  %d folds\" % len(folds))\n",
    "\n",
    "        # Partial resume\n",
    "        partial_path = os.path.join(CFG.walkforward_dir(market), \"wf_partial_%s.parquet\" % horizon_tag)\n",
    "        wf_rows = []\n",
    "        completed_keys = set()\n",
    "        if os.path.exists(partial_path):\n",
    "            pdf = pd.read_parquet(partial_path)\n",
    "            wf_rows = pdf.to_dict('records')\n",
    "            completed_keys = set(zip(pdf[\"strategy_id\"], pdf[\"fold_idx\"].astype(int)))\n",
    "\n",
    "        mh_cands = all_candidates[\n",
    "            (all_candidates[\"market\"] == market) & (all_candidates[\"horizon\"] == horizon_tag)\n",
    "        ]\n",
    "\n",
    "        for fi, (train_s, train_e, test_s, test_e) in enumerate(folds):\n",
    "            d_idx = valid.index.get_level_values(0)\n",
    "            train_data = valid[(d_idx >= train_s) & (d_idx < train_e)]\n",
    "            test_data = valid[(d_idx >= test_s) & (d_idx < test_e)]\n",
    "\n",
    "            for sid in top_ids:\n",
    "                if (sid, fi) in completed_keys:\n",
    "                    continue\n",
    "                try:\n",
    "                    cr = mh_cands[mh_cands[\"strategy_id\"] == sid].iloc[0]\n",
    "                    stype = cr[\"type\"]\n",
    "                    test_mask_strat = build_mask(test_data, stype, cr, sid, market, horizon_tag, feat_cols)\n",
    "                    test_returns = test_data.loc[test_mask_strat, fwd_col].values\n",
    "                    if len(test_returns) < 20:\n",
    "                        continue\n",
    "                    edge = evaluate_strategy_edge(test_returns)\n",
    "                    if edge is None:\n",
    "                        continue\n",
    "\n",
    "                    # Turnover estimation within fold\n",
    "                    test_dates = test_data.index.get_level_values(0).unique().sort_values()\n",
    "                    rebal_dates = test_dates[::21]\n",
    "                    turnovers = []\n",
    "                    prev_set = None\n",
    "                    for dt in rebal_dates:\n",
    "                        try:\n",
    "                            dt_ix = test_data.index.get_level_values(0) == dt\n",
    "                            day_sig = test_mask_strat[dt_ix]\n",
    "                            sel = set(test_data.index[dt_ix][day_sig].get_level_values(1))\n",
    "                        except Exception:\n",
    "                            continue\n",
    "                        if prev_set is not None and (prev_set or sel):\n",
    "                            u = len(prev_set | sel)\n",
    "                            if u > 0:\n",
    "                                turnovers.append(len(prev_set ^ sel) / u)\n",
    "                        prev_set = sel\n",
    "                    ann_turnover = float(np.mean(turnovers) * 252 / 21) if turnovers else 0.0\n",
    "\n",
    "                    edge[\"strategy_id\"] = sid\n",
    "                    edge[\"market\"] = market\n",
    "                    edge[\"horizon\"] = horizon_tag\n",
    "                    edge[\"fold_idx\"] = fi\n",
    "                    edge[\"turnover\"] = ann_turnover\n",
    "                    edge[\"test_start\"] = str(test_s.date())\n",
    "                    edge[\"test_end\"] = str(test_e.date())\n",
    "                    wf_rows.append(edge)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            if wf_rows:\n",
    "                pd.DataFrame(wf_rows).to_parquet(partial_path)\n",
    "            print(\"    Fold %d/%d done\" % (fi + 1, len(folds)))\n",
    "\n",
    "        wdf = pd.DataFrame(wf_rows) if wf_rows else pd.DataFrame(columns=_WF_COLS)\n",
    "        wdf.to_parquet(wf_path)\n",
    "        all_wf_results.append(wdf)\n",
    "\n",
    "        if os.path.exists(partial_path):\n",
    "            os.remove(partial_path)\n",
    "\n",
    "        elapsed = time.time() - t0\n",
    "        tracker.mark_completed(STEP, {\"n\": len(wdf), \"folds\": len(folds), \"time_sec\": elapsed})\n",
    "        print(\"  WF %s/%s: %d results, %d folds (%.0fs)\" % (\n",
    "            market, horizon_tag, len(wdf), len(folds), elapsed))\n",
    "        gc.collect()\n",
    "\n",
    "wf_results = pd.concat(all_wf_results, ignore_index=True) if all_wf_results else pd.DataFrame(columns=_WF_COLS)\n",
    "print(\"\\nTotal WF results: %d\" % len(wf_results))\n",
    "if len(wf_results) > 0:\n",
    "    print(\"Strategies: %d | Folds: %d\" % (\n",
    "        wf_results[\"strategy_id\"].nunique(), wf_results[\"fold_idx\"].nunique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Overfitting Control (Strengthened)\n",
    "\n",
    "Filters strategies using:\n",
    "- **Stability** >= 0.5 (fraction of folds with positive return)\n",
    "- **Sharpe** >= 0.5 across folds\n",
    "- **Win rate** >= 0.52\n",
    "- **Sign-flip rejection**: penalise strategies whose return sign flips across folds\n",
    "- **Bootstrap CI** with `min_samples` guard\n",
    "- **FDR correction** (Benjamini-Hochberg) for multiple testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP = \"overfitting_control\"\n",
    "filtered_path = os.path.join(CFG.global_eval_dir, \"filtered_strategies.parquet\")\n",
    "\n",
    "if tracker.is_completed(STEP):\n",
    "    logger.info(\"[SKIP] %s\" % STEP)\n",
    "    filtered_strategies = pd.read_parquet(filtered_path)\n",
    "    print(\"Loaded %d filtered strategies.\" % len(filtered_strategies))\n",
    "else:\n",
    "    logger.info(\"[RUN] %s\" % STEP)\n",
    "    t0 = time.time()\n",
    "\n",
    "    if len(wf_results) == 0:\n",
    "        print(\"No walk-forward results. Skipping.\")\n",
    "        filtered_strategies = pd.DataFrame()\n",
    "        tracker.mark_completed(STEP, {\"n\": 0})\n",
    "    else:\n",
    "        strategy_stats = []\n",
    "        for sid, group in wf_results.groupby(\"strategy_id\"):\n",
    "            n_folds = len(group)\n",
    "            if n_folds < 2:\n",
    "                continue\n",
    "            fold_returns = group[\"mean_return\"].values\n",
    "            fold_sharpes = group[\"sharpe\"].values\n",
    "            fold_win_rates = group[\"win_rate\"].values\n",
    "            fold_turnovers = group[\"turnover\"].values if \"turnover\" in group.columns else np.zeros(n_folds)\n",
    "\n",
    "            stability = float((fold_returns > 0).mean())\n",
    "            mean_sharpe = float(np.mean(fold_sharpes))\n",
    "            sharpe_std = float(np.std(fold_sharpes, ddof=1))\n",
    "            mean_win_rate = float(np.mean(fold_win_rates))\n",
    "            min_win_rate = float(np.min(fold_win_rates))\n",
    "            sign_flips = int(np.sum(np.diff(np.sign(fold_returns)) != 0))\n",
    "            mean_turnover = float(np.mean(fold_turnovers))\n",
    "\n",
    "            all_n = group[\"n_trades\"].values\n",
    "            total_trades = int(all_n.sum())\n",
    "\n",
    "            # Bootstrap CI for win rate\n",
    "            if total_trades >= CFG.bootstrap_min_samples:\n",
    "                total_wins = int((fold_win_rates * all_n).sum())\n",
    "                bs = np.random.binomial(total_trades, total_wins / max(1, total_trades), CFG.bootstrap_n)\n",
    "                bs_wr = bs / total_trades\n",
    "                ci_low = float(np.percentile(bs_wr, (1 - CFG.bootstrap_ci) / 2 * 100))\n",
    "                ci_high = float(np.percentile(bs_wr, (1 + CFG.bootstrap_ci) / 2 * 100))\n",
    "            else:\n",
    "                ci_low, ci_high = 0.0, 1.0\n",
    "\n",
    "            # Compute a p-value for win rate > 0.5 (binomial test)\n",
    "            from scipy import stats as sp_stats\n",
    "            if total_trades > 0:\n",
    "                total_wins_int = int((fold_win_rates * all_n).sum())\n",
    "                pval = sp_stats.binomtest(total_wins_int, total_trades, 0.5, alternative='greater').pvalue\n",
    "            else:\n",
    "                pval = 1.0\n",
    "\n",
    "            mkt = group[\"market\"].iloc[0] if \"market\" in group.columns else \"\"\n",
    "            hor = group[\"horizon\"].iloc[0] if \"horizon\" in group.columns else \"\"\n",
    "\n",
    "            strategy_stats.append({\n",
    "                \"strategy_id\": sid, \"market\": mkt, \"horizon\": hor,\n",
    "                \"n_folds\": n_folds, \"stability\": stability,\n",
    "                \"mean_sharpe\": mean_sharpe, \"sharpe_std\": sharpe_std,\n",
    "                \"mean_win_rate\": mean_win_rate, \"min_win_rate\": min_win_rate,\n",
    "                \"sign_flips\": sign_flips, \"total_trades\": total_trades,\n",
    "                \"mean_turnover\": mean_turnover,\n",
    "                \"wr_ci_low\": ci_low, \"wr_ci_high\": ci_high,\n",
    "                \"pval_wr\": pval,\n",
    "                \"mean_return\": float(np.mean(fold_returns)),\n",
    "            })\n",
    "\n",
    "        stats_df = pd.DataFrame(strategy_stats)\n",
    "\n",
    "        # FDR correction\n",
    "        if CFG.apply_multiple_testing_correction and len(stats_df) > 0 and \"pval_wr\" in stats_df.columns:\n",
    "            from statsmodels.stats.multitest import multipletests\n",
    "            reject, pvals_corr, _, _ = multipletests(\n",
    "                stats_df[\"pval_wr\"].fillna(1.0).values,\n",
    "                alpha=0.05, method='fdr_bh',\n",
    "            )\n",
    "            stats_df[\"pval_fdr\"] = pvals_corr\n",
    "            stats_df[\"fdr_reject\"] = reject\n",
    "            logger.info(\"FDR correction: %d / %d rejected null\" % (reject.sum(), len(reject)))\n",
    "        else:\n",
    "            stats_df[\"pval_fdr\"] = stats_df.get(\"pval_wr\", 1.0)\n",
    "            stats_df[\"fdr_reject\"] = True\n",
    "\n",
    "        # Apply filters\n",
    "        n_before = len(stats_df)\n",
    "        mask = (\n",
    "            (stats_df[\"stability\"] >= CFG.min_stability)\n",
    "            & (stats_df[\"mean_sharpe\"] >= CFG.min_sharpe)\n",
    "            & (stats_df[\"mean_win_rate\"] >= CFG.min_win_rate)\n",
    "            & (stats_df[\"wr_ci_low\"] >= 0.48)\n",
    "            & (stats_df[\"fdr_reject\"] == True)\n",
    "        )\n",
    "        filtered_strategies = stats_df[mask].copy()\n",
    "        filtered_strategies = filtered_strategies.sort_values(\"mean_sharpe\", ascending=False)\n",
    "\n",
    "        print(\"\\nOverfitting filters:\")\n",
    "        print(\"  Before: %d\" % n_before)\n",
    "        print(\"  Stability >= %.1f: %d\" % (CFG.min_stability, (stats_df[\"stability\"] >= CFG.min_stability).sum()))\n",
    "        print(\"  Sharpe >= %.1f: %d\" % (CFG.min_sharpe, (stats_df[\"mean_sharpe\"] >= CFG.min_sharpe).sum()))\n",
    "        print(\"  Win rate >= %.2f: %d\" % (CFG.min_win_rate, (stats_df[\"mean_win_rate\"] >= CFG.min_win_rate).sum()))\n",
    "        print(\"  CI lower >= 0.48: %d\" % (stats_df[\"wr_ci_low\"] >= 0.48).sum())\n",
    "        print(\"  FDR significant: %d\" % stats_df[\"fdr_reject\"].sum())\n",
    "        print(\"  After ALL: %d\" % len(filtered_strategies))\n",
    "\n",
    "        filtered_strategies.to_parquet(filtered_path)\n",
    "        elapsed = time.time() - t0\n",
    "        tracker.mark_completed(STEP, {\"n\": len(filtered_strategies), \"time_sec\": elapsed})\n",
    "        gc.collect()\n",
    "\n",
    "print(\"\\nFiltered strategies: %d\" % len(filtered_strategies))\n",
    "if len(filtered_strategies) > 0:\n",
    "    print(filtered_strategies[\n",
    "        [\"strategy_id\", \"market\", \"horizon\", \"stability\", \"mean_sharpe\",\n",
    "         \"mean_win_rate\", \"wr_ci_low\", \"mean_turnover\", \"total_trades\"]\n",
    "    ].head(20).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Turnover & Cost Filtering\n",
    "\n",
    "Reject strategies whose estimated annualised turnover exceeds `max_turnover`.\n",
    "Apply a turnover penalty to the remaining strategies' scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP = \"turnover_filter\"\n",
    "turnover_path = os.path.join(CFG.global_eval_dir, \"turnover_filtered.parquet\")\n",
    "\n",
    "if tracker.is_completed(STEP):\n",
    "    logger.info(\"[SKIP] %s\" % STEP)\n",
    "    turnover_filtered = pd.read_parquet(turnover_path)\n",
    "    print(\"Loaded %d turnover-filtered strategies.\" % len(turnover_filtered))\n",
    "else:\n",
    "    logger.info(\"[RUN] %s\" % STEP)\n",
    "\n",
    "    if len(filtered_strategies) == 0:\n",
    "        turnover_filtered = pd.DataFrame()\n",
    "    else:\n",
    "        before = len(filtered_strategies)\n",
    "        turnover_filtered = filtered_strategies[\n",
    "            filtered_strategies[\"mean_turnover\"] <= CFG.max_turnover\n",
    "        ].copy()\n",
    "        print(\"Turnover filter (max %.1f): %d -> %d\" % (\n",
    "            CFG.max_turnover, before, len(turnover_filtered)))\n",
    "\n",
    "    turnover_filtered.to_parquet(turnover_path) if len(turnover_filtered) > 0 else None\n",
    "    tracker.mark_completed(STEP, {\"n\": len(turnover_filtered)})\n",
    "\n",
    "print(\"Turnover-filtered: %d\" % len(turnover_filtered))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Strategy Scoring (Turnover-Aware)\n",
    "\n",
    "Composite score per strategy:\n",
    "```\n",
    "Score = w_stability * S + w_sharpe * Sh + w_lift * L + w_sample * N - penalty_turnover * T\n",
    "```\n",
    "\n",
    "Followed by correlation-based clustering to remove redundant strategies.\n",
    "Keep top `portfolio_max_strategies` per market per horizon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "STEP = \"scoring\"\n",
    "scored_path = os.path.join(CFG.global_eval_dir, \"scored_strategies.parquet\")\n",
    "\n",
    "if tracker.is_completed(STEP):\n",
    "    logger.info(\"[SKIP] %s\" % STEP)\n",
    "    scored_strategies = pd.read_parquet(scored_path)\n",
    "    print(\"Loaded %d scored strategies.\" % len(scored_strategies))\n",
    "else:\n",
    "    logger.info(\"[RUN] %s\" % STEP)\n",
    "\n",
    "    if len(turnover_filtered) == 0:\n",
    "        print(\"No strategies passed turnover filter.\")\n",
    "        scored_strategies = pd.DataFrame()\n",
    "        tracker.mark_completed(STEP, {\"n\": 0})\n",
    "    else:\n",
    "        df = turnover_filtered.copy()\n",
    "\n",
    "        def norm(s):\n",
    "            r = s.max() - s.min()\n",
    "            return (s - s.min()) / r if r > 1e-8 else pd.Series(0.5, index=s.index)\n",
    "\n",
    "        df[\"stability_norm\"] = norm(df[\"stability\"])\n",
    "        df[\"sharpe_norm\"] = norm(df[\"mean_sharpe\"])\n",
    "\n",
    "        # Lift from edge results\n",
    "        lift_map = edge_results.set_index(\"strategy_id\")[\"lift\"].to_dict() if len(edge_results) > 0 else {}\n",
    "        df[\"lift\"] = df[\"strategy_id\"].map(lift_map).fillna(0)\n",
    "        df[\"lift_norm\"] = norm(df[\"lift\"])\n",
    "        df[\"sample_score\"] = norm(np.log1p(df[\"total_trades\"]))\n",
    "\n",
    "        # Turnover penalty\n",
    "        turnover_norm = norm(df[\"mean_turnover\"]) if \"mean_turnover\" in df.columns else 0\n",
    "\n",
    "        df[\"composite_score\"] = (\n",
    "            CFG.w_stability * df[\"stability_norm\"]\n",
    "            + CFG.w_sharpe * df[\"sharpe_norm\"]\n",
    "            + CFG.w_lift * df[\"lift_norm\"]\n",
    "            + CFG.w_sample * df[\"sample_score\"]\n",
    "            - CFG.penalty_turnover * turnover_norm\n",
    "        )\n",
    "\n",
    "        # --- Correlation filter via clustering ---\n",
    "        if CFG.cluster_strategies and len(df) > 5 and len(wf_results) > 0:\n",
    "            pivot = wf_results.pivot_table(\n",
    "                values=\"mean_return\", index=\"fold_idx\",\n",
    "                columns=\"strategy_id\", aggfunc=\"first\",\n",
    "            )\n",
    "            scored_ids = df[\"strategy_id\"].tolist()\n",
    "            pivot = pivot[[c for c in pivot.columns if c in scored_ids]].dropna(axis=1, how='all').fillna(0)\n",
    "\n",
    "            if pivot.shape[1] >= 5:\n",
    "                corr = pivot.corr().values\n",
    "                distance = 1 - np.abs(corr)\n",
    "                np.fill_diagonal(distance, 0)\n",
    "                distance = np.maximum(distance, 0)\n",
    "\n",
    "                n_clust = max(3, min(CFG.portfolio_max_strategies, len(pivot.columns) // 3))\n",
    "                clustering = AgglomerativeClustering(\n",
    "                    n_clusters=n_clust, metric='precomputed', linkage='average',\n",
    "                )\n",
    "                labels = clustering.fit_predict(distance)\n",
    "                cluster_map = dict(zip(pivot.columns, labels))\n",
    "                df[\"cluster\"] = df[\"strategy_id\"].map(cluster_map)\n",
    "\n",
    "                # Keep best per cluster\n",
    "                df = df.sort_values(\"composite_score\", ascending=False)\n",
    "                best_per = df.dropna(subset=[\"cluster\"]).groupby(\"cluster\").first().reset_index(drop=True)\n",
    "                # Also keep unclustered\n",
    "                unclustered = df[df[\"cluster\"].isna()]\n",
    "                df = pd.concat([best_per, unclustered], ignore_index=True)\n",
    "                print(\"Correlation clustering: %d clusters, %d strategies kept\" % (n_clust, len(df)))\n",
    "\n",
    "        # Keep top N per market per horizon\n",
    "        final_parts = []\n",
    "        for (m, h), grp in df.groupby([\"market\", \"horizon\"]):\n",
    "            top = grp.nlargest(CFG.portfolio_max_strategies, \"composite_score\")\n",
    "            final_parts.append(top)\n",
    "\n",
    "        scored_strategies = pd.concat(final_parts, ignore_index=True) if final_parts else pd.DataFrame()\n",
    "        scored_strategies = scored_strategies.sort_values(\"composite_score\", ascending=False).reset_index(drop=True)\n",
    "        scored_strategies[\"rank\"] = range(1, len(scored_strategies) + 1)\n",
    "        scored_strategies.to_parquet(scored_path)\n",
    "\n",
    "        tracker.mark_completed(STEP, {\"n\": len(scored_strategies)})\n",
    "\n",
    "print(\"\\n=== SCORED STRATEGIES ===\")\n",
    "if len(scored_strategies) > 0:\n",
    "    print(scored_strategies[\n",
    "        [\"rank\", \"strategy_id\", \"market\", \"horizon\", \"composite_score\",\n",
    "         \"stability\", \"mean_sharpe\", \"mean_win_rate\", \"mean_turnover\"]\n",
    "    ].head(30).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Regime Analysis\n",
    "\n",
    "Split walk-forward results into **bull** and **bear** regimes based on market\n",
    "momentum during each test period. Reject strategies where the min-regime Sharpe\n",
    "is less than `min_regime_performance_ratio` times the max-regime Sharpe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP = \"regime_analysis\"\n",
    "regime_path = os.path.join(CFG.global_eval_dir, \"regime_results.parquet\")\n",
    "\n",
    "if tracker.is_completed(STEP):\n",
    "    logger.info(\"[SKIP] %s\" % STEP)\n",
    "    regime_filtered = pd.read_parquet(regime_path)\n",
    "    print(\"Loaded %d regime-filtered strategies.\" % len(regime_filtered))\n",
    "else:\n",
    "    logger.info(\"[RUN] %s\" % STEP)\n",
    "\n",
    "    if not CFG.evaluate_by_regime or len(scored_strategies) == 0 or len(wf_results) == 0:\n",
    "        regime_filtered = scored_strategies.copy() if len(scored_strategies) > 0 else pd.DataFrame()\n",
    "        if len(regime_filtered) > 0:\n",
    "            regime_filtered.to_parquet(regime_path)\n",
    "        tracker.mark_completed(STEP, {\"n\": len(regime_filtered), \"skipped\": True})\n",
    "    else:\n",
    "        # Classify each WF fold as bull or bear\n",
    "        wf_with_regime = wf_results.copy()\n",
    "        wf_with_regime[\"regime\"] = \"unknown\"\n",
    "\n",
    "        for idx, row in wf_with_regime.iterrows():\n",
    "            mkt = row.get(\"market\", \"US\")\n",
    "            ts = pd.Timestamp(row[\"test_start\"])\n",
    "            te = pd.Timestamp(row[\"test_end\"])\n",
    "            mkt_idx = market_indices.get(mkt, pd.DataFrame())\n",
    "            if \"close\" not in mkt_idx.columns or mkt_idx.empty:\n",
    "                continue\n",
    "            mc = mkt_idx[\"close\"]\n",
    "            period = mc.loc[ts:te]\n",
    "            if len(period) >= 2:\n",
    "                ret = (period.iloc[-1] / period.iloc[0]) - 1\n",
    "                wf_with_regime.at[idx, \"regime\"] = \"bull\" if ret > 0 else \"bear\"\n",
    "\n",
    "        # Per-strategy regime performance\n",
    "        regime_stats = []\n",
    "        scored_ids = set(scored_strategies[\"strategy_id\"])\n",
    "        for sid, grp in wf_with_regime[wf_with_regime[\"strategy_id\"].isin(scored_ids)].groupby(\"strategy_id\"):\n",
    "            bull = grp[grp[\"regime\"] == \"bull\"][\"sharpe\"]\n",
    "            bear = grp[grp[\"regime\"] == \"bear\"][\"sharpe\"]\n",
    "            bull_mean = float(bull.mean()) if len(bull) > 0 else 0.0\n",
    "            bear_mean = float(bear.mean()) if len(bear) > 0 else 0.0\n",
    "            max_regime = max(abs(bull_mean), abs(bear_mean))\n",
    "            min_regime = min(bull_mean, bear_mean)\n",
    "            ratio = min_regime / max_regime if max_regime > 1e-8 else 0.0\n",
    "            regime_stats.append({\n",
    "                \"strategy_id\": sid,\n",
    "                \"bull_sharpe\": bull_mean,\n",
    "                \"bear_sharpe\": bear_mean,\n",
    "                \"regime_ratio\": ratio,\n",
    "            })\n",
    "\n",
    "        rdf = pd.DataFrame(regime_stats)\n",
    "        # Merge and filter\n",
    "        merged = scored_strategies.merge(rdf, on=\"strategy_id\", how=\"left\")\n",
    "        merged[\"regime_ratio\"] = merged[\"regime_ratio\"].fillna(0)\n",
    "\n",
    "        before = len(merged)\n",
    "        regime_filtered = merged[merged[\"regime_ratio\"] >= CFG.min_regime_performance_ratio].copy()\n",
    "        # If too aggressive, keep all\n",
    "        if len(regime_filtered) == 0 and len(merged) > 0:\n",
    "            logger.warning(\"Regime filter removed all strategies — relaxing to keep top half\")\n",
    "            merged = merged.sort_values(\"regime_ratio\", ascending=False)\n",
    "            regime_filtered = merged.head(max(1, len(merged) // 2)).copy()\n",
    "\n",
    "        print(\"Regime filter: %d -> %d\" % (before, len(regime_filtered)))\n",
    "        regime_filtered.to_parquet(regime_path)\n",
    "        tracker.mark_completed(STEP, {\"n\": len(regime_filtered)})\n",
    "\n",
    "print(\"Regime-filtered strategies: %d\" % len(regime_filtered))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Portfolio-Level Evaluation\n",
    "\n",
    "- Per market: equal-weight top strategies\n",
    "- Cross-market combination with FX-adjusted returns\n",
    "- Diversification benefit: combined Sharpe vs average individual Sharpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP = \"portfolio_eval\"\n",
    "port_path = os.path.join(CFG.global_eval_dir, \"portfolio_results.json\")\n",
    "\n",
    "if tracker.is_completed(STEP):\n",
    "    logger.info(\"[SKIP] %s\" % STEP)\n",
    "    with open(port_path, 'r') as f:\n",
    "        portfolio_results = json.load(f)\n",
    "    print(\"Portfolio results loaded.\")\n",
    "else:\n",
    "    logger.info(\"[RUN] %s\" % STEP)\n",
    "    portfolio_results = {\"per_market\": {}, \"cross_market\": {}}\n",
    "\n",
    "    final_ids = regime_filtered[\"strategy_id\"].tolist() if len(regime_filtered) > 0 else []\n",
    "\n",
    "    # Per-market portfolio\n",
    "    for mkt in CFG.markets:\n",
    "        mkt_ids = [s for s in final_ids if s.startswith(mkt + \"_\")]\n",
    "        if not mkt_ids:\n",
    "            continue\n",
    "        mkt_wf = wf_results[wf_results[\"strategy_id\"].isin(mkt_ids)]\n",
    "        if mkt_wf.empty:\n",
    "            continue\n",
    "        port_ret = mkt_wf.groupby(\"fold_idx\")[\"mean_return\"].mean()\n",
    "        port_sharpe = float(\n",
    "            port_ret.mean() / port_ret.std() * np.sqrt(252 / 21)\n",
    "        ) if port_ret.std() > 1e-8 else 0.0\n",
    "        port_total = float(port_ret.sum())\n",
    "        avg_indiv_sharpe = float(\n",
    "            mkt_wf.groupby(\"strategy_id\")[\"sharpe\"].mean().mean()\n",
    "        )\n",
    "        diversification = port_sharpe / avg_indiv_sharpe if avg_indiv_sharpe > 1e-8 else 1.0\n",
    "\n",
    "        portfolio_results[\"per_market\"][mkt] = {\n",
    "            \"n_strategies\": len(mkt_ids),\n",
    "            \"portfolio_sharpe\": round(port_sharpe, 4),\n",
    "            \"total_return\": round(port_total, 6),\n",
    "            \"avg_individual_sharpe\": round(avg_indiv_sharpe, 4),\n",
    "            \"diversification_ratio\": round(diversification, 4),\n",
    "            \"win_folds\": int((port_ret > 0).sum()),\n",
    "            \"total_folds\": len(port_ret),\n",
    "        }\n",
    "        print(\"%s portfolio: Sharpe=%.2f, Return=%.4f, Diversification=%.2f\" % (\n",
    "            mkt, port_sharpe, port_total, diversification))\n",
    "\n",
    "    # Cross-market portfolio\n",
    "    if final_ids and len(wf_results) > 0:\n",
    "        cross_wf = wf_results[wf_results[\"strategy_id\"].isin(final_ids)]\n",
    "        if not cross_wf.empty:\n",
    "            cross_ret = cross_wf.groupby(\"fold_idx\")[\"mean_return\"].mean()\n",
    "            cross_sharpe = float(\n",
    "                cross_ret.mean() / cross_ret.std() * np.sqrt(252 / 21)\n",
    "            ) if cross_ret.std() > 1e-8 else 0.0\n",
    "            portfolio_results[\"cross_market\"] = {\n",
    "                \"n_strategies\": len(final_ids),\n",
    "                \"portfolio_sharpe\": round(cross_sharpe, 4),\n",
    "                \"total_return\": round(float(cross_ret.sum()), 6),\n",
    "                \"win_folds\": int((cross_ret > 0).sum()),\n",
    "                \"total_folds\": len(cross_ret),\n",
    "            }\n",
    "            print(\"\\nCross-market portfolio: Sharpe=%.2f, Return=%.4f\" % (\n",
    "                cross_sharpe, float(cross_ret.sum())))\n",
    "\n",
    "    with open(port_path, 'w') as f:\n",
    "        json.dump(portfolio_results, f, indent=2)\n",
    "    tracker.mark_completed(STEP, portfolio_results)\n",
    "\n",
    "print(\"\\n=== Portfolio Summary ===\")\n",
    "print(json.dumps(portfolio_results, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Final Output & Dashboard\n",
    "\n",
    "- Per-market ranked strategy tables\n",
    "- Net performance after costs\n",
    "- Equity curves per market\n",
    "- Turnover metrics\n",
    "- Cross-market portfolio & diversification analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "if len(scored_strategies) == 0:\n",
    "    print(\"No strategies to visualize.\")\n",
    "else:\n",
    "    fig = plt.figure(figsize=(22, 18))\n",
    "    gs = gridspec.GridSpec(3, 2, hspace=0.40, wspace=0.30)\n",
    "\n",
    "    # --- Panel 1: Composite score bar (top 20) ---\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    top20 = scored_strategies.head(20)\n",
    "    colors = ['#4CAF50' if s >= 0.7 else '#FFC107' if s >= 0.4 else '#F44336'\n",
    "              for s in top20['composite_score']]\n",
    "    ax1.barh(range(len(top20)), top20['composite_score'], color=colors, edgecolor='white')\n",
    "    ax1.set_yticks(range(len(top20)))\n",
    "    labels = (top20['market'] + \"/\" + top20['horizon'] + \" \" + top20['strategy_id'].str[-20:])\n",
    "    ax1.set_yticklabels(labels, fontsize=6)\n",
    "    ax1.set_xlabel('Composite Score')\n",
    "    ax1.set_title('Strategy Rankings (Top 20)', fontweight='bold')\n",
    "    ax1.invert_yaxis()\n",
    "\n",
    "    # --- Panel 2: Sharpe stability across folds (top 5) ---\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    top5 = scored_strategies.head(5)\n",
    "    for _, row in top5.iterrows():\n",
    "        sid = row['strategy_id']\n",
    "        fd = wf_results[wf_results['strategy_id'] == sid]\n",
    "        if len(fd) > 0:\n",
    "            ax2.plot(fd['fold_idx'], fd['sharpe'], 'o-',\n",
    "                     label=sid[:30], linewidth=2, markersize=5)\n",
    "    ax2.axhline(y=0, color='gray', linewidth=0.5, linestyle='--')\n",
    "    ax2.set_xlabel('Fold')\n",
    "    ax2.set_ylabel('Sharpe Ratio')\n",
    "    ax2.set_title('Stability: Sharpe Across Folds (Top 5)', fontweight='bold')\n",
    "    ax2.legend(fontsize=6)\n",
    "    ax2.grid(alpha=0.3)\n",
    "\n",
    "    # --- Panel 3: Equity curves (top 5) ---\n",
    "    ax3 = fig.add_subplot(gs[1, 0])\n",
    "    for _, row in top5.iterrows():\n",
    "        sid = row['strategy_id']\n",
    "        fd = wf_results[wf_results['strategy_id'] == sid].sort_values('fold_idx')\n",
    "        if len(fd) > 0:\n",
    "            cum = np.cumsum(fd['mean_return'].values)\n",
    "            ax3.plot(range(len(cum)), cum, 'o-', label=sid[:30], linewidth=2)\n",
    "    ax3.axhline(y=0, color='gray', linewidth=0.5, linestyle='--')\n",
    "    ax3.set_xlabel('Walk-Forward Fold')\n",
    "    ax3.set_ylabel('Cumulative Return (net of costs)')\n",
    "    ax3.set_title('Equity Curves (Top 5)', fontweight='bold')\n",
    "    ax3.legend(fontsize=6)\n",
    "    ax3.grid(alpha=0.3)\n",
    "\n",
    "    # --- Panel 4: Per-market portfolio ---\n",
    "    ax4 = fig.add_subplot(gs[1, 1])\n",
    "    for mkt in CFG.markets:\n",
    "        mkt_ids = [s for s in regime_filtered[\"strategy_id\"].tolist() if s.startswith(mkt + \"_\")] if len(regime_filtered) > 0 else []\n",
    "        if not mkt_ids:\n",
    "            continue\n",
    "        mkt_wf = wf_results[wf_results[\"strategy_id\"].isin(mkt_ids)]\n",
    "        if mkt_wf.empty:\n",
    "            continue\n",
    "        pret = mkt_wf.groupby(\"fold_idx\")[\"mean_return\"].mean().sort_index()\n",
    "        cum = np.cumsum(pret.values)\n",
    "        ax4.plot(range(len(cum)), cum, 'o-', label=\"%s (%d strats)\" % (mkt, len(mkt_ids)), linewidth=2)\n",
    "    ax4.axhline(y=0, color='gray', linewidth=0.5, linestyle='--')\n",
    "    ax4.set_xlabel('Walk-Forward Fold')\n",
    "    ax4.set_ylabel('Cumulative Return')\n",
    "    ax4.set_title('Per-Market Portfolios', fontweight='bold')\n",
    "    ax4.legend(fontsize=8)\n",
    "    ax4.grid(alpha=0.3)\n",
    "\n",
    "    # --- Panel 5: Turnover distribution ---\n",
    "    ax5 = fig.add_subplot(gs[2, 0])\n",
    "    if \"mean_turnover\" in scored_strategies.columns:\n",
    "        ax5.hist(scored_strategies[\"mean_turnover\"].dropna(), bins=30, color='steelblue', edgecolor='white')\n",
    "        ax5.axvline(x=CFG.max_turnover, color='red', linestyle='--', label='Max turnover (%.1f)' % CFG.max_turnover)\n",
    "        ax5.set_xlabel('Annualised Turnover')\n",
    "        ax5.set_ylabel('Count')\n",
    "        ax5.set_title('Turnover Distribution', fontweight='bold')\n",
    "        ax5.legend()\n",
    "\n",
    "    # --- Panel 6: Cross-market diversification ---\n",
    "    ax6 = fig.add_subplot(gs[2, 1])\n",
    "    sharpes = []\n",
    "    labels_bar = []\n",
    "    for mkt in CFG.markets:\n",
    "        pm = portfolio_results.get(\"per_market\", {}).get(mkt, {})\n",
    "        if pm:\n",
    "            sharpes.append(pm[\"portfolio_sharpe\"])\n",
    "            labels_bar.append(mkt)\n",
    "    cm = portfolio_results.get(\"cross_market\", {})\n",
    "    if cm:\n",
    "        sharpes.append(cm[\"portfolio_sharpe\"])\n",
    "        labels_bar.append(\"Cross-Market\")\n",
    "    if sharpes:\n",
    "        bar_colors = ['#2196F3'] * (len(sharpes) - 1) + ['#4CAF50'] if len(sharpes) > 1 else ['#2196F3']\n",
    "        ax6.bar(labels_bar, sharpes, color=bar_colors, edgecolor='white')\n",
    "        ax6.set_ylabel('Portfolio Sharpe')\n",
    "        ax6.set_title('Diversification Benefit', fontweight='bold')\n",
    "        ax6.grid(axis='y', alpha=0.3)\n",
    "\n",
    "    fig.suptitle('Multi-Market Quant Pipeline v2 — Results Dashboard',\n",
    "                 fontsize=16, fontweight='bold', y=1.01)\n",
    "    plt.savefig(os.path.join(CFG.drive_root, 'pipeline_results_v2.png'),\n",
    "                dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Dashboard saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"PIPELINE v2 COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "tracker.summary()\n",
    "\n",
    "if len(scored_strategies) > 0:\n",
    "    print(\"\\n=== Top Strategy ===\")\n",
    "    best = scored_strategies.iloc[0]\n",
    "    print(\"  ID:           %s\" % best['strategy_id'])\n",
    "    print(\"  Market:       %s\" % best.get('market', ''))\n",
    "    print(\"  Horizon:      %s\" % best.get('horizon', ''))\n",
    "    print(\"  Composite:    %.4f\" % best['composite_score'])\n",
    "    print(\"  Stability:    %.2f\" % best['stability'])\n",
    "    print(\"  Sharpe:       %.2f\" % best['mean_sharpe'])\n",
    "    print(\"  Win Rate:     %.2f%%\" % (best['mean_win_rate'] * 100))\n",
    "    print(\"  Turnover:     %.2f\" % best.get('mean_turnover', 0))\n",
    "    print(\"  Total Trades: %d\" % best['total_trades'])\n",
    "\n",
    "    # Portfolio summary\n",
    "    print(\"\\n=== Portfolio Summary ===\")\n",
    "    for mkt, pm in portfolio_results.get(\"per_market\", {}).items():\n",
    "        print(\"  %s: Sharpe=%.2f  Return=%.4f  Diversification=%.2f  Strats=%d\" % (\n",
    "            mkt, pm[\"portfolio_sharpe\"], pm[\"total_return\"],\n",
    "            pm[\"diversification_ratio\"], pm[\"n_strategies\"]))\n",
    "    cm = portfolio_results.get(\"cross_market\", {})\n",
    "    if cm:\n",
    "        print(\"  CROSS-MARKET: Sharpe=%.2f  Return=%.4f  Strats=%d\" % (\n",
    "            cm[\"portfolio_sharpe\"], cm[\"total_return\"], cm[\"n_strategies\"]))\n",
    "\n",
    "    # Save final report\n",
    "    report = {\n",
    "        \"pipeline_version\": \"v2\",\n",
    "        \"markets\": CFG.markets,\n",
    "        \"forward_horizons\": CFG.forward_days_list,\n",
    "        \"cost_bps_roundtrip\": CFG.total_cost_bps * 2,\n",
    "        \"n_candidates_total\": len(all_candidates),\n",
    "        \"n_edge_evaluated\": len(edge_results),\n",
    "        \"n_walk_forward\": len(wf_results),\n",
    "        \"n_filtered\": len(filtered_strategies),\n",
    "        \"n_turnover_filtered\": len(turnover_filtered) if 'turnover_filtered' in dir() else 0,\n",
    "        \"n_scored\": len(scored_strategies),\n",
    "        \"n_regime_filtered\": len(regime_filtered) if 'regime_filtered' in dir() else 0,\n",
    "        \"top_strategy\": best['strategy_id'],\n",
    "        \"top_composite\": float(best['composite_score']),\n",
    "        \"portfolio\": portfolio_results,\n",
    "    }\n",
    "    report_path = os.path.join(CFG.drive_root, 'report_v2.json')\n",
    "    with open(report_path, 'w') as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "    print(\"\\nReport saved to:\", report_path)\n",
    "else:\n",
    "    print(\"\\nNo viable strategies found. Consider:\")\n",
    "    print(\"  - Relaxing filter thresholds\")\n",
    "    print(\"  - Adding more tickers or longer data period\")\n",
    "    print(\"  - Reducing min_sample_size\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}