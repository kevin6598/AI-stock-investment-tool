{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AI Stock Investment Tool - Colab Training\n",
        "\n",
        "Train multiple model configurations with walk-forward validation on GPU.\n",
        "\n",
        "**Steps:**\n",
        "1. Install dependencies & clone repo\n",
        "2. Mount Google Drive & load data\n",
        "3. Build features\n",
        "4. Configure models\n",
        "5. Train with walk-forward validation\n",
        "6. Compare results\n",
        "7. Save best models to Drive"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q yfinance lightgbm torch optuna pyarrow scikit-learn scipy pandas numpy"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/kevin6598/AI-stock-investment-tool.git\n",
        "%cd AI-stock-investment-tool"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Mount Google Drive & Load Data"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Configure paths\n",
        "DRIVE_DIR = \"/content/drive/MyDrive/ai_stock_tool\"\n",
        "os.makedirs(DRIVE_DIR, exist_ok=True)\n",
        "\n",
        "DATA_PATH = os.path.join(DRIVE_DIR, \"dataset.parquet\")\n",
        "OUTPUT_DIR = os.path.join(DRIVE_DIR, \"models_registry\")\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Drive dir: {DRIVE_DIR}\")\n",
        "print(f\"Output dir: {OUTPUT_DIR}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Fetch Data & Build Features\n",
        "\n",
        "If you already exported `dataset.parquet` locally and uploaded it to Drive, skip this cell and go to **Load existing dataset**."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# === Option A: Build dataset from scratch ===\n",
        "\n",
        "TICKERS = [\"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"NVDA\", \"META\", \"TSLA\", \"JPM\", \"V\", \"JNJ\"]\n",
        "PERIOD = \"5y\"\n",
        "FORWARD_HORIZONS = [21, 63, 126]  # 1M, 3M, 6M\n",
        "\n",
        "from data.stock_api import get_historical_data, get_stock_info\n",
        "from training.feature_engineering import build_panel_dataset, cross_sectional_normalize\n",
        "\n",
        "print(\"Fetching stock data...\")\n",
        "stock_dfs = {}\n",
        "stock_infos = {}\n",
        "for ticker in TICKERS:\n",
        "    df = get_historical_data(ticker, period=PERIOD)\n",
        "    if not df.empty:\n",
        "        stock_dfs[ticker] = df\n",
        "        stock_infos[ticker] = get_stock_info(ticker) or {}\n",
        "        print(f\"  {ticker}: {len(df)} rows\")\n",
        "\n",
        "market_df = get_historical_data(\"SPY\", period=PERIOD)\n",
        "print(f\"  SPY (market): {len(market_df)} rows\")\n",
        "\n",
        "print(\"\\nBuilding features...\")\n",
        "panel = build_panel_dataset(stock_dfs, stock_infos, market_df, FORWARD_HORIZONS)\n",
        "panel = cross_sectional_normalize(panel)\n",
        "print(f\"Panel shape: {panel.shape}\")\n",
        "\n",
        "# Save to Drive\n",
        "panel.to_parquet(DATA_PATH)\n",
        "print(f\"\\nSaved to {DATA_PATH}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Option B: Load existing dataset from Drive ===\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "panel = pd.read_parquet(DATA_PATH)\n",
        "print(f\"Loaded panel: {panel.shape}\")\n",
        "print(f\"Tickers: {panel.index.get_level_values(1).unique().tolist()}\")\n",
        "print(f\"Date range: {panel.index.get_level_values(0).min()} to {panel.index.get_level_values(0).max()}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Configure Models"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from training.model_config import ModelConfig, ConfigGrid\n",
        "\n",
        "# Option 1: Manual configs\n",
        "configs = [\n",
        "    ModelConfig(model_type=\"elastic_net\", learning_rate=0.1, epochs=1),\n",
        "    ModelConfig(model_type=\"lightgbm\", learning_rate=0.05, epochs=500,\n",
        "                extra_params={\"num_leaves\": 31, \"max_depth\": 6}),\n",
        "    ModelConfig(model_type=\"lightgbm\", learning_rate=0.01, epochs=500,\n",
        "                extra_params={\"num_leaves\": 63, \"max_depth\": 8}),\n",
        "    ModelConfig(model_type=\"lstm_attention\", learning_rate=1e-3, epochs=100,\n",
        "                dropout=0.2, sequence_length=60),\n",
        "    ModelConfig(model_type=\"transformer\", learning_rate=3e-4, epochs=100,\n",
        "                dropout=0.2, sequence_length=60),\n",
        "]\n",
        "\n",
        "# Option 2: Grid search (uncomment to use)\n",
        "# configs = ConfigGrid.from_grid({\n",
        "#     \"model_type\": [\"lightgbm\"],\n",
        "#     \"learning_rate\": [0.01, 0.05, 0.1],\n",
        "#     \"extra_params\": [{\"num_leaves\": 31}, {\"num_leaves\": 63}],\n",
        "# })\n",
        "\n",
        "# Option 3: Random search (uncomment to use)\n",
        "# configs = ConfigGrid.from_random({\n",
        "#     \"model_type\": [\"lightgbm\", \"lstm_attention\"],\n",
        "#     \"learning_rate\": [0.001, 0.005, 0.01, 0.05],\n",
        "#     \"dropout\": [0.1, 0.2, 0.3],\n",
        "# }, n_samples=8)\n",
        "\n",
        "print(f\"Total configs to train: {len(configs)}\")\n",
        "for i, c in enumerate(configs):\n",
        "    print(f\"  [{i}] {c.model_type} | lr={c.learning_rate} | dropout={c.dropout} | epochs={c.epochs}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Train with Walk-Forward Validation"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from training.model_config import MultiConfigRunner\n",
        "from training.model_selection import WalkForwardConfig\n",
        "\n",
        "# Define walk-forward settings\n",
        "HORIZON = \"1M\"  # Change to \"3M\" or \"6M\" as needed\n",
        "TARGET_COL = \"fwd_return_21d\"  # Must match horizon: 21d=1M, 63d=3M, 126d=6M\n",
        "\n",
        "wf_config = WalkForwardConfig(\n",
        "    train_start=\"2015-01-01\",\n",
        "    test_end=\"2025-01-01\",\n",
        "    train_min_months=36,\n",
        "    val_months=6,\n",
        "    test_months=6,\n",
        "    step_months=6,\n",
        "    embargo_days=21,\n",
        "    expanding=True,\n",
        ")\n",
        "\n",
        "# Feature columns (exclude targets and _close)\n",
        "feature_cols = [\n",
        "    c for c in panel.columns\n",
        "    if not c.startswith(\"fwd_return_\") and c != \"_close\"\n",
        "]\n",
        "print(f\"Features: {len(feature_cols)}\")\n",
        "print(f\"Target: {TARGET_COL}\")\n",
        "print(f\"Horizon: {HORIZON}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Prune low-importance features to speed up training\n",
        "from training.feature_engineering import prune_features\n",
        "\n",
        "sample = panel.head(5000)  # Use a sample for feature selection\n",
        "_, selected_cols = prune_features(\n",
        "    sample[feature_cols],\n",
        "    sample[TARGET_COL],\n",
        "    importance_threshold=0.005,\n",
        ")\n",
        "print(f\"Pruned: {len(feature_cols)} -> {len(selected_cols)} features\")\n",
        "\n",
        "# Uncomment to use pruned features:\n",
        "# feature_cols = selected_cols"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "runner = MultiConfigRunner(save_to_registry=False)\n",
        "\n",
        "print(\"Starting training...\")\n",
        "print(\"=\" * 60)\n",
        "t0 = time.time()\n",
        "\n",
        "results = runner.run(\n",
        "    configs=configs,\n",
        "    panel=panel,\n",
        "    target_col=TARGET_COL,\n",
        "    feature_cols=feature_cols,\n",
        "    wf_config=wf_config,\n",
        "    horizon=HORIZON,\n",
        ")\n",
        "\n",
        "total_time = time.time() - t0\n",
        "print(\"=\" * 60)\n",
        "print(f\"Training complete in {total_time:.1f}s\")\n",
        "print(f\"Configs evaluated: {len(results)}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Compare Results"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Build results table\n",
        "rows = []\n",
        "for r in results:\n",
        "    ev = r.evaluation\n",
        "    rows.append({\n",
        "        \"Model\": r.config.model_type,\n",
        "        \"LR\": r.config.learning_rate,\n",
        "        \"Dropout\": r.config.dropout,\n",
        "        \"IC\": round(ev.mean_ic, 4),\n",
        "        \"ICIR\": round(ev.icir, 2),\n",
        "        \"Sharpe\": round(ev.mean_sharpe, 2),\n",
        "        \"Max DD\": round(ev.mean_mdd, 4),\n",
        "        \"Calmar\": round(ev.mean_calmar, 2),\n",
        "        \"Hit Ratio\": round(ev.mean_hit_ratio, 4),\n",
        "        \"Folds\": len(ev.fold_results),\n",
        "        \"Time (s)\": round(r.training_time, 1),\n",
        "    })\n",
        "\n",
        "df_results = pd.DataFrame(rows)\n",
        "df_results = df_results.sort_values(\"IC\", ascending=False).reset_index(drop=True)\n",
        "print(\"\\nModel Comparison (sorted by IC):\")\n",
        "print(\"=\" * 80)\n",
        "display(df_results)"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Statistical comparison\n",
        "from training.model_comparison import ModelComparisonEngine\n",
        "\n",
        "evaluations = [r.evaluation for r in results]\n",
        "engine = ModelComparisonEngine()\n",
        "report = engine.compare(evaluations)\n",
        "\n",
        "print(\"Rankings by IC:\")\n",
        "for name, val in report.rankings.get(\"ic\", []):\n",
        "    print(f\"  {name}: {val:.4f}\")\n",
        "\n",
        "print(f\"\\nStability scores:\")\n",
        "for name, score in report.stability_scores.items():\n",
        "    print(f\"  {name}: {score:.4f}\")\n",
        "\n",
        "print(f\"\\nBest per horizon: {report.best_per_horizon}\")\n",
        "\n",
        "if report.significance_tests:\n",
        "    print(f\"\\nSignificance tests:\")\n",
        "    for key, test in report.significance_tests.items():\n",
        "        sig = \"YES\" if test[\"ttest_significant_5pct\"] else \"no\"\n",
        "        print(f\"  {key}: p={test['ttest_p_value']:.4f} (significant: {sig})\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize results\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# IC by model\n",
        "axes[0].barh(df_results[\"Model\"] + \" (lr=\" + df_results[\"LR\"].astype(str) + \")\", df_results[\"IC\"])\n",
        "axes[0].set_xlabel(\"Mean IC\")\n",
        "axes[0].set_title(\"Information Coefficient\")\n",
        "\n",
        "# Sharpe by model\n",
        "axes[1].barh(df_results[\"Model\"] + \" (lr=\" + df_results[\"LR\"].astype(str) + \")\", df_results[\"Sharpe\"])\n",
        "axes[1].set_xlabel(\"Mean Sharpe\")\n",
        "axes[1].set_title(\"Sharpe Ratio\")\n",
        "\n",
        "# Training time\n",
        "axes[2].barh(df_results[\"Model\"] + \" (lr=\" + df_results[\"LR\"].astype(str) + \")\", df_results[\"Time (s)\"])\n",
        "axes[2].set_xlabel(\"Seconds\")\n",
        "axes[2].set_title(\"Training Time\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, \"comparison.png\"), dpi=150, bbox_inches=\"tight\")\n",
        "plt.show()"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Save Best Models to Drive"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Save all results\n",
        "results_data = []\n",
        "for i, r in enumerate(results):\n",
        "    model_dir = os.path.join(OUTPUT_DIR, f\"{r.config.model_type}_v{i}\")\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "    # Save config\n",
        "    with open(os.path.join(model_dir, \"config.json\"), \"w\") as f:\n",
        "        json.dump(r.config.to_json(), f, indent=2)\n",
        "\n",
        "    # Save metrics\n",
        "    metrics = {\n",
        "        \"mean_ic\": r.evaluation.mean_ic,\n",
        "        \"icir\": r.evaluation.icir,\n",
        "        \"mean_sharpe\": r.evaluation.mean_sharpe,\n",
        "        \"mean_mdd\": r.evaluation.mean_mdd,\n",
        "        \"mean_calmar\": r.evaluation.mean_calmar,\n",
        "        \"mean_hit_ratio\": r.evaluation.mean_hit_ratio,\n",
        "        \"n_folds\": len(r.evaluation.fold_results),\n",
        "    }\n",
        "    with open(os.path.join(model_dir, \"metrics.json\"), \"w\") as f:\n",
        "        json.dump(metrics, f, indent=2)\n",
        "\n",
        "    results_data.append({\n",
        "        \"config\": r.config.to_json(),\n",
        "        \"training_time\": r.training_time,\n",
        "        \"best_params\": r.best_params,\n",
        "        \"metrics\": metrics,\n",
        "    })\n",
        "\n",
        "    print(f\"Saved: {model_dir}\")\n",
        "\n",
        "# Save combined results JSON (for local import)\n",
        "results_path = os.path.join(OUTPUT_DIR, \"results.json\")\n",
        "with open(results_path, \"w\") as f:\n",
        "    json.dump(results_data, f, indent=2)\n",
        "\n",
        "print(f\"\\nAll results saved to {OUTPUT_DIR}\")\n",
        "print(f\"Import locally with: DataExporter.import_results('{results_path}')\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. (Optional) Hyperparameter Search with Optuna"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Optuna HP search for the best model type\n",
        "from training.hyperparameter_search import run_optuna_search\n",
        "\n",
        "best_model_type = results[0].config.model_type\n",
        "print(f\"Running Optuna HP search for: {best_model_type}\")\n",
        "\n",
        "best_params, best_ic = run_optuna_search(\n",
        "    model_type=best_model_type,\n",
        "    panel=panel,\n",
        "    target_col=TARGET_COL,\n",
        "    feature_cols=feature_cols,\n",
        "    n_trials=20,\n",
        ")\n",
        "\n",
        "print(f\"\\nBest IC: {best_ic:.4f}\")\n",
        "print(f\"Best params: {best_params}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}