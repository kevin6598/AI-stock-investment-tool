{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# AI Stock Investment Tool - Colab Training\n\nTrain multiple model configurations with walk-forward validation on GPU, then serve the API for your frontend.\n\n**Before starting:**\n1. Go to **Runtime > Change runtime type > T4 GPU**\n2. Run cells **in order** from top to bottom\n3. If repo is private, you'll need a [GitHub Personal Access Token](https://github.com/settings/tokens) with `repo` scope\n\n**Sections:**\n- 1-2: Setup & environment\n- 3: Fetch data & build features\n- 4-6: Multi-config training & comparison\n- 7: Save results to Drive\n- 8: Optuna hyperparameter search\n- 9-10: Hybrid multi-modal training & export\n- 11: Serve API via ngrok (connect to frontend)\n- 12: Download artifacts to local machine",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Setup"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "!pip install -q yfinance lightgbm torch optuna pyarrow scikit-learn scipy pandas numpy matplotlib feedparser pyngrok",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T12:35:45.185035800Z",
     "start_time": "2026-02-09T12:35:23.067676700Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import os\nos.chdir(\"/content\")\n\n# Clean previous clone if any\n!rm -rf AI-stock-investment-tool\n\n# Try public clone first, fall back to token auth\nREPO = \"https://github.com/kevin6598/AI-stock-investment-tool.git\"\nret = os.system(\"git clone %s 2>/dev/null\" % REPO)\n\nif ret != 0:\n    from getpass import getpass\n    print(\"Public clone failed -- repo is private.\")\n    print(\"Create a token at: https://github.com/settings/tokens (repo scope)\")\n    token = getpass(\"Paste your GitHub token: \")\n    os.system(\"git clone https://%s@github.com/kevin6598/AI-stock-investment-tool.git\" % token)\n    del token\n\nos.chdir(\"/content/AI-stock-investment-tool\")\nprint(\"Working dir: %s\" % os.getcwd())\n!git log --oneline -3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T12:35:45.593941800Z",
     "start_time": "2026-02-09T12:35:45.188027900Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import torch, sys\nprint(f\"Python: {sys.version}\")\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\nelse:\n    print(\"WARNING: No GPU detected. Go to Runtime > Change runtime type > T4 GPU\")",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Mount Google Drive & Load Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import os\n\n# Configure paths -- artifacts persist on Google Drive across sessions\nDRIVE_DIR = \"/content/drive/MyDrive/ai_stock_tool\"\nos.makedirs(DRIVE_DIR, exist_ok=True)\n\nDATA_PATH = os.path.join(DRIVE_DIR, \"dataset.parquet\")\nOUTPUT_DIR = os.path.join(DRIVE_DIR, \"models_registry\")\nARTIFACT_DIR = os.path.join(DRIVE_DIR, \"artifacts\")\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nos.makedirs(ARTIFACT_DIR, exist_ok=True)\n\nprint(\"Drive dir:    %s\" % DRIVE_DIR)\nprint(\"Data path:    %s\" % DATA_PATH)\nprint(\"Output dir:   %s\" % OUTPUT_DIR)\nprint(\"Artifact dir: %s\" % ARTIFACT_DIR)",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Fetch Data & Build Features\n",
    "\n",
    "If you already exported `dataset.parquet` locally and uploaded it to Drive, skip this cell and go to **Load existing dataset**."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# === Option A: Build dataset from scratch ===\n",
    "\n",
    "TICKERS = [\"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"NVDA\", \"META\", \"TSLA\", \"JPM\", \"V\", \"JNJ\"]\n",
    "PERIOD = \"5y\"\n",
    "FORWARD_HORIZONS = [21, 63, 126]  # 1M, 3M, 6M\n",
    "\n",
    "from data.stock_api import get_historical_data, get_stock_info\n",
    "from training.feature_engineering import build_panel_dataset, cross_sectional_normalize\n",
    "\n",
    "print(\"Fetching stock data...\")\n",
    "stock_dfs = {}\n",
    "stock_infos = {}\n",
    "for ticker in TICKERS:\n",
    "    df = get_historical_data(ticker, period=PERIOD)\n",
    "    if not df.empty:\n",
    "        stock_dfs[ticker] = df\n",
    "        stock_infos[ticker] = get_stock_info(ticker) or {}\n",
    "        print(f\"  {ticker}: {len(df)} rows\")\n",
    "\n",
    "market_df = get_historical_data(\"SPY\", period=PERIOD)\n",
    "print(f\"  SPY (market): {len(market_df)} rows\")\n",
    "\n",
    "print(\"\\nBuilding features...\")\n",
    "panel = build_panel_dataset(stock_dfs, stock_infos, market_df, FORWARD_HORIZONS)\n",
    "panel = cross_sectional_normalize(panel)\n",
    "print(f\"Panel shape: {panel.shape}\")\n",
    "\n",
    "# Save to Drive\n",
    "panel.to_parquet(DATA_PATH)\n",
    "print(f\"\\nSaved to {DATA_PATH}\")"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# === Option B: Load existing dataset from Drive ===\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "panel = pd.read_parquet(DATA_PATH)\n",
    "print(f\"Loaded panel: {panel.shape}\")\n",
    "print(f\"Tickers: {panel.index.get_level_values(1).unique().tolist()}\")\n",
    "print(f\"Date range: {panel.index.get_level_values(0).min()} to {panel.index.get_level_values(0).max()}\")"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Configure Models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from training.model_config import ModelConfig, ConfigGrid\n",
    "\n",
    "# Option 1: Manual configs\n",
    "configs = [\n",
    "    ModelConfig(model_type=\"elastic_net\", learning_rate=0.1, epochs=1),\n",
    "    ModelConfig(model_type=\"lightgbm\", learning_rate=0.05, epochs=500,\n",
    "                extra_params={\"num_leaves\": 31, \"max_depth\": 6}),\n",
    "    ModelConfig(model_type=\"lightgbm\", learning_rate=0.01, epochs=500,\n",
    "                extra_params={\"num_leaves\": 63, \"max_depth\": 8}),\n",
    "    ModelConfig(model_type=\"lstm_attention\", learning_rate=1e-3, epochs=100,\n",
    "                dropout=0.2, sequence_length=60),\n",
    "    ModelConfig(model_type=\"transformer\", learning_rate=3e-4, epochs=100,\n",
    "                dropout=0.2, sequence_length=60),\n",
    "]\n",
    "\n",
    "# Option 2: Grid search (uncomment to use)\n",
    "# configs = ConfigGrid.from_grid({\n",
    "#     \"model_type\": [\"lightgbm\"],\n",
    "#     \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "#     \"extra_params\": [{\"num_leaves\": 31}, {\"num_leaves\": 63}],\n",
    "# })\n",
    "\n",
    "# Option 3: Random search (uncomment to use)\n",
    "# configs = ConfigGrid.from_random({\n",
    "#     \"model_type\": [\"lightgbm\", \"lstm_attention\"],\n",
    "#     \"learning_rate\": [0.001, 0.005, 0.01, 0.05],\n",
    "#     \"dropout\": [0.1, 0.2, 0.3],\n",
    "# }, n_samples=8)\n",
    "\n",
    "print(f\"Total configs to train: {len(configs)}\")\n",
    "for i, c in enumerate(configs):\n",
    "    print(f\"  [{i}] {c.model_type} | lr={c.learning_rate} | dropout={c.dropout} | epochs={c.epochs}\")"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Train with Walk-Forward Validation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from training.model_config import MultiConfigRunner\n",
    "from training.model_selection import WalkForwardConfig\n",
    "\n",
    "# Define walk-forward settings\n",
    "HORIZON = \"1M\"  # Change to \"3M\" or \"6M\" as needed\n",
    "TARGET_COL = \"fwd_return_21d\"  # Must match horizon: 21d=1M, 63d=3M, 126d=6M\n",
    "\n",
    "wf_config = WalkForwardConfig(\n",
    "    train_start=\"2015-01-01\",\n",
    "    test_end=\"2025-01-01\",\n",
    "    train_min_months=36,\n",
    "    val_months=6,\n",
    "    test_months=6,\n",
    "    step_months=6,\n",
    "    embargo_days=21,\n",
    "    expanding=True,\n",
    ")\n",
    "\n",
    "# Feature columns (exclude targets and _close)\n",
    "feature_cols = [\n",
    "    c for c in panel.columns\n",
    "    if not c.startswith(\"fwd_return_\") and c != \"_close\"\n",
    "]\n",
    "print(f\"Features: {len(feature_cols)}\")\n",
    "print(f\"Target: {TARGET_COL}\")\n",
    "print(f\"Horizon: {HORIZON}\")"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Optional: Prune low-importance features to speed up training\n",
    "from training.feature_engineering import prune_features\n",
    "\n",
    "sample = panel.head(5000)  # Use a sample for feature selection\n",
    "_, selected_cols = prune_features(\n",
    "    sample[feature_cols],\n",
    "    sample[TARGET_COL],\n",
    "    importance_threshold=0.005,\n",
    ")\n",
    "print(f\"Pruned: {len(feature_cols)} -> {len(selected_cols)} features\")\n",
    "\n",
    "# Uncomment to use pruned features:\n",
    "# feature_cols = selected_cols"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "runner = MultiConfigRunner(save_to_registry=False)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"=\" * 60)\n",
    "t0 = time.time()\n",
    "\n",
    "results = runner.run(\n",
    "    configs=configs,\n",
    "    panel=panel,\n",
    "    target_col=TARGET_COL,\n",
    "    feature_cols=feature_cols,\n",
    "    wf_config=wf_config,\n",
    "    horizon=HORIZON,\n",
    ")\n",
    "\n",
    "total_time = time.time() - t0\n",
    "print(\"=\" * 60)\n",
    "print(f\"Training complete in {total_time:.1f}s\")\n",
    "print(f\"Configs evaluated: {len(results)}\")"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Compare Results"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Build results table\n",
    "rows = []\n",
    "for r in results:\n",
    "    ev = r.evaluation\n",
    "    rows.append({\n",
    "        \"Model\": r.config.model_type,\n",
    "        \"LR\": r.config.learning_rate,\n",
    "        \"Dropout\": r.config.dropout,\n",
    "        \"IC\": round(ev.mean_ic, 4),\n",
    "        \"ICIR\": round(ev.icir, 2),\n",
    "        \"Sharpe\": round(ev.mean_sharpe, 2),\n",
    "        \"Max DD\": round(ev.mean_mdd, 4),\n",
    "        \"Calmar\": round(ev.mean_calmar, 2),\n",
    "        \"Hit Ratio\": round(ev.mean_hit_ratio, 4),\n",
    "        \"Folds\": len(ev.fold_results),\n",
    "        \"Time (s)\": round(r.training_time, 1),\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(rows)\n",
    "df_results = df_results.sort_values(\"IC\", ascending=False).reset_index(drop=True)\n",
    "print(\"\\nModel Comparison (sorted by IC):\")\n",
    "print(\"=\" * 80)\n",
    "display(df_results)"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Statistical comparison\n",
    "from training.model_comparison import ModelComparisonEngine\n",
    "\n",
    "evaluations = [r.evaluation for r in results]\n",
    "engine = ModelComparisonEngine()\n",
    "report = engine.compare(evaluations)\n",
    "\n",
    "print(\"Rankings by IC:\")\n",
    "for name, val in report.rankings.get(\"ic\", []):\n",
    "    print(f\"  {name}: {val:.4f}\")\n",
    "\n",
    "print(f\"\\nStability scores:\")\n",
    "for name, score in report.stability_scores.items():\n",
    "    print(f\"  {name}: {score:.4f}\")\n",
    "\n",
    "print(f\"\\nBest per horizon: {report.best_per_horizon}\")\n",
    "\n",
    "if report.significance_tests:\n",
    "    print(f\"\\nSignificance tests:\")\n",
    "    for key, test in report.significance_tests.items():\n",
    "        sig = \"YES\" if test[\"ttest_significant_5pct\"] else \"no\"\n",
    "        print(f\"  {key}: p={test['ttest_p_value']:.4f} (significant: {sig})\")"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Visualize results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# IC by model\n",
    "axes[0].barh(df_results[\"Model\"] + \" (lr=\" + df_results[\"LR\"].astype(str) + \")\", df_results[\"IC\"])\n",
    "axes[0].set_xlabel(\"Mean IC\")\n",
    "axes[0].set_title(\"Information Coefficient\")\n",
    "\n",
    "# Sharpe by model\n",
    "axes[1].barh(df_results[\"Model\"] + \" (lr=\" + df_results[\"LR\"].astype(str) + \")\", df_results[\"Sharpe\"])\n",
    "axes[1].set_xlabel(\"Mean Sharpe\")\n",
    "axes[1].set_title(\"Sharpe Ratio\")\n",
    "\n",
    "# Training time\n",
    "axes[2].barh(df_results[\"Model\"] + \" (lr=\" + df_results[\"LR\"].astype(str) + \")\", df_results[\"Time (s)\"])\n",
    "axes[2].set_xlabel(\"Seconds\")\n",
    "axes[2].set_title(\"Training Time\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"comparison.png\"), dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7. Save Best Models to Drive"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "# Save all results\n",
    "results_data = []\n",
    "for i, r in enumerate(results):\n",
    "    model_dir = os.path.join(OUTPUT_DIR, f\"{r.config.model_type}_v{i}\")\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    # Save config\n",
    "    with open(os.path.join(model_dir, \"config.json\"), \"w\") as f:\n",
    "        json.dump(r.config.to_json(), f, indent=2)\n",
    "\n",
    "    # Save metrics\n",
    "    metrics = {\n",
    "        \"mean_ic\": r.evaluation.mean_ic,\n",
    "        \"icir\": r.evaluation.icir,\n",
    "        \"mean_sharpe\": r.evaluation.mean_sharpe,\n",
    "        \"mean_mdd\": r.evaluation.mean_mdd,\n",
    "        \"mean_calmar\": r.evaluation.mean_calmar,\n",
    "        \"mean_hit_ratio\": r.evaluation.mean_hit_ratio,\n",
    "        \"n_folds\": len(r.evaluation.fold_results),\n",
    "    }\n",
    "    with open(os.path.join(model_dir, \"metrics.json\"), \"w\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "\n",
    "    results_data.append({\n",
    "        \"config\": r.config.to_json(),\n",
    "        \"training_time\": r.training_time,\n",
    "        \"best_params\": r.best_params,\n",
    "        \"metrics\": metrics,\n",
    "    })\n",
    "\n",
    "    print(f\"Saved: {model_dir}\")\n",
    "\n",
    "# Save combined results JSON (for local import)\n",
    "results_path = os.path.join(OUTPUT_DIR, \"results.json\")\n",
    "with open(results_path, \"w\") as f:\n",
    "    json.dump(results_data, f, indent=2)\n",
    "\n",
    "print(f\"\\nAll results saved to {OUTPUT_DIR}\")\n",
    "print(f\"Import locally with: DataExporter.import_results('{results_path}')\")"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8. (Optional) Hyperparameter Search with Optuna"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Run Optuna HP search for the best model type\nfrom training.hyperparameter_search import HyperparameterSearcher\n\nbest_model_type = results[0].config.model_type\nprint(\"Running Optuna HP search for: %s\" % best_model_type)\n\nsearcher = HyperparameterSearcher(\n    model_type=best_model_type,\n    panel=panel,\n    target_col=TARGET_COL,\n    feature_cols=feature_cols,\n    outer_config=wf_config,\n    n_trials=20,\n    inner_folds_count=3,\n)\n\nsearch_results = searcher.search()\n\nprint(\"\\nBest params per fold:\")\nfor fold_idx, params in search_results.get(\"best_params_per_fold\", {}).items():\n    print(\"  Fold %s: %s\" % (fold_idx, params))\n\neval_result = search_results.get(\"evaluation\")\nif eval_result:\n    print(\"\\nEvaluation after HP search:\")\n    print(\"  IC: %.4f\" % eval_result.mean_ic)\n    print(\"  ICIR: %.2f\" % eval_result.icir)\n    print(\"  Sharpe: %.2f\" % eval_result.mean_sharpe)",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 9. Hybrid Multi-Modal Model Training (50-Ticker Universe)\n\nTrain the hybrid multi-modal model on the full 50-ticker universe with ticker embeddings and export production artifacts.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 50-ticker universe for hybrid model\nTICKERS_50 = [\n    \"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"META\", \"NVDA\", \"TSLA\", \"BRK-B\", \"JPM\", \"JNJ\",\n    \"V\", \"PG\", \"UNH\", \"HD\", \"MA\", \"DIS\", \"PYPL\", \"BAC\", \"NFLX\", \"ADBE\",\n    \"CRM\", \"CMCSA\", \"XOM\", \"VZ\", \"KO\", \"INTC\", \"PEP\", \"ABT\", \"CSCO\", \"TMO\",\n    \"COST\", \"MRK\", \"WMT\", \"AVGO\", \"ACN\", \"CVX\", \"NKE\", \"LLY\", \"MCD\", \"TXN\",\n    \"QCOM\", \"DHR\", \"UPS\", \"BMY\", \"PM\", \"LIN\", \"NEE\", \"ORCL\", \"RTX\", \"HON\",\n]\n\nfrom data.stock_api import get_historical_data, get_stock_info\nfrom training.feature_engineering import (\n    build_panel_dataset, cross_sectional_normalize, add_ticker_embedding_column\n)\n\nprint(f\"Fetching data for {len(TICKERS_50)} tickers...\")\nstock_dfs_50 = {}\nstock_infos_50 = {}\nfor ticker in TICKERS_50:\n    try:\n        df = get_historical_data(ticker, period=\"5y\")\n        if not df.empty and len(df) > 300:\n            stock_dfs_50[ticker] = df\n            stock_infos_50[ticker] = get_stock_info(ticker) or {}\n    except Exception as e:\n        print(f\"  WARN: {ticker} failed: {e}\")\n\nmarket_df_50 = get_historical_data(\"SPY\", period=\"5y\")\nvalid_tickers = sorted(stock_dfs_50.keys())\nprint(f\"Valid tickers: {len(valid_tickers)}\")\n\nprint(\"Building panel...\")\npanel_50 = build_panel_dataset(stock_dfs_50, stock_infos_50, market_df_50, [21, 63, 126])\npanel_50 = cross_sectional_normalize(panel_50)\npanel_50, ticker_to_id = add_ticker_embedding_column(panel_50, valid_tickers)\nprint(f\"Panel shape: {panel_50.shape}\")\nprint(f\"Ticker IDs: {ticker_to_id}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "import time\nimport numpy as np\nfrom training.models import create_model\n\nTARGET_COL_50 = \"fwd_return_21d\"\nfeature_cols_50 = [c for c in panel_50.columns\n                   if not c.startswith(\"fwd_return_\") and c not in (\"_close\", \"ticker_id\")]\n\nX_50 = panel_50[feature_cols_50].values.astype(np.float32)\ny_50 = panel_50[TARGET_COL_50].values.astype(np.float32)\nnp.nan_to_num(X_50, copy=False, nan=0.0, posinf=0.0, neginf=0.0)\nnp.nan_to_num(y_50, copy=False, nan=0.0, posinf=0.0, neginf=0.0)\n\nsplit = int(len(X_50) * 0.8)\nval_split = int(len(X_50) * 0.9)\n\nprint(f\"Features: {X_50.shape[1]}\")\nprint(f\"Samples: train={split}, val={val_split-split}, test={len(X_50)-val_split}\")\n\nprint(\"\\nTraining hybrid multi-modal model...\")\nt0 = time.time()\n\nhybrid = create_model(\"hybrid_multimodal\", {\n    \"epochs\": 50,\n    \"patience\": 10,\n    \"n_tickers\": len(valid_tickers),\n    \"hidden_dim\": 128,\n    \"fusion_dim\": 128,\n    \"vae_latent_dim\": 16,\n    \"sequence_length\": 60,\n    \"learning_rate\": 1e-3,\n    \"batch_size\": 64,\n})\n\nhybrid.fit(\n    X_50[:split], y_50[:split],\n    X_50[split:val_split], y_50[split:val_split],\n    feature_names=feature_cols_50,\n)\n\ntrain_time = time.time() - t0\nprint(f\"Training complete in {train_time:.1f}s\")\n\n# Quick evaluation\ntest_preds = hybrid.predict(X_50[val_split:])\nvalid_mask = ~np.isnan(test_preds)\nfrom training.model_selection import compute_prediction_metrics\ntest_metrics = compute_prediction_metrics(y_50[val_split:][valid_mask], test_preds[valid_mask])\nprint(f\"Test IC: {test_metrics.ic:.4f}, Hit ratio: {test_metrics.hit_ratio:.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 10. Export Production Artifacts\n\nExport model, scaler, config, feature columns, and ticker list for the FastAPI backend.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import json\nimport pickle\nfrom datetime import datetime\n\n# Use the ARTIFACT_DIR configured in section 2\nos.makedirs(ARTIFACT_DIR, exist_ok=True)\n\n# 1. Save model\nmodel_path = os.path.join(ARTIFACT_DIR, \"model.pkl\")\nwith open(model_path, \"wb\") as f:\n    pickle.dump(hybrid, f)\nprint(\"Model saved: %s\" % model_path)\n\n# Also save PyTorch state_dict if applicable\nif hasattr(hybrid, 'net'):\n    import torch\n    hybrid.net.eval()\n    pt_path = os.path.join(ARTIFACT_DIR, \"model.pt\")\n    torch.save(hybrid.net.state_dict(), pt_path)\n    print(\"State dict saved: %s\" % pt_path)\n\n# 2. Save feature scaler\nif hasattr(hybrid, 'scaler'):\n    scaler_path = os.path.join(ARTIFACT_DIR, \"feature_scaler.pkl\")\n    with open(scaler_path, \"wb\") as f:\n        pickle.dump(hybrid.scaler, f)\n    print(\"Scaler saved\")\n\n# 3. Save config\nconfig = {\n    \"model_type\": \"hybrid_multimodal\",\n    \"horizons\": [\"1M\", \"3M\", \"6M\"],\n    \"horizon_days\": [21, 63, 126],\n    \"n_features\": len(feature_cols_50),\n    \"n_tickers\": len(valid_tickers),\n    \"hidden_dim\": 128,\n    \"fusion_dim\": 128,\n    \"vae_latent_dim\": 16,\n    \"sequence_length\": 60,\n}\nwith open(os.path.join(ARTIFACT_DIR, \"config.json\"), \"w\") as f:\n    json.dump(config, f, indent=2)\nprint(\"Config saved\")\n\n# 4. Save feature columns (preserves training order)\nwith open(os.path.join(ARTIFACT_DIR, \"feature_columns.json\"), \"w\") as f:\n    json.dump(feature_cols_50, f)\nprint(\"Feature columns saved (%d cols)\" % len(feature_cols_50))\n\n# 5. Save ticker list\nwith open(os.path.join(ARTIFACT_DIR, \"ticker_list.json\"), \"w\") as f:\n    json.dump(valid_tickers, f)\nprint(\"Ticker list saved (%d tickers)\" % len(valid_tickers))\n\n# 6. Save training metadata\nmetadata = {\n    \"version\": \"hybrid_v%s\" % datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n    \"trained_at\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n    \"n_tickers\": len(valid_tickers),\n    \"n_features\": len(feature_cols_50),\n    \"n_samples\": len(X_50),\n    \"train_size\": split,\n    \"test_ic\": float(test_metrics.ic),\n    \"test_hit_ratio\": float(test_metrics.hit_ratio),\n    \"training_time_seconds\": round(train_time, 1),\n    \"tickers\": valid_tickers,\n}\nwith open(os.path.join(ARTIFACT_DIR, \"training_metadata.json\"), \"w\") as f:\n    json.dump(metadata, f, indent=2)\n\nprint(\"\\nAll artifacts exported to: %s\" % ARTIFACT_DIR)\nprint(\"Version: %s\" % metadata[\"version\"])\n\n# Also copy artifacts into repo's artifacts/ dir for API serving\nLOCAL_ARTIFACTS = \"/content/AI-stock-investment-tool/artifacts\"\nos.makedirs(LOCAL_ARTIFACTS, exist_ok=True)\n!cp -r {ARTIFACT_DIR}/* {LOCAL_ARTIFACTS}/\nprint(\"\\nCopied to repo artifacts/ for API serving\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 11. Serve API via ngrok (Connect to Frontend)\n\nStart the FastAPI backend on Colab and expose it via ngrok so your local Next.js frontend can call it.\n\n**Prerequisites:** You need a free [ngrok auth token](https://dashboard.ngrok.com/get-started/your-authtoken). Free tier gives 1 tunnel.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import subprocess\nimport time\nfrom pyngrok import ngrok\n\n# --- Configure ngrok ---\n# Get your free auth token at: https://dashboard.ngrok.com/get-started/your-authtoken\nNGROK_AUTH_TOKEN = \"\"  # <-- Paste your token here\n\nif NGROK_AUTH_TOKEN:\n    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\nelse:\n    print(\"WARNING: No ngrok auth token set. Tunnel may not work.\")\n    print(\"Get one at: https://dashboard.ngrok.com/get-started/your-authtoken\")\n\n# Make sure artifacts are in the repo dir\nLOCAL_ARTIFACTS = \"/content/AI-stock-investment-tool/artifacts\"\nassert os.path.exists(os.path.join(LOCAL_ARTIFACTS, \"model.pkl\")), \\\n    \"No model.pkl found! Run sections 9-10 first to train and export.\"\n\n# Install uvicorn if not present\n!pip install -q uvicorn fastapi pydantic python-multipart\n\n# Start FastAPI in background\nos.chdir(\"/content/AI-stock-investment-tool\")\nserver_proc = subprocess.Popen(\n    [\"python\", \"-m\", \"uvicorn\", \"api.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"],\n    stdout=subprocess.PIPE,\n    stderr=subprocess.PIPE,\n)\ntime.sleep(3)\n\n# Check if server started successfully\nif server_proc.poll() is not None:\n    print(\"ERROR: Server failed to start!\")\n    print(server_proc.stderr.read().decode())\nelse:\n    print(\"FastAPI server started (PID: %d)\" % server_proc.pid)\n\n    # Open ngrok tunnel\n    public_url = ngrok.connect(8000)\n    print(\"\\n\" + \"=\" * 60)\n    print(\"PUBLIC API URL: %s\" % public_url)\n    print(\"=\" * 60)\n    print(\"\\nAPI Docs:  %s/docs\" % public_url)\n    print(\"Health:    %s/api/v1/health\" % public_url)\n    print(\"Predict:   %s/api/v1/predict\" % public_url)\n    print(\"\\n--- To connect your local frontend ---\")\n    print(\"Option A (PowerShell): set env var before npm run dev:\")\n    print('  $env:NEXT_PUBLIC_API_URL=\"%s\"' % public_url)\n    print(\"  cd frontend; npm run dev\")\n    print(\"\\nOption B: Create frontend/.env.local with:\")\n    print(\"  NEXT_PUBLIC_API_URL=%s\" % public_url)\n    print(\"\\nKeep this cell running! The tunnel closes when the runtime stops.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Quick test: verify API is responding\nimport urllib.request\nimport json\n\ntest_url = \"http://localhost:8000/api/v1/health\"\ntry:\n    resp = urllib.request.urlopen(test_url, timeout=5)\n    data = json.loads(resp.read().decode())\n    print(\"Health check OK:\")\n    for k, v in data.items():\n        print(\"  %s: %s\" % (k, v))\nexcept Exception as e:\n    print(\"Health check failed: %s\" % e)\n    print(\"Check server logs:\")\n    if server_proc.poll() is not None:\n        print(server_proc.stderr.read().decode()[-500:])",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 12. Download Artifacts to Local Machine\n\nIf you prefer to run the API locally instead of via ngrok, download the trained artifacts.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Zip artifacts and download to your local machine\n!cd {ARTIFACT_DIR} && zip -r /content/artifacts.zip .\n\nprint(\"Artifact contents:\")\n!ls -lh {ARTIFACT_DIR}\n\nprint(\"\\nTotal zip size:\")\n!ls -lh /content/artifacts.zip\n\n# Download via browser\nfrom google.colab import files\nfiles.download(\"/content/artifacts.zip\")\n\nprint(\"\\nAfter downloading, on your local machine:\")\nprint(\"  1. Unzip into your project root:\")\nprint(\"     unzip artifacts.zip -d artifacts/\")\nprint(\"  2. Start the API:\")\nprint(\"     python -m api.main\")\nprint(\"  3. Start the frontend:\")\nprint(\"     cd frontend && npm run dev\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 13. Cleanup\n\nStop the API server and ngrok tunnel when done.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Stop server and tunnel\ntry:\n    ngrok.disconnect(public_url)\n    ngrok.kill()\n    print(\"ngrok tunnel closed\")\nexcept Exception:\n    pass\n\ntry:\n    server_proc.terminate()\n    server_proc.wait(timeout=5)\n    print(\"FastAPI server stopped\")\nexcept Exception:\n    pass",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}