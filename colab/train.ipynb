{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# AI Stock Investment Tool - Colab Training\n\nTrain and compare all model types (including Hybrid Multi-Modal) on GPU, then serve the API for your frontend.\n\n**Before starting:**\n1. Go to **Runtime > Change runtime type > T4 GPU**\n2. Run cells **in order** from top to bottom\n3. If repo is private, you'll need a [GitHub Personal Access Token](https://github.com/settings/tokens) with `repo` scope\n\n**Sections:**\n- 1-2: Setup & environment\n- 3: Fetch data & build features (50-ticker universe)\n- 4-6: Configure, train, and compare all models\n- 7: Save results to Drive\n- 8: Optuna hyperparameter search (optional)\n- 9: Export production artifacts (best model)\n- 10: Serve API via ngrok (connect to frontend)\n- 11: Download artifacts to local machine",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Setup"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "!pip install -q yfinance lightgbm torch optuna pyarrow scikit-learn scipy pandas numpy matplotlib feedparser pyngrok",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T12:35:45.185035800Z",
     "start_time": "2026-02-09T12:35:23.067676700Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import os\nos.chdir(\"/content\")\n\n# Clean previous clone if any\n!rm -rf AI-stock-investment-tool\n\n# Try public clone first, fall back to token auth\nREPO = \"https://github.com/kevin6598/AI-stock-investment-tool.git\"\nret = os.system(\"git clone %s 2>/dev/null\" % REPO)\n\nif ret != 0:\n    from getpass import getpass\n    print(\"Public clone failed -- repo is private.\")\n    print(\"Create a token at: https://github.com/settings/tokens (repo scope)\")\n    token = getpass(\"Paste your GitHub token: \")\n    os.system(\"git clone https://%s@github.com/kevin6598/AI-stock-investment-tool.git\" % token)\n    del token\n\nos.chdir(\"/content/AI-stock-investment-tool\")\nprint(\"Working dir: %s\" % os.getcwd())\n!git log --oneline -3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T12:35:45.593941800Z",
     "start_time": "2026-02-09T12:35:45.188027900Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import torch, sys\nprint(f\"Python: {sys.version}\")\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\nelse:\n    print(\"WARNING: No GPU detected. Go to Runtime > Change runtime type > T4 GPU\")",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Mount Google Drive & Load Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import os\n\n# Configure paths -- artifacts persist on Google Drive across sessions\nDRIVE_DIR = \"/content/drive/MyDrive/ai_stock_tool\"\nos.makedirs(DRIVE_DIR, exist_ok=True)\n\nDATA_PATH = os.path.join(DRIVE_DIR, \"dataset.parquet\")\nOUTPUT_DIR = os.path.join(DRIVE_DIR, \"models_registry\")\nARTIFACT_DIR = os.path.join(DRIVE_DIR, \"artifacts\")\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nos.makedirs(ARTIFACT_DIR, exist_ok=True)\n\nprint(\"Drive dir:    %s\" % DRIVE_DIR)\nprint(\"Data path:    %s\" % DATA_PATH)\nprint(\"Output dir:   %s\" % OUTPUT_DIR)\nprint(\"Artifact dir: %s\" % ARTIFACT_DIR)",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 3. Fetch Data & Build Features\n\nFetches 50-ticker universe with ticker embeddings (needed for Hybrid Multi-Modal).\nIf you already built the dataset, skip Option A and use Option B to load from Drive.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# === Option A: Build dataset from scratch (50-ticker universe) ===\n\nTICKERS = [\n    \"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"META\", \"NVDA\", \"TSLA\", \"BRK-B\", \"JPM\", \"JNJ\",\n    \"V\", \"PG\", \"UNH\", \"HD\", \"MA\", \"DIS\", \"PYPL\", \"BAC\", \"NFLX\", \"ADBE\",\n    \"CRM\", \"CMCSA\", \"XOM\", \"VZ\", \"KO\", \"INTC\", \"PEP\", \"ABT\", \"CSCO\", \"TMO\",\n    \"COST\", \"MRK\", \"WMT\", \"AVGO\", \"ACN\", \"CVX\", \"NKE\", \"LLY\", \"MCD\", \"TXN\",\n    \"QCOM\", \"DHR\", \"UPS\", \"BMY\", \"PM\", \"LIN\", \"NEE\", \"ORCL\", \"RTX\", \"HON\",\n]\nPERIOD = \"5y\"\nFORWARD_HORIZONS = [21, 63, 126]  # 1M, 3M, 6M\n\nfrom data.stock_api import get_historical_data, get_stock_info\nfrom training.feature_engineering import (\n    build_panel_dataset, cross_sectional_normalize, add_ticker_embedding_column,\n)\n\nprint(\"Fetching stock data for %d tickers...\" % len(TICKERS))\nstock_dfs = {}\nstock_infos = {}\nfor ticker in TICKERS:\n    try:\n        df = get_historical_data(ticker, period=PERIOD)\n        if not df.empty and len(df) > 300:\n            stock_dfs[ticker] = df\n            stock_infos[ticker] = get_stock_info(ticker) or {}\n            print(\"  %s: %d rows\" % (ticker, len(df)))\n    except Exception as e:\n        print(\"  WARN: %s failed: %s\" % (ticker, e))\n\nmarket_df = get_historical_data(\"SPY\", period=PERIOD)\nprint(\"  SPY (market): %d rows\" % len(market_df))\n\nvalid_tickers = sorted(stock_dfs.keys())\nprint(\"\\nValid tickers: %d / %d\" % (len(valid_tickers), len(TICKERS)))\n\nprint(\"Building features...\")\npanel = build_panel_dataset(stock_dfs, stock_infos, market_df, FORWARD_HORIZONS)\npanel = cross_sectional_normalize(panel)\npanel, ticker_to_id = add_ticker_embedding_column(panel, valid_tickers)\nprint(\"Panel shape: %s\" % str(panel.shape))\n\n# Save to Drive\npanel.to_parquet(DATA_PATH)\nprint(\"\\nSaved to %s\" % DATA_PATH)",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# === Option B: Load existing dataset from Drive ===\n\nimport pandas as pd\n\npanel = pd.read_parquet(DATA_PATH)\nvalid_tickers = panel.index.get_level_values(1).unique().tolist()\nprint(\"Loaded panel: %s\" % str(panel.shape))\nprint(\"Tickers (%d): %s\" % (len(valid_tickers), valid_tickers))\nprint(\"Date range: %s to %s\" % (\n    panel.index.get_level_values(0).min(),\n    panel.index.get_level_values(0).max(),\n))",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Configure Models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from training.model_config import ModelConfig, ConfigGrid\n\n# All 5 model types in one comparison\nconfigs = [\n    ModelConfig(model_type=\"elastic_net\", learning_rate=0.1, epochs=1),\n    ModelConfig(model_type=\"lightgbm\", learning_rate=0.05, epochs=500,\n                extra_params={\"num_leaves\": 31, \"max_depth\": 6}),\n    ModelConfig(model_type=\"lightgbm\", learning_rate=0.01, epochs=500,\n                extra_params={\"num_leaves\": 63, \"max_depth\": 8}),\n    ModelConfig(model_type=\"lstm_attention\", learning_rate=1e-3, epochs=100,\n                dropout=0.2, sequence_length=60),\n    ModelConfig(model_type=\"transformer\", learning_rate=3e-4, epochs=100,\n                dropout=0.2, sequence_length=60),\n    ModelConfig(model_type=\"hybrid_multimodal\", learning_rate=1e-3, epochs=50,\n                dropout=0.2, batch_size=64,\n                extra_params={\n                    \"n_tickers\": len(valid_tickers),\n                    \"hidden_dim\": 128,\n                    \"fusion_dim\": 128,\n                    \"vae_latent_dim\": 16,\n                    \"patience\": 10,\n                }),\n]\n\nprint(\"Total configs to train: %d\" % len(configs))\nfor i, c in enumerate(configs):\n    print(\"  [%d] %s | lr=%s | dropout=%s | epochs=%s\" % (\n        i, c.model_type, c.learning_rate, c.dropout, c.epochs))",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Train with Walk-Forward Validation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from training.model_config import MultiConfigRunner\nfrom training.model_selection import WalkForwardConfig\n\n# Define walk-forward settings\nHORIZON = \"1M\"  # Change to \"3M\" or \"6M\" as needed\nTARGET_COL = \"fwd_return_21d\"  # Must match horizon: 21d=1M, 63d=3M, 126d=6M\n\nwf_config = WalkForwardConfig(\n    train_start=\"2015-01-01\",\n    test_end=\"2025-01-01\",\n    train_min_months=36,\n    val_months=6,\n    test_months=6,\n    step_months=6,\n    embargo_days=21,\n    expanding=True,\n)\n\n# Feature columns (exclude targets, _close, and ticker_id)\nfeature_cols = [\n    c for c in panel.columns\n    if not c.startswith(\"fwd_return_\")\n    and not c.startswith(\"residual_return_\")\n    and not c.startswith(\"ranked_target_\")\n    and c not in (\"_close\", \"ticker_id\")\n]\nprint(\"Features: %d\" % len(feature_cols))\nprint(\"Target: %s\" % TARGET_COL)\nprint(\"Horizon: %s\" % HORIZON)",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Optional: Prune low-importance features to speed up training\n",
    "from training.feature_engineering import prune_features\n",
    "\n",
    "sample = panel.head(5000)  # Use a sample for feature selection\n",
    "_, selected_cols = prune_features(\n",
    "    sample[feature_cols],\n",
    "    sample[TARGET_COL],\n",
    "    importance_threshold=0.005,\n",
    ")\n",
    "print(f\"Pruned: {len(feature_cols)} -> {len(selected_cols)} features\")\n",
    "\n",
    "# Uncomment to use pruned features:\n",
    "# feature_cols = selected_cols"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "runner = MultiConfigRunner(save_to_registry=False)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"=\" * 60)\n",
    "t0 = time.time()\n",
    "\n",
    "results = runner.run(\n",
    "    configs=configs,\n",
    "    panel=panel,\n",
    "    target_col=TARGET_COL,\n",
    "    feature_cols=feature_cols,\n",
    "    wf_config=wf_config,\n",
    "    horizon=HORIZON,\n",
    ")\n",
    "\n",
    "total_time = time.time() - t0\n",
    "print(\"=\" * 60)\n",
    "print(f\"Training complete in {total_time:.1f}s\")\n",
    "print(f\"Configs evaluated: {len(results)}\")"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Compare Results"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Build results table\n",
    "rows = []\n",
    "for r in results:\n",
    "    ev = r.evaluation\n",
    "    rows.append({\n",
    "        \"Model\": r.config.model_type,\n",
    "        \"LR\": r.config.learning_rate,\n",
    "        \"Dropout\": r.config.dropout,\n",
    "        \"IC\": round(ev.mean_ic, 4),\n",
    "        \"ICIR\": round(ev.icir, 2),\n",
    "        \"Sharpe\": round(ev.mean_sharpe, 2),\n",
    "        \"Max DD\": round(ev.mean_mdd, 4),\n",
    "        \"Calmar\": round(ev.mean_calmar, 2),\n",
    "        \"Hit Ratio\": round(ev.mean_hit_ratio, 4),\n",
    "        \"Folds\": len(ev.fold_results),\n",
    "        \"Time (s)\": round(r.training_time, 1),\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(rows)\n",
    "df_results = df_results.sort_values(\"IC\", ascending=False).reset_index(drop=True)\n",
    "print(\"\\nModel Comparison (sorted by IC):\")\n",
    "print(\"=\" * 80)\n",
    "display(df_results)"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Statistical comparison\n",
    "from training.model_comparison import ModelComparisonEngine\n",
    "\n",
    "evaluations = [r.evaluation for r in results]\n",
    "engine = ModelComparisonEngine()\n",
    "report = engine.compare(evaluations)\n",
    "\n",
    "print(\"Rankings by IC:\")\n",
    "for name, val in report.rankings.get(\"ic\", []):\n",
    "    print(f\"  {name}: {val:.4f}\")\n",
    "\n",
    "print(f\"\\nStability scores:\")\n",
    "for name, score in report.stability_scores.items():\n",
    "    print(f\"  {name}: {score:.4f}\")\n",
    "\n",
    "print(f\"\\nBest per horizon: {report.best_per_horizon}\")\n",
    "\n",
    "if report.significance_tests:\n",
    "    print(f\"\\nSignificance tests:\")\n",
    "    for key, test in report.significance_tests.items():\n",
    "        sig = \"YES\" if test[\"ttest_significant_5pct\"] else \"no\"\n",
    "        print(f\"  {key}: p={test['ttest_p_value']:.4f} (significant: {sig})\")"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Visualize results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# IC by model\n",
    "axes[0].barh(df_results[\"Model\"] + \" (lr=\" + df_results[\"LR\"].astype(str) + \")\", df_results[\"IC\"])\n",
    "axes[0].set_xlabel(\"Mean IC\")\n",
    "axes[0].set_title(\"Information Coefficient\")\n",
    "\n",
    "# Sharpe by model\n",
    "axes[1].barh(df_results[\"Model\"] + \" (lr=\" + df_results[\"LR\"].astype(str) + \")\", df_results[\"Sharpe\"])\n",
    "axes[1].set_xlabel(\"Mean Sharpe\")\n",
    "axes[1].set_title(\"Sharpe Ratio\")\n",
    "\n",
    "# Training time\n",
    "axes[2].barh(df_results[\"Model\"] + \" (lr=\" + df_results[\"LR\"].astype(str) + \")\", df_results[\"Time (s)\"])\n",
    "axes[2].set_xlabel(\"Seconds\")\n",
    "axes[2].set_title(\"Training Time\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"comparison.png\"), dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7. Save Best Models to Drive"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "# Save all results\n",
    "results_data = []\n",
    "for i, r in enumerate(results):\n",
    "    model_dir = os.path.join(OUTPUT_DIR, f\"{r.config.model_type}_v{i}\")\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    # Save config\n",
    "    with open(os.path.join(model_dir, \"config.json\"), \"w\") as f:\n",
    "        json.dump(r.config.to_json(), f, indent=2)\n",
    "\n",
    "    # Save metrics\n",
    "    metrics = {\n",
    "        \"mean_ic\": r.evaluation.mean_ic,\n",
    "        \"icir\": r.evaluation.icir,\n",
    "        \"mean_sharpe\": r.evaluation.mean_sharpe,\n",
    "        \"mean_mdd\": r.evaluation.mean_mdd,\n",
    "        \"mean_calmar\": r.evaluation.mean_calmar,\n",
    "        \"mean_hit_ratio\": r.evaluation.mean_hit_ratio,\n",
    "        \"n_folds\": len(r.evaluation.fold_results),\n",
    "    }\n",
    "    with open(os.path.join(model_dir, \"metrics.json\"), \"w\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "\n",
    "    results_data.append({\n",
    "        \"config\": r.config.to_json(),\n",
    "        \"training_time\": r.training_time,\n",
    "        \"best_params\": r.best_params,\n",
    "        \"metrics\": metrics,\n",
    "    })\n",
    "\n",
    "    print(f\"Saved: {model_dir}\")\n",
    "\n",
    "# Save combined results JSON (for local import)\n",
    "results_path = os.path.join(OUTPUT_DIR, \"results.json\")\n",
    "with open(results_path, \"w\") as f:\n",
    "    json.dump(results_data, f, indent=2)\n",
    "\n",
    "print(f\"\\nAll results saved to {OUTPUT_DIR}\")\n",
    "print(f\"Import locally with: DataExporter.import_results('{results_path}')\")"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8. (Optional) Hyperparameter Search with Optuna"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Run Optuna HP search for the best model type\nfrom training.hyperparameter_search import HyperparameterSearcher\n\nbest_model_type = results[0].config.model_type\nprint(\"Running Optuna HP search for: %s\" % best_model_type)\n\nsearcher = HyperparameterSearcher(\n    model_type=best_model_type,\n    panel=panel,\n    target_col=TARGET_COL,\n    feature_cols=feature_cols,\n    outer_config=wf_config,\n    n_trials=20,\n    inner_folds_count=3,\n)\n\nsearch_results = searcher.search()\n\nprint(\"\\nBest params per fold:\")\nfor fold_idx, params in search_results.get(\"best_params_per_fold\", {}).items():\n    print(\"  Fold %s: %s\" % (fold_idx, params))\n\neval_result = search_results.get(\"evaluation\")\nif eval_result:\n    print(\"\\nEvaluation after HP search:\")\n    print(\"  IC: %.4f\" % eval_result.mean_ic)\n    print(\"  ICIR: %.2f\" % eval_result.icir)\n    print(\"  Sharpe: %.2f\" % eval_result.mean_sharpe)",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 9. Export Production Artifacts\n\nExport the best model from section 6 as production artifacts for the FastAPI backend.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import json\nimport pickle\nimport numpy as np\nfrom datetime import datetime\nfrom training.models import create_model\nfrom training.model_selection import compute_prediction_metrics\n\n# Use the best model from training results (sorted by IC)\nbest_result = results[0]\nbest_config = best_result.config\nprint(\"Best model: %s (IC=%.4f, Sharpe=%.2f)\" % (\n    best_config.model_type, best_result.evaluation.mean_ic, best_result.evaluation.mean_sharpe))\n\n# Retrain best model on full data for production\nprint(\"\\nRetraining %s on full dataset for production...\" % best_config.model_type)\n\nX = panel[feature_cols].values.astype(np.float32)\ny = panel[TARGET_COL].values.astype(np.float32)\nnp.nan_to_num(X, copy=False, nan=0.0, posinf=0.0, neginf=0.0)\nnp.nan_to_num(y, copy=False, nan=0.0, posinf=0.0, neginf=0.0)\n\nsplit = int(len(X) * 0.85)\nval_split = int(len(X) * 0.95)\n\nmodel = create_model(best_config.model_type, best_config.to_dict())\nmodel.fit(X[:split], y[:split], X[split:val_split], y[split:val_split], feature_names=feature_cols)\n\n# Evaluate on held-out test set\ntest_preds = model.predict(X[val_split:])\nvalid_mask = ~np.isnan(test_preds)\ntest_metrics = compute_prediction_metrics(y[val_split:][valid_mask], test_preds[valid_mask])\nprint(\"Production model test IC: %.4f, Hit ratio: %.4f\" % (test_metrics.ic, test_metrics.hit_ratio))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Save all artifacts to Drive and repo\nos.makedirs(ARTIFACT_DIR, exist_ok=True)\n\n# 1. Model\nmodel_path = os.path.join(ARTIFACT_DIR, \"model.pkl\")\nwith open(model_path, \"wb\") as f:\n    pickle.dump(model, f)\nprint(\"Model saved: %s\" % model_path)\n\nif hasattr(model, 'net'):\n    import torch\n    model.net.eval()\n    torch.save(model.net.state_dict(), os.path.join(ARTIFACT_DIR, \"model.pt\"))\n    print(\"State dict saved\")\n\n# 2. Feature scaler\nif hasattr(model, 'scaler'):\n    with open(os.path.join(ARTIFACT_DIR, \"feature_scaler.pkl\"), \"wb\") as f:\n        pickle.dump(model.scaler, f)\n    print(\"Scaler saved\")\n\n# 3. Config\nconfig_data = {\n    \"model_type\": best_config.model_type,\n    \"horizons\": [\"1M\", \"3M\", \"6M\"],\n    \"horizon_days\": [21, 63, 126],\n    \"n_features\": len(feature_cols),\n    \"n_tickers\": len(valid_tickers),\n}\nconfig_data.update(best_config.extra_params)\nwith open(os.path.join(ARTIFACT_DIR, \"config.json\"), \"w\") as f:\n    json.dump(config_data, f, indent=2)\n\n# 4. Feature columns\nwith open(os.path.join(ARTIFACT_DIR, \"feature_columns.json\"), \"w\") as f:\n    json.dump(feature_cols, f)\n\n# 5. Ticker list\nwith open(os.path.join(ARTIFACT_DIR, \"ticker_list.json\"), \"w\") as f:\n    json.dump(valid_tickers, f)\n\n# 6. Training metadata\nmetadata = {\n    \"version\": \"%s_v%s\" % (best_config.model_type, datetime.now().strftime(\"%Y%m%d_%H%M%S\")),\n    \"trained_at\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n    \"model_type\": best_config.model_type,\n    \"n_tickers\": len(valid_tickers),\n    \"n_features\": len(feature_cols),\n    \"n_samples\": len(X),\n    \"train_size\": split,\n    \"test_ic\": float(test_metrics.ic),\n    \"test_hit_ratio\": float(test_metrics.hit_ratio),\n    \"walkforward_ic\": float(best_result.evaluation.mean_ic),\n    \"walkforward_sharpe\": float(best_result.evaluation.mean_sharpe),\n    \"tickers\": valid_tickers,\n}\nwith open(os.path.join(ARTIFACT_DIR, \"training_metadata.json\"), \"w\") as f:\n    json.dump(metadata, f, indent=2)\n\nprint(\"\\nAll artifacts exported to: %s\" % ARTIFACT_DIR)\nprint(\"Version: %s\" % metadata[\"version\"])\n\n# Copy to repo for API serving\nLOCAL_ARTIFACTS = \"/content/AI-stock-investment-tool/artifacts\"\nos.makedirs(LOCAL_ARTIFACTS, exist_ok=True)\n!cp -r {ARTIFACT_DIR}/* {LOCAL_ARTIFACTS}/\nprint(\"Copied to repo artifacts/ for API serving\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 10. Serve API via ngrok (Connect to Frontend)\n\nStart the FastAPI backend on Colab and expose it via ngrok so your local Next.js frontend can call it.\n\n**Prerequisites:** You need a free [ngrok auth token](https://dashboard.ngrok.com/get-started/your-authtoken). Free tier gives 1 tunnel.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import subprocess\nimport time\nfrom pyngrok import ngrok\n\n# --- Configure ngrok ---\n# Get your free auth token at: https://dashboard.ngrok.com/get-started/your-authtoken\nNGROK_AUTH_TOKEN = \"\"  # <-- Paste your token here\n\nif NGROK_AUTH_TOKEN:\n    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\nelse:\n    print(\"WARNING: No ngrok auth token set. Tunnel may not work.\")\n    print(\"Get one at: https://dashboard.ngrok.com/get-started/your-authtoken\")\n\n# Make sure artifacts are in the repo dir\nLOCAL_ARTIFACTS = \"/content/AI-stock-investment-tool/artifacts\"\nassert os.path.exists(os.path.join(LOCAL_ARTIFACTS, \"model.pkl\")), \\\n    \"No model.pkl found! Run sections 9-10 first to train and export.\"\n\n# Install uvicorn if not present\n!pip install -q uvicorn fastapi pydantic python-multipart\n\n# Start FastAPI in background\nos.chdir(\"/content/AI-stock-investment-tool\")\nserver_proc = subprocess.Popen(\n    [\"python\", \"-m\", \"uvicorn\", \"api.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"],\n    stdout=subprocess.PIPE,\n    stderr=subprocess.PIPE,\n)\ntime.sleep(3)\n\n# Check if server started successfully\nif server_proc.poll() is not None:\n    print(\"ERROR: Server failed to start!\")\n    print(server_proc.stderr.read().decode())\nelse:\n    print(\"FastAPI server started (PID: %d)\" % server_proc.pid)\n\n    # Open ngrok tunnel\n    public_url = ngrok.connect(8000)\n    print(\"\\n\" + \"=\" * 60)\n    print(\"PUBLIC API URL: %s\" % public_url)\n    print(\"=\" * 60)\n    print(\"\\nAPI Docs:  %s/docs\" % public_url)\n    print(\"Health:    %s/api/v1/health\" % public_url)\n    print(\"Predict:   %s/api/v1/predict\" % public_url)\n    print(\"\\n--- To connect your local frontend ---\")\n    print(\"Option A (PowerShell): set env var before npm run dev:\")\n    print('  $env:NEXT_PUBLIC_API_URL=\"%s\"' % public_url)\n    print(\"  cd frontend; npm run dev\")\n    print(\"\\nOption B: Create frontend/.env.local with:\")\n    print(\"  NEXT_PUBLIC_API_URL=%s\" % public_url)\n    print(\"\\nKeep this cell running! The tunnel closes when the runtime stops.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Quick test: verify API is responding\nimport urllib.request\nimport json\n\ntest_url = \"http://localhost:8000/api/v1/health\"\ntry:\n    resp = urllib.request.urlopen(test_url, timeout=5)\n    data = json.loads(resp.read().decode())\n    print(\"Health check OK:\")\n    for k, v in data.items():\n        print(\"  %s: %s\" % (k, v))\nexcept Exception as e:\n    print(\"Health check failed: %s\" % e)\n    print(\"Check server logs:\")\n    if server_proc.poll() is not None:\n        print(server_proc.stderr.read().decode()[-500:])",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 11. Download Artifacts to Local Machine\n\nIf you prefer to run the API locally instead of via ngrok, download the trained artifacts.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Zip artifacts and download to your local machine\n!cd {ARTIFACT_DIR} && zip -r /content/artifacts.zip .\n\nprint(\"Artifact contents:\")\n!ls -lh {ARTIFACT_DIR}\n\nprint(\"\\nTotal zip size:\")\n!ls -lh /content/artifacts.zip\n\n# Download via browser\nfrom google.colab import files\nfiles.download(\"/content/artifacts.zip\")\n\nprint(\"\\nAfter downloading, on your local machine:\")\nprint(\"  1. Unzip into your project root:\")\nprint(\"     unzip artifacts.zip -d artifacts/\")\nprint(\"  2. Start the API:\")\nprint(\"     python -m api.main\")\nprint(\"  3. Start the frontend:\")\nprint(\"     cd frontend && npm run dev\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 12. Cleanup\n\nStop the API server and ngrok tunnel when done.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Stop server and tunnel\ntry:\n    ngrok.disconnect(public_url)\n    ngrok.kill()\n    print(\"ngrok tunnel closed\")\nexcept Exception:\n    pass\n\ntry:\n    server_proc.terminate()\n    server_proc.wait(timeout=5)\n    print(\"FastAPI server stopped\")\nexcept Exception:\n    pass",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}