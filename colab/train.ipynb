{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# AI Stock Investment Tool - Colab Training\n\nTrain and compare all model types (including Hybrid Multi-Modal) on GPU, then serve the API for your frontend.\n\n**Before starting:**\n1. Go to **Runtime > Change runtime type > T4 GPU**\n2. Run cells **in order** from top to bottom\n3. If repo is private, you'll need a [GitHub Personal Access Token](https://github.com/settings/tokens) with `repo` scope\n\n**Sections:**\n- 1-2: Setup & environment\n- 3: Fetch data & build features (50-ticker universe)\n- 4-6: Configure, train, and compare all models (+ dashboard)\n- 7: Save results to Drive\n- 8: Optuna hyperparameter search (optional)\n- 9: Export production artifacts (best model)\n- 9b: Prediction visualizations & model scorecard\n- 10: Serve API via ngrok (connect to frontend)\n- 11: Download artifacts to local machine",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Setup"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "!pip install -q yfinance lightgbm torch optuna pyarrow scikit-learn scipy pandas numpy matplotlib feedparser pyngrok",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T12:35:45.185035800Z",
     "start_time": "2026-02-09T12:35:23.067676700Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import os\nos.chdir(\"/content\")\n\n# Clean previous clone if any\n!rm -rf AI-stock-investment-tool\n\n# Try public clone first, fall back to token auth\nREPO = \"https://github.com/kevin6598/AI-stock-investment-tool.git\"\nret = os.system(\"git clone %s 2>/dev/null\" % REPO)\n\nif ret != 0:\n    from getpass import getpass\n    print(\"Public clone failed -- repo is private.\")\n    print(\"Create a token at: https://github.com/settings/tokens (repo scope)\")\n    token = getpass(\"Paste your GitHub token: \")\n    os.system(\"git clone https://%s@github.com/kevin6598/AI-stock-investment-tool.git\" % token)\n    del token\n\nos.chdir(\"/content/AI-stock-investment-tool\")\nprint(\"Working dir: %s\" % os.getcwd())\n!git log --oneline -3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T12:35:45.593941800Z",
     "start_time": "2026-02-09T12:35:45.188027900Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import torch, sys\nprint(f\"Python: {sys.version}\")\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\nelse:\n    print(\"WARNING: No GPU detected. Go to Runtime > Change runtime type > T4 GPU\")",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Mount Google Drive & Load Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import os\n\n# Configure paths -- artifacts persist on Google Drive across sessions\nDRIVE_DIR = \"/content/drive/MyDrive/ai_stock_tool\"\nos.makedirs(DRIVE_DIR, exist_ok=True)\n\nDATA_PATH = os.path.join(DRIVE_DIR, \"dataset.parquet\")\nOUTPUT_DIR = os.path.join(DRIVE_DIR, \"models_registry\")\nARTIFACT_DIR = os.path.join(DRIVE_DIR, \"artifacts\")\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nos.makedirs(ARTIFACT_DIR, exist_ok=True)\n\nprint(\"Drive dir:    %s\" % DRIVE_DIR)\nprint(\"Data path:    %s\" % DATA_PATH)\nprint(\"Output dir:   %s\" % OUTPUT_DIR)\nprint(\"Artifact dir: %s\" % ARTIFACT_DIR)",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 3. Fetch Data & Build Features\n\nFetches 50-ticker universe with ticker embeddings (needed for Hybrid Multi-Modal).\nIf you already built the dataset, skip Option A and use Option B to load from Drive.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# === Option A: Build dataset from scratch (50-ticker universe) ===\n\nTICKERS = [\n    \"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"META\", \"NVDA\", \"TSLA\", \"BRK-B\", \"JPM\", \"JNJ\",\n    \"V\", \"PG\", \"UNH\", \"HD\", \"MA\", \"DIS\", \"PYPL\", \"BAC\", \"NFLX\", \"ADBE\",\n    \"CRM\", \"CMCSA\", \"XOM\", \"VZ\", \"KO\", \"INTC\", \"PEP\", \"ABT\", \"CSCO\", \"TMO\",\n    \"COST\", \"MRK\", \"WMT\", \"AVGO\", \"ACN\", \"CVX\", \"NKE\", \"LLY\", \"MCD\", \"TXN\",\n    \"QCOM\", \"DHR\", \"UPS\", \"BMY\", \"PM\", \"LIN\", \"NEE\", \"ORCL\", \"RTX\", \"HON\",\n]\nPERIOD = \"5y\"\nFORWARD_HORIZONS = [21, 63, 126]  # 1M, 3M, 6M\n\nfrom data.stock_api import get_historical_data, get_stock_info\nfrom training.feature_engineering import (\n    build_panel_dataset, cross_sectional_normalize, add_ticker_embedding_column,\n)\n\nprint(\"Fetching stock data for %d tickers...\" % len(TICKERS))\nstock_dfs = {}\nstock_infos = {}\nfor ticker in TICKERS:\n    try:\n        df = get_historical_data(ticker, period=PERIOD)\n        if not df.empty and len(df) > 300:\n            stock_dfs[ticker] = df\n            stock_infos[ticker] = get_stock_info(ticker) or {}\n            print(\"  %s: %d rows\" % (ticker, len(df)))\n    except Exception as e:\n        print(\"  WARN: %s failed: %s\" % (ticker, e))\n\nmarket_df = get_historical_data(\"SPY\", period=PERIOD)\nprint(\"  SPY (market): %d rows\" % len(market_df))\n\nvalid_tickers = sorted(stock_dfs.keys())\nprint(\"\\nValid tickers: %d / %d\" % (len(valid_tickers), len(TICKERS)))\n\nprint(\"Building features...\")\npanel = build_panel_dataset(stock_dfs, stock_infos, market_df, FORWARD_HORIZONS)\npanel = cross_sectional_normalize(panel)\npanel, ticker_to_id = add_ticker_embedding_column(panel, valid_tickers)\nprint(\"Panel shape: %s\" % str(panel.shape))\n\n# Save to Drive\npanel.to_parquet(DATA_PATH)\nprint(\"\\nSaved to %s\" % DATA_PATH)",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# === Option B: Load existing dataset from Drive ===\n\nimport pandas as pd\n\npanel = pd.read_parquet(DATA_PATH)\nvalid_tickers = panel.index.get_level_values(1).unique().tolist()\nprint(\"Loaded panel: %s\" % str(panel.shape))\nprint(\"Tickers (%d): %s\" % (len(valid_tickers), valid_tickers))\nprint(\"Date range: %s to %s\" % (\n    panel.index.get_level_values(0).min(),\n    panel.index.get_level_values(0).max(),\n))",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# --- Dual Sentiment Engine Demo ---\n# Shows both FinBERT and Sentence Embedding outputs + IC validation\n\nfrom training.feature_engineering import validate_sentiment_ic\nimport pandas as pd\n\n# Run IC validation on the panel's NLP features\nnlp_cols = [c for c in panel.columns if c.startswith(\"nlp_\")]\nprint(\"NLP sentiment features (%d):\" % len(nlp_cols))\nfor c in nlp_cols:\n    print(\"  %s\" % c)\n\n# Validate IC against 1M forward returns\nTARGET_COL_IC = \"fwd_return_21d\"\nif TARGET_COL_IC in panel.columns and nlp_cols:\n    # Use a representative subset for IC computation\n    sample_panel = panel.dropna(subset=[TARGET_COL_IC])\n    validated_panel, ic_report = validate_sentiment_ic(\n        sample_panel[nlp_cols].head(10000),\n        sample_panel[TARGET_COL_IC].head(10000),\n        ic_threshold=0.01,\n    )\n\n    print(\"\\nPer-feature IC values:\")\n    print(\"-\" * 50)\n    for feat, ic_val in sorted(ic_report.items(), key=lambda x: abs(x[1]), reverse=True):\n        status = \"PASS\" if abs(ic_val) >= 0.01 else \"FAIL\"\n        print(\"  %s: IC=%.4f [%s]\" % (feat.ljust(30), ic_val, status))\n\n    n_pass = sum(1 for v in ic_report.values() if abs(v) >= 0.01)\n    print(\"\\nSummary: %d/%d features passed IC threshold\" % (n_pass, len(ic_report)))\nelse:\n    print(\"Skipping IC validation (target or NLP cols not available)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 3b. Dual Sentiment Engine & IC Validation\n\nRun the dual sentiment engine (FinBERT + Sentence Embedding) on a sample ticker\nand validate per-feature IC against forward returns.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Configure Models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from training.model_config import ModelConfig, ConfigGrid\n\n# All 5 model types in one comparison\nconfigs = [\n    ModelConfig(model_type=\"elastic_net\", learning_rate=0.1, epochs=1),\n    ModelConfig(model_type=\"lightgbm\", learning_rate=0.05, epochs=500,\n                extra_params={\"num_leaves\": 31, \"max_depth\": 6}),\n    ModelConfig(model_type=\"lightgbm\", learning_rate=0.01, epochs=500,\n                extra_params={\"num_leaves\": 63, \"max_depth\": 8}),\n    ModelConfig(model_type=\"lstm_attention\", learning_rate=1e-3, epochs=100,\n                dropout=0.2, sequence_length=60),\n    ModelConfig(model_type=\"transformer\", learning_rate=3e-4, epochs=100,\n                dropout=0.2, sequence_length=60),\n    ModelConfig(model_type=\"hybrid_multimodal\", learning_rate=1e-3, epochs=50,\n                dropout=0.2, batch_size=64,\n                extra_params={\n                    \"n_tickers\": len(valid_tickers),\n                    \"hidden_dim\": 128,\n                    \"fusion_dim\": 128,\n                    \"vae_latent_dim\": 16,\n                    \"patience\": 10,\n                }),\n]\n\nprint(\"Total configs to train: %d\" % len(configs))\nfor i, c in enumerate(configs):\n    print(\"  [%d] %s | lr=%s | dropout=%s | epochs=%s\" % (\n        i, c.model_type, c.learning_rate, c.dropout, c.epochs))",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Train with Walk-Forward Validation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from training.model_config import MultiConfigRunner\nfrom training.model_selection import WalkForwardConfig\n\n# Define walk-forward settings\nHORIZON = \"1M\"  # Change to \"3M\" or \"6M\" as needed\nTARGET_COL = \"fwd_return_21d\"  # Must match horizon: 21d=1M, 63d=3M, 126d=6M\n\nwf_config = WalkForwardConfig(\n    train_start=\"2015-01-01\",\n    test_end=\"2025-01-01\",\n    train_min_months=36,\n    val_months=6,\n    test_months=6,\n    step_months=6,\n    embargo_days=21,\n    expanding=True,\n)\n\n# Feature columns (exclude targets, _close, and ticker_id)\nfeature_cols = [\n    c for c in panel.columns\n    if not c.startswith(\"fwd_return_\")\n    and not c.startswith(\"residual_return_\")\n    and not c.startswith(\"ranked_target_\")\n    and c not in (\"_close\", \"ticker_id\")\n]\nprint(\"Features: %d\" % len(feature_cols))\nprint(\"Target: %s\" % TARGET_COL)\nprint(\"Horizon: %s\" % HORIZON)",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Optional: Prune low-importance features to speed up training\n",
    "from training.feature_engineering import prune_features\n",
    "\n",
    "sample = panel.head(5000)  # Use a sample for feature selection\n",
    "_, selected_cols = prune_features(\n",
    "    sample[feature_cols],\n",
    "    sample[TARGET_COL],\n",
    "    importance_threshold=0.005,\n",
    ")\n",
    "print(f\"Pruned: {len(feature_cols)} -> {len(selected_cols)} features\")\n",
    "\n",
    "# Uncomment to use pruned features:\n",
    "# feature_cols = selected_cols"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "runner = MultiConfigRunner(save_to_registry=False)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"=\" * 60)\n",
    "t0 = time.time()\n",
    "\n",
    "results = runner.run(\n",
    "    configs=configs,\n",
    "    panel=panel,\n",
    "    target_col=TARGET_COL,\n",
    "    feature_cols=feature_cols,\n",
    "    wf_config=wf_config,\n",
    "    horizon=HORIZON,\n",
    ")\n",
    "\n",
    "total_time = time.time() - t0\n",
    "print(\"=\" * 60)\n",
    "print(f\"Training complete in {total_time:.1f}s\")\n",
    "print(f\"Configs evaluated: {len(results)}\")"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Compare Results"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Build results table\n",
    "rows = []\n",
    "for r in results:\n",
    "    ev = r.evaluation\n",
    "    rows.append({\n",
    "        \"Model\": r.config.model_type,\n",
    "        \"LR\": r.config.learning_rate,\n",
    "        \"Dropout\": r.config.dropout,\n",
    "        \"IC\": round(ev.mean_ic, 4),\n",
    "        \"ICIR\": round(ev.icir, 2),\n",
    "        \"Sharpe\": round(ev.mean_sharpe, 2),\n",
    "        \"Max DD\": round(ev.mean_mdd, 4),\n",
    "        \"Calmar\": round(ev.mean_calmar, 2),\n",
    "        \"Hit Ratio\": round(ev.mean_hit_ratio, 4),\n",
    "        \"Folds\": len(ev.fold_results),\n",
    "        \"Time (s)\": round(r.training_time, 1),\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(rows)\n",
    "df_results = df_results.sort_values(\"IC\", ascending=False).reset_index(drop=True)\n",
    "print(\"\\nModel Comparison (sorted by IC):\")\n",
    "print(\"=\" * 80)\n",
    "display(df_results)"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Statistical comparison\n",
    "from training.model_comparison import ModelComparisonEngine\n",
    "\n",
    "evaluations = [r.evaluation for r in results]\n",
    "engine = ModelComparisonEngine()\n",
    "report = engine.compare(evaluations)\n",
    "\n",
    "print(\"Rankings by IC:\")\n",
    "for name, val in report.rankings.get(\"ic\", []):\n",
    "    print(f\"  {name}: {val:.4f}\")\n",
    "\n",
    "print(f\"\\nStability scores:\")\n",
    "for name, score in report.stability_scores.items():\n",
    "    print(f\"  {name}: {score:.4f}\")\n",
    "\n",
    "print(f\"\\nBest per horizon: {report.best_per_horizon}\")\n",
    "\n",
    "if report.significance_tests:\n",
    "    print(f\"\\nSignificance tests:\")\n",
    "    for key, test in report.significance_tests.items():\n",
    "        sig = \"YES\" if test[\"ttest_significant_5pct\"] else \"no\"\n",
    "        print(f\"  {key}: p={test['ttest_p_value']:.4f} (significant: {sig})\")"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Visualize results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# IC by model\n",
    "axes[0].barh(df_results[\"Model\"] + \" (lr=\" + df_results[\"LR\"].astype(str) + \")\", df_results[\"IC\"])\n",
    "axes[0].set_xlabel(\"Mean IC\")\n",
    "axes[0].set_title(\"Information Coefficient\")\n",
    "\n",
    "# Sharpe by model\n",
    "axes[1].barh(df_results[\"Model\"] + \" (lr=\" + df_results[\"LR\"].astype(str) + \")\", df_results[\"Sharpe\"])\n",
    "axes[1].set_xlabel(\"Mean Sharpe\")\n",
    "axes[1].set_title(\"Sharpe Ratio\")\n",
    "\n",
    "# Training time\n",
    "axes[2].barh(df_results[\"Model\"] + \" (lr=\" + df_results[\"LR\"].astype(str) + \")\", df_results[\"Time (s)\"])\n",
    "axes[2].set_xlabel(\"Seconds\")\n",
    "axes[2].set_title(\"Training Time\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"comparison.png\"), dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# === Model Comparison Dashboard ===\n# 4-panel view: radar chart, grouped bars, color table, winner summary\n\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nfrom matplotlib.patches import FancyBboxPatch\nimport numpy as np\n\n# --- Prepare data ---\nmodel_labels = []\nfor _, row in df_results.iterrows():\n    lbl = \"%s (lr=%s)\" % (row[\"Model\"], row[\"LR\"])\n    model_labels.append(lbl)\n\nmetric_names = [\"IC\", \"ICIR\", \"Sharpe\", \"Hit Ratio\", \"Calmar\"]\nraw_values = df_results[metric_names].values.astype(float)\n\n# Normalize each metric to 0-1 for radar chart\nmins = raw_values.min(axis=0)\nmaxs = raw_values.max(axis=0)\nranges = maxs - mins\nranges[ranges == 0] = 1.0\nnorm_values = (raw_values - mins) / ranges\n\nn_models = len(model_labels)\nn_metrics = len(metric_names)\ncmap = plt.cm.get_cmap(\"tab10\")\ncolors = [cmap(i) for i in range(n_models)]\n\nfig = plt.figure(figsize=(18, 14))\ngs = gridspec.GridSpec(2, 2, hspace=0.35, wspace=0.3)\n\n# --- Panel 1: Radar chart ---\nax_radar = fig.add_subplot(gs[0, 0], polar=True)\nangles = np.linspace(0, 2 * np.pi, n_metrics, endpoint=False).tolist()\nangles.append(angles[0])  # close the polygon\n\nfor i in range(n_models):\n    vals = norm_values[i].tolist()\n    vals.append(vals[0])\n    ax_radar.plot(angles, vals, \"o-\", color=colors[i], linewidth=2, label=model_labels[i])\n    ax_radar.fill(angles, vals, alpha=0.08, color=colors[i])\n\nax_radar.set_xticks(angles[:-1])\nax_radar.set_xticklabels(metric_names, fontsize=10)\nax_radar.set_ylim(0, 1.1)\nax_radar.set_title(\"Normalized Metric Profiles\", fontsize=13, fontweight=\"bold\", pad=20)\nax_radar.legend(loc=\"upper right\", bbox_to_anchor=(1.35, 1.15), fontsize=8)\n\n# --- Panel 2: Grouped bar chart ---\nax_bar = fig.add_subplot(gs[0, 1])\nbar_metrics = [\"IC\", \"ICIR\", \"Sharpe\"]\nx = np.arange(len(bar_metrics))\nwidth = 0.8 / n_models\n\nfor i in range(n_models):\n    offsets = x + (i - n_models / 2.0 + 0.5) * width\n    vals = [df_results.iloc[i][m] for m in bar_metrics]\n    ax_bar.bar(offsets, vals, width, color=colors[i], label=model_labels[i], edgecolor=\"white\")\n\nax_bar.set_xticks(x)\nax_bar.set_xticklabels(bar_metrics, fontsize=11)\nax_bar.set_title(\"Key Metrics Comparison\", fontsize=13, fontweight=\"bold\")\nax_bar.legend(fontsize=7, loc=\"upper right\")\nax_bar.axhline(y=0, color=\"gray\", linewidth=0.5, linestyle=\"--\")\nax_bar.grid(axis=\"y\", alpha=0.3)\n\n# --- Panel 3: Color-coded metrics table ---\nax_table = fig.add_subplot(gs[1, 0])\nax_table.axis(\"off\")\n\ntable_metrics = [\"IC\", \"ICIR\", \"Sharpe\", \"Hit Ratio\", \"Calmar\", \"Max DD\", \"Time (s)\"]\ncell_text = []\ncell_colors = []\n\nfor i in range(n_models):\n    row_text = []\n    row_colors = []\n    for m in table_metrics:\n        val = df_results.iloc[i][m]\n        row_text.append(\"%.4f\" % val if m in (\"IC\", \"Hit Ratio\", \"Max DD\") else \"%.2f\" % val)\n        # Color: green for best, red for worst per column\n        col_vals = df_results[m].values\n        if m == \"Max DD\":\n            # Lower is better for max drawdown\n            is_best = (val == col_vals.min())\n            is_worst = (val == col_vals.max())\n        elif m == \"Time (s)\":\n            is_best = (val == col_vals.min())\n            is_worst = (val == col_vals.max())\n        else:\n            is_best = (val == col_vals.max())\n            is_worst = (val == col_vals.min())\n\n        if n_models == 1:\n            row_colors.append(\"#ffffff\")\n        elif is_best:\n            row_colors.append(\"#c6efce\")\n        elif is_worst:\n            row_colors.append(\"#ffc7ce\")\n        else:\n            row_colors.append(\"#ffffff\")\n    cell_text.append(row_text)\n    cell_colors.append(row_colors)\n\ntbl = ax_table.table(\n    cellText=cell_text,\n    rowLabels=[lbl[:25] for lbl in model_labels],\n    colLabels=table_metrics,\n    cellColours=cell_colors,\n    loc=\"center\",\n    cellLoc=\"center\",\n)\ntbl.auto_set_font_size(False)\ntbl.set_fontsize(9)\ntbl.scale(1.0, 1.4)\nax_table.set_title(\"Metrics Table (green=best, red=worst)\", fontsize=13, fontweight=\"bold\", pad=15)\n\n# --- Panel 4: Winner summary ---\nax_winner = fig.add_subplot(gs[1, 1])\nax_winner.axis(\"off\")\n\n# Composite score: IC + 0.5*ICIR + 0.3*Sharpe + 0.2*HitRatio + 0.1*Calmar\ncomposite = (\n    norm_values[:, 0] * 1.0    # IC\n    + norm_values[:, 1] * 0.5  # ICIR\n    + norm_values[:, 2] * 0.3  # Sharpe\n    + norm_values[:, 3] * 0.2  # Hit Ratio\n    + norm_values[:, 4] * 0.1  # Calmar\n)\noverall_idx = int(np.argmax(composite))\n\nlines = []\nlines.append(\"OVERALL WINNER\")\nlines.append(\"  %s\" % model_labels[overall_idx])\nlines.append(\"  Composite score: %.3f / %.3f\" % (composite[overall_idx], 2.1))\nlines.append(\"\")\n\n# Category winners\nfor j, m in enumerate(metric_names):\n    if m == \"Max DD\":\n        best_idx = int(np.argmin(raw_values[:, j]))\n    else:\n        best_idx = int(np.argmax(raw_values[:, j]))\n    lines.append(\"Best %s: %s (%.4f)\" % (m, model_labels[best_idx], raw_values[best_idx, j]))\n\nlines.append(\"\")\nlines.append(\"Fastest: %s (%.1fs)\" % (\n    model_labels[int(df_results[\"Time (s)\"].values.argmin())],\n    df_results[\"Time (s)\"].min(),\n))\n\ntext = \"\\n\".join(lines)\nax_winner.text(\n    0.05, 0.95, text, transform=ax_winner.transAxes,\n    fontsize=11, verticalalignment=\"top\", fontfamily=\"monospace\",\n    bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"#f0f0f0\", edgecolor=\"#cccccc\"),\n)\nax_winner.set_title(\"Winner Summary\", fontsize=13, fontweight=\"bold\", pad=15)\n\nfig.suptitle(\"Model Comparison Dashboard\", fontsize=16, fontweight=\"bold\", y=1.01)\nplt.savefig(os.path.join(OUTPUT_DIR, \"dashboard.png\"), dpi=150, bbox_inches=\"tight\")\nplt.show()\nprint(\"Dashboard saved to %s/dashboard.png\" % OUTPUT_DIR)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7. Save Best Models to Drive"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "# Save all results\n",
    "results_data = []\n",
    "for i, r in enumerate(results):\n",
    "    model_dir = os.path.join(OUTPUT_DIR, f\"{r.config.model_type}_v{i}\")\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    # Save config\n",
    "    with open(os.path.join(model_dir, \"config.json\"), \"w\") as f:\n",
    "        json.dump(r.config.to_json(), f, indent=2)\n",
    "\n",
    "    # Save metrics\n",
    "    metrics = {\n",
    "        \"mean_ic\": r.evaluation.mean_ic,\n",
    "        \"icir\": r.evaluation.icir,\n",
    "        \"mean_sharpe\": r.evaluation.mean_sharpe,\n",
    "        \"mean_mdd\": r.evaluation.mean_mdd,\n",
    "        \"mean_calmar\": r.evaluation.mean_calmar,\n",
    "        \"mean_hit_ratio\": r.evaluation.mean_hit_ratio,\n",
    "        \"n_folds\": len(r.evaluation.fold_results),\n",
    "    }\n",
    "    with open(os.path.join(model_dir, \"metrics.json\"), \"w\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "\n",
    "    results_data.append({\n",
    "        \"config\": r.config.to_json(),\n",
    "        \"training_time\": r.training_time,\n",
    "        \"best_params\": r.best_params,\n",
    "        \"metrics\": metrics,\n",
    "    })\n",
    "\n",
    "    print(f\"Saved: {model_dir}\")\n",
    "\n",
    "# Save combined results JSON (for local import)\n",
    "results_path = os.path.join(OUTPUT_DIR, \"results.json\")\n",
    "with open(results_path, \"w\") as f:\n",
    "    json.dump(results_data, f, indent=2)\n",
    "\n",
    "print(f\"\\nAll results saved to {OUTPUT_DIR}\")\n",
    "print(f\"Import locally with: DataExporter.import_results('{results_path}')\")"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8. (Optional) Hyperparameter Search with Optuna"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Run Optuna HP search for the best model type\nfrom training.hyperparameter_search import HyperparameterSearcher\n\nbest_model_type = results[0].config.model_type\nprint(\"Running Optuna HP search for: %s\" % best_model_type)\n\nsearcher = HyperparameterSearcher(\n    model_type=best_model_type,\n    panel=panel,\n    target_col=TARGET_COL,\n    feature_cols=feature_cols,\n    outer_config=wf_config,\n    n_trials=20,\n    inner_folds_count=3,\n)\n\nsearch_results = searcher.search()\n\nprint(\"\\nBest params per fold:\")\nfor entry in search_results.get(\"best_params_per_fold\", []):\n    print(\"  Fold %s (inner IC=%.4f): %s\" % (entry[\"fold\"], entry[\"inner_ic\"], entry[\"params\"]))\n\neval_result = search_results.get(\"evaluation\")\nif eval_result:\n    print(\"\\nEvaluation after HP search:\")\n    print(\"  IC: %.4f\" % eval_result.mean_ic)\n    print(\"  ICIR: %.2f\" % eval_result.icir)\n    print(\"  Sharpe: %.2f\" % eval_result.mean_sharpe)",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 9. Export Production Artifacts\n\nExport the best model from section 6 as production artifacts for the FastAPI backend.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import json\nimport pickle\nimport numpy as np\nfrom datetime import datetime\nfrom training.models import create_model\nfrom training.model_selection import compute_prediction_metrics\n\n# Use the best model from training results (sorted by IC)\nbest_result = results[0]\nbest_config = best_result.config\nprint(\"Best model: %s (IC=%.4f, Sharpe=%.2f)\" % (\n    best_config.model_type, best_result.evaluation.mean_ic, best_result.evaluation.mean_sharpe))\n\n# Retrain best model on full data for production\nprint(\"\\nRetraining %s on full dataset for production...\" % best_config.model_type)\n\nX = panel[feature_cols].values.astype(np.float32)\ny = panel[TARGET_COL].values.astype(np.float32)\nnp.nan_to_num(X, copy=False, nan=0.0, posinf=0.0, neginf=0.0)\nnp.nan_to_num(y, copy=False, nan=0.0, posinf=0.0, neginf=0.0)\n\nsplit = int(len(X) * 0.85)\nval_split = int(len(X) * 0.95)\n\nmodel = create_model(best_config.model_type, best_config.to_dict())\nmodel.fit(X[:split], y[:split], X[split:val_split], y[split:val_split], feature_names=feature_cols)\n\n# Evaluate on held-out test set\ntest_preds = model.predict(X[val_split:])\nvalid_mask = ~np.isnan(test_preds)\ntest_metrics = compute_prediction_metrics(y[val_split:][valid_mask], test_preds[valid_mask])\nprint(\"Production model test IC: %.4f, Hit ratio: %.4f\" % (test_metrics.ic, test_metrics.hit_ratio))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Save all artifacts to Drive and repo\nos.makedirs(ARTIFACT_DIR, exist_ok=True)\n\n# 1. Model\nmodel_path = os.path.join(ARTIFACT_DIR, \"model.pkl\")\nwith open(model_path, \"wb\") as f:\n    pickle.dump(model, f)\nprint(\"Model saved: %s\" % model_path)\n\nif hasattr(model, 'net'):\n    import torch\n    model.net.eval()\n    torch.save(model.net.state_dict(), os.path.join(ARTIFACT_DIR, \"model.pt\"))\n    print(\"State dict saved\")\n\n# 2. Feature scaler\nif hasattr(model, 'scaler'):\n    with open(os.path.join(ARTIFACT_DIR, \"feature_scaler.pkl\"), \"wb\") as f:\n        pickle.dump(model.scaler, f)\n    print(\"Scaler saved\")\n\n# 3. Config\nconfig_data = {\n    \"model_type\": best_config.model_type,\n    \"horizons\": [\"1M\", \"3M\", \"6M\"],\n    \"horizon_days\": [21, 63, 126],\n    \"n_features\": len(feature_cols),\n    \"n_tickers\": len(valid_tickers),\n}\nconfig_data.update(best_config.extra_params)\nwith open(os.path.join(ARTIFACT_DIR, \"config.json\"), \"w\") as f:\n    json.dump(config_data, f, indent=2)\n\n# 4. Feature columns\nwith open(os.path.join(ARTIFACT_DIR, \"feature_columns.json\"), \"w\") as f:\n    json.dump(feature_cols, f)\n\n# 5. Ticker list\nwith open(os.path.join(ARTIFACT_DIR, \"ticker_list.json\"), \"w\") as f:\n    json.dump(valid_tickers, f)\n\n# 6. Training metadata\nmetadata = {\n    \"version\": \"%s_v%s\" % (best_config.model_type, datetime.now().strftime(\"%Y%m%d_%H%M%S\")),\n    \"trained_at\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n    \"model_type\": best_config.model_type,\n    \"n_tickers\": len(valid_tickers),\n    \"n_features\": len(feature_cols),\n    \"n_samples\": len(X),\n    \"train_size\": split,\n    \"test_ic\": float(test_metrics.ic),\n    \"test_hit_ratio\": float(test_metrics.hit_ratio),\n    \"walkforward_ic\": float(best_result.evaluation.mean_ic),\n    \"walkforward_sharpe\": float(best_result.evaluation.mean_sharpe),\n    \"tickers\": valid_tickers,\n}\nwith open(os.path.join(ARTIFACT_DIR, \"training_metadata.json\"), \"w\") as f:\n    json.dump(metadata, f, indent=2)\n\nprint(\"\\nAll artifacts exported to: %s\" % ARTIFACT_DIR)\nprint(\"Version: %s\" % metadata[\"version\"])\n\n# Copy to repo for API serving\nLOCAL_ARTIFACTS = \"/content/AI-stock-investment-tool/artifacts\"\nos.makedirs(LOCAL_ARTIFACTS, exist_ok=True)\n!cp -r {ARTIFACT_DIR}/* {LOCAL_ARTIFACTS}/\nprint(\"Copied to repo artifacts/ for API serving\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 9b. Top 10 Stock Picks\n\nRun the Top 10 engine on the trained model to generate market-specific picks.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# --- Top 10 Engine Demo ---\n# Uses the trained production model to generate Top 10 picks\n\nfrom engine.top10 import Top10Engine\nfrom training.feature_engineering import build_feature_matrix\nfrom data.stock_api import get_historical_data, get_stock_info\nimport numpy as np\n\n# Build a predict function using the trained model\n_market_df = get_historical_data(\"SPY\", period=\"2y\")\n\ndef colab_predict(ticker, horizon=\"1M\"):\n    \"\"\"Prediction wrapper for Colab Top 10 demo.\"\"\"\n    stock_df = get_historical_data(ticker, period=\"2y\")\n    if stock_df.empty:\n        raise ValueError(\"No data for %s\" % ticker)\n    info = get_stock_info(ticker) or {}\n    feat = build_feature_matrix(stock_df, info, _market_df, [21], ticker=ticker)\n    if feat.empty:\n        raise ValueError(\"No features for %s\" % ticker)\n    fc = [c for c in feat.columns if not c.startswith(\"fwd_return_\") and c != \"_close\"]\n    X_pred = feat[fc].values[-1:].astype(np.float32)\n    np.nan_to_num(X_pred, copy=False, nan=0.0)\n    point = float(model.predict(X_pred)[0])\n    return {\n        \"ticker\": ticker,\n        \"point_estimate\": point,\n        \"probability_up\": 0.5 + point * 5,\n        \"p_up\": max(0.0, min(1.0, 0.5 + point * 5)),\n        \"confidence\": 0.5,\n        \"risk_score\": 0.4,\n        \"direction\": \"UP\" if point > 0 else \"DOWN\",\n        \"meta_trade_probability\": 0.5,\n        \"uncertainty\": 0.3,\n        \"quantiles\": {\"q10\": point - 0.05, \"p10\": point - 0.05},\n    }\n\nengine = Top10Engine(\n    predict_fn=colab_predict,\n    model_version=metadata.get(\"version\", \"unknown\"),\n)\n\n# Generate Top 10 for US market\nprint(\"Generating Top 10 picks for US market (1M horizon)...\")\nresult = engine.select(market=\"US\", horizon=\"1M\")\n\nif result.stocks:\n    print(\"\\nTop 10 US Picks:\")\n    print(\"=\" * 80)\n    print(\"%-4s %-8s %-5s %7s %8s %8s %7s %8s\" % (\n        \"Rank\", \"Ticker\", \"Dir\", \"Score\", \"P(Up)\", \"Return\", \"Conf\", \"Weight\"))\n    print(\"-\" * 80)\n    for s in result.stocks:\n        print(\"%-4d %-8s %-5s %7.3f %7.1f%% %+7.2f%% %6.1f%% %7.1f%%\" % (\n            s.rank, s.ticker, s.direction, s.score,\n            s.p_up * 100, s.expected_return * 100,\n            s.confidence * 100, s.allocation_weight * 100))\n    print(\"-\" * 80)\n    print(\"Candidates: %d | Pass rate: %.1f%%\" % (\n        result.total_candidates, result.pass_rate * 100))\nelse:\n    print(\"No stocks passed the filter criteria.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 9b. Prediction Visualizations\n\nVisualize the production model's predictions on a specific ticker.\nChange `TICKER` below to analyze a different stock.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# === Prediction Analysis for a Specific Ticker ===\n# 4-panel visualization: scatter, cumulative PnL, confidence bands, rolling hit ratio\n\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport numpy as np\nfrom scipy import stats\n\nTICKER = \"AAPL\"  # <-- Change this to analyze a different stock\n\n# --- Extract ticker data from panel ---\ntry:\n    ticker_data = panel.loc[(slice(None), TICKER), :].droplevel(1)\nexcept KeyError:\n    print(\"Ticker %s not found in panel. Available: %s\" % (TICKER, valid_tickers[:10]))\n    raise\n\nticker_X = ticker_data[feature_cols].values.astype(np.float32)\nticker_y = ticker_data[TARGET_COL].values.astype(np.float32)\nnp.nan_to_num(ticker_X, copy=False, nan=0.0, posinf=0.0, neginf=0.0)\nnp.nan_to_num(ticker_y, copy=False, nan=0.0, posinf=0.0, neginf=0.0)\ndates = ticker_data.index\n\n# Use only the test portion (last 5% mirrors val_split ratio from cell above)\nn_total = len(ticker_X)\nt_split = int(n_total * 0.95)\ntest_X = ticker_X[t_split:]\ntest_y = ticker_y[t_split:]\ntest_dates = dates[t_split:]\n\npreds = model.predict(test_X)\nvalid = ~np.isnan(preds)\npreds_v = preds[valid]\nactual_v = test_y[valid]\ndates_v = test_dates[valid]\n\nprint(\"Ticker: %s | Test samples: %d | Valid predictions: %d\" % (TICKER, len(test_y), int(valid.sum())))\n\n# --- Figure ---\nfig = plt.figure(figsize=(18, 14))\ngs = gridspec.GridSpec(2, 2, hspace=0.32, wspace=0.28)\n\n# ============================================================\n# Panel 1: Predicted vs Actual scatter with regression line\n# ============================================================\nax1 = fig.add_subplot(gs[0, 0])\n\nerrors = np.abs(preds_v - actual_v)\nscatter = ax1.scatter(actual_v, preds_v, c=errors, cmap=\"RdYlGn_r\", alpha=0.6, s=20, edgecolors=\"none\")\nplt.colorbar(scatter, ax=ax1, label=\"Absolute Error\", shrink=0.8)\n\n# Regression line\nif len(preds_v) > 2:\n    slope, intercept, r_val, p_val, _ = stats.linregress(actual_v, preds_v)\n    x_line = np.linspace(actual_v.min(), actual_v.max(), 100)\n    ax1.plot(x_line, slope * x_line + intercept, \"r--\", linewidth=2, label=\"Regression\")\n    # Perfect prediction line\n    ax1.plot(x_line, x_line, \"k:\", linewidth=1, alpha=0.5, label=\"Perfect\")\n\n    # IC (Spearman)\n    ic_val, _ = stats.spearmanr(actual_v, preds_v)\n    ax1.text(0.05, 0.95,\n             \"IC=%.3f  R2=%.3f\\nSlope=%.3f  n=%d\" % (ic_val, r_val ** 2, slope, len(preds_v)),\n             transform=ax1.transAxes, fontsize=9, verticalalignment=\"top\",\n             bbox=dict(boxstyle=\"round\", facecolor=\"wheat\", alpha=0.8))\n\nax1.set_xlabel(\"Actual %s\" % TARGET_COL, fontsize=10)\nax1.set_ylabel(\"Predicted\", fontsize=10)\nax1.set_title(\"Predicted vs Actual (%s)\" % TICKER, fontsize=13, fontweight=\"bold\")\nax1.legend(fontsize=8)\nax1.grid(alpha=0.3)\n\n# ============================================================\n# Panel 2: Cumulative PnL -- model signal vs buy-and-hold\n# ============================================================\nax2 = fig.add_subplot(gs[0, 1])\n\n# Signal returns: go long when prediction > 0, short otherwise\nsignal = np.where(preds_v > 0, 1.0, -1.0)\nsignal_returns = signal * actual_v\ncum_signal = np.cumsum(signal_returns)\ncum_bh = np.cumsum(actual_v)\n\nax2.plot(dates_v, cum_signal, color=\"#2196F3\", linewidth=2, label=\"Model Signal\")\nax2.plot(dates_v, cum_bh, color=\"#9E9E9E\", linewidth=1.5, linestyle=\"--\", label=\"Buy & Hold\")\n\n# Green/red fill for excess return\nexcess = cum_signal - cum_bh\nax2.fill_between(dates_v, cum_bh, cum_signal,\n                 where=(excess >= 0), color=\"#4CAF50\", alpha=0.15, label=\"Excess > 0\")\nax2.fill_between(dates_v, cum_bh, cum_signal,\n                 where=(excess < 0), color=\"#F44336\", alpha=0.15, label=\"Excess < 0\")\n\ntotal_signal = cum_signal[-1] if len(cum_signal) > 0 else 0\ntotal_bh = cum_bh[-1] if len(cum_bh) > 0 else 0\nax2.text(0.05, 0.95,\n         \"Signal: %.2f%%\\nBuy&Hold: %.2f%%\\nExcess: %.2f%%\" % (\n             total_signal * 100, total_bh * 100, (total_signal - total_bh) * 100),\n         transform=ax2.transAxes, fontsize=9, verticalalignment=\"top\",\n         bbox=dict(boxstyle=\"round\", facecolor=\"lightyellow\", alpha=0.8))\n\nax2.set_xlabel(\"Date\", fontsize=10)\nax2.set_ylabel(\"Cumulative Return\", fontsize=10)\nax2.set_title(\"Cumulative PnL (%s)\" % TICKER, fontsize=13, fontweight=\"bold\")\nax2.legend(fontsize=8, loc=\"lower right\")\nax2.grid(alpha=0.3)\nfig.autofmt_xdate()\n\n# ============================================================\n# Panel 3: Confidence bands (quantile predictions)\n# ============================================================\nax3 = fig.add_subplot(gs[1, 0])\n\nhas_quantiles = False\ntry:\n    q_preds = model.predict_quantiles(test_X, [0.10, 0.50, 0.90])\n    q10 = q_preds[0.10][valid]\n    q50 = q_preds[0.50][valid]\n    q90 = q_preds[0.90][valid]\n    has_quantiles = True\nexcept Exception as e:\n    print(\"Quantile prediction not available: %s\" % e)\n\nif has_quantiles:\n    ax3.fill_between(dates_v, q10, q90, alpha=0.2, color=\"#2196F3\", label=\"10th-90th percentile\")\n    ax3.plot(dates_v, q50, color=\"#2196F3\", linewidth=1.5, label=\"Median prediction\")\n    ax3.scatter(dates_v, actual_v, color=\"#F44336\", s=12, zorder=5, alpha=0.7, label=\"Actual\")\n\n    # Coverage: what fraction of actuals fall within the band?\n    inside = np.sum((actual_v >= q10) & (actual_v <= q90))\n    coverage = inside / len(actual_v) * 100 if len(actual_v) > 0 else 0\n    band_width = np.mean(q90 - q10)\n    ax3.text(0.05, 0.95,\n             \"Coverage: %.1f%% (target 80%%)\\nMean band width: %.4f\" % (coverage, band_width),\n             transform=ax3.transAxes, fontsize=9, verticalalignment=\"top\",\n             bbox=dict(boxstyle=\"round\", facecolor=\"lightcyan\", alpha=0.8))\n    ax3.legend(fontsize=8)\nelse:\n    ax3.text(0.5, 0.5, \"Quantile predictions\\nnot available for\\nthis model type\",\n             transform=ax3.transAxes, fontsize=14, ha=\"center\", va=\"center\", color=\"gray\")\n\nax3.set_xlabel(\"Date\", fontsize=10)\nax3.set_ylabel(\"Return\", fontsize=10)\nax3.set_title(\"Confidence Bands (%s)\" % TICKER, fontsize=13, fontweight=\"bold\")\nax3.grid(alpha=0.3)\n\n# ============================================================\n# Panel 4: Rolling hit ratio (63-day window)\n# ============================================================\nax4 = fig.add_subplot(gs[1, 1])\n\nwindow = min(63, max(10, len(preds_v) // 3))\nhits = (np.sign(preds_v) == np.sign(actual_v)).astype(float)\n\nif len(hits) >= window:\n    rolling_hit = np.convolve(hits, np.ones(window) / window, mode=\"valid\")\n    roll_dates = dates_v[window - 1:]\n\n    ax4.plot(roll_dates, rolling_hit, color=\"#2196F3\", linewidth=2, label=\"%d-day rolling\" % window)\n    ax4.axhline(y=0.5, color=\"gray\", linewidth=1, linestyle=\"--\", label=\"50% baseline\")\n\n    # Green/red fill around 50%\n    ax4.fill_between(roll_dates, 0.5, rolling_hit,\n                     where=(rolling_hit >= 0.5), color=\"#4CAF50\", alpha=0.2)\n    ax4.fill_between(roll_dates, 0.5, rolling_hit,\n                     where=(rolling_hit < 0.5), color=\"#F44336\", alpha=0.2)\n\n    avg_hit = np.mean(hits)\n    ax4.text(0.05, 0.95,\n             \"Overall hit ratio: %.1f%%\\nWindow: %d days\" % (avg_hit * 100, window),\n             transform=ax4.transAxes, fontsize=9, verticalalignment=\"top\",\n             bbox=dict(boxstyle=\"round\", facecolor=\"lightyellow\", alpha=0.8))\n    ax4.legend(fontsize=8)\nelse:\n    ax4.text(0.5, 0.5, \"Not enough data\\nfor rolling window\\n(%d < %d)\" % (len(hits), window),\n             transform=ax4.transAxes, fontsize=14, ha=\"center\", va=\"center\", color=\"gray\")\n\nax4.set_xlabel(\"Date\", fontsize=10)\nax4.set_ylabel(\"Hit Ratio\", fontsize=10)\nax4.set_title(\"Rolling Hit Ratio (%s)\" % TICKER, fontsize=13, fontweight=\"bold\")\nax4.set_ylim(0.2, 0.8)\nax4.grid(alpha=0.3)\n\nfig.suptitle(\"Prediction Analysis: %s (%s)\" % (TICKER, best_config.model_type),\n             fontsize=16, fontweight=\"bold\", y=1.01)\nplt.savefig(os.path.join(OUTPUT_DIR, \"prediction_%s.png\" % TICKER), dpi=150, bbox_inches=\"tight\")\nplt.show()\nprint(\"Prediction analysis saved to %s/prediction_%s.png\" % (OUTPUT_DIR, TICKER))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# === Model Scorecard ===\n# Text summary + visual 3x3 scorecard grid\n\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport numpy as np\n\n# --- Text Summary ---\nev = best_result.evaluation\nprint(\"=\" * 60)\nprint(\"MODEL SCORECARD\")\nprint(\"=\" * 60)\nprint(\"\")\nprint(\"Model Configuration:\")\nprint(\"  Type:          %s\" % best_config.model_type)\nprint(\"  Learning rate: %s\" % best_config.learning_rate)\nprint(\"  Dropout:       %s\" % best_config.dropout)\nprint(\"  Epochs:        %s\" % best_config.epochs)\nif best_config.extra_params:\n    for k, v in best_config.extra_params.items():\n        print(\"  %s: %s\" % (k, v))\n\nprint(\"\")\nprint(\"Walk-Forward Validation (%d folds):\" % len(ev.fold_results))\nprint(\"  Mean IC:       %.4f\" % ev.mean_ic)\nprint(\"  ICIR:          %.2f\" % ev.icir)\nprint(\"  Mean Sharpe:   %.2f\" % ev.mean_sharpe)\nprint(\"  Mean Max DD:   %.4f\" % ev.mean_mdd)\nprint(\"  Mean Calmar:   %.2f\" % ev.mean_calmar)\nprint(\"  Mean Hit Ratio:%.4f\" % ev.mean_hit_ratio)\nprint(\"  Overfit Ratio: %.2f\" % ev.overfit_ratio)\n\nprint(\"\")\nprint(\"Production Test Set:\")\nprint(\"  Test IC:       %.4f\" % test_metrics.ic)\nprint(\"  Test Hit Ratio:%.4f\" % test_metrics.hit_ratio)\nprint(\"  Test RMSE:     %.6f\" % test_metrics.rmse)\nprint(\"  Test MAE:      %.6f\" % test_metrics.mae)\nprint(\"  Test Samples:  %d\" % test_metrics.n_samples)\n\n# Uncertainty estimate\nhas_uncertainty = False\ntry:\n    sample_X = X[val_split:val_split + 100]\n    unc_mean, unc_var = model.predict_with_uncertainty(sample_X, n_mc_passes=20)\n    mean_uncertainty = float(np.mean(np.sqrt(unc_var)))\n    has_uncertainty = True\n    print(\"\")\n    print(\"Uncertainty Estimate (MC dropout, 100 samples):\")\n    print(\"  Mean StdDev:   %.6f\" % mean_uncertainty)\nexcept Exception as e:\n    print(\"\")\n    print(\"Uncertainty estimation not available: %s\" % e)\n\nprint(\"\")\nprint(\"=\" * 60)\n\n# --- Visual Scorecard: 3x3 grid ---\n# Define 9 metrics with thresholds (good/bad)\nscorecard_items = [\n    (\"IC\", ev.mean_ic, 0.03, True),           # good if > 0.03\n    (\"ICIR\", ev.icir, 0.5, True),              # good if > 0.5\n    (\"Sharpe\", ev.mean_sharpe, 0.5, True),     # good if > 0.5\n    (\"Hit Ratio\", ev.mean_hit_ratio, 0.52, True),  # good if > 52%\n    (\"Calmar\", ev.mean_calmar, 0.5, True),     # good if > 0.5\n    (\"Max DD\", ev.mean_mdd, 0.15, False),      # good if < 15%\n    (\"Overfit\", ev.overfit_ratio, 3.0, False),  # good if < 3.0\n    (\"Test IC\", test_metrics.ic, 0.02, True),  # good if > 0.02\n    (\"Test Hit\", test_metrics.hit_ratio, 0.50, True),  # good if > 50%\n]\n\nfig, axes = plt.subplots(3, 3, figsize=(12, 10))\nfig.suptitle(\"Model Scorecard: %s\" % best_config.model_type, fontsize=16, fontweight=\"bold\", y=1.02)\n\nfor idx, (name, value, threshold, higher_is_better) in enumerate(scorecard_items):\n    row = idx // 3\n    col = idx % 3\n    ax = axes[row][col]\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.axis(\"off\")\n\n    if higher_is_better:\n        is_good = value >= threshold\n    else:\n        is_good = value <= threshold\n\n    bg_color = \"#c6efce\" if is_good else \"#ffc7ce\"\n    indicator = \"PASS\" if is_good else \"WARN\"\n    ind_color = \"#006100\" if is_good else \"#9c0006\"\n\n    # Background box\n    rect = plt.Rectangle((0.05, 0.05), 0.9, 0.9, linewidth=2,\n                          edgecolor=ind_color, facecolor=bg_color, alpha=0.3)\n    ax.add_patch(rect)\n\n    # Metric name\n    ax.text(0.5, 0.78, name, ha=\"center\", va=\"center\", fontsize=13, fontweight=\"bold\")\n\n    # Value\n    if abs(value) >= 10:\n        fmt = \"%.1f\"\n    elif abs(value) >= 1:\n        fmt = \"%.2f\"\n    else:\n        fmt = \"%.4f\"\n    ax.text(0.5, 0.48, fmt % value, ha=\"center\", va=\"center\", fontsize=20,\n            fontweight=\"bold\", color=ind_color)\n\n    # Threshold reference\n    if higher_is_better:\n        ref_text = \"(> %s)\" % (fmt % threshold)\n    else:\n        ref_text = \"(< %s)\" % (fmt % threshold)\n    ax.text(0.5, 0.25, ref_text, ha=\"center\", va=\"center\", fontsize=9, color=\"gray\")\n\n    # Status badge\n    ax.text(0.5, 0.1, indicator, ha=\"center\", va=\"center\", fontsize=10,\n            fontweight=\"bold\", color=ind_color,\n            bbox=dict(boxstyle=\"round,pad=0.2\", facecolor=bg_color, edgecolor=ind_color, alpha=0.6))\n\nplt.tight_layout()\nplt.savefig(os.path.join(OUTPUT_DIR, \"scorecard.png\"), dpi=150, bbox_inches=\"tight\")\nplt.show()\nprint(\"Scorecard saved to %s/scorecard.png\" % OUTPUT_DIR)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 10. Serve API via ngrok (Connect to Frontend)\n\nStart the FastAPI backend on Colab and expose it via ngrok so your local Next.js frontend can call it.\n\n**Prerequisites:** You need a free [ngrok auth token](https://dashboard.ngrok.com/get-started/your-authtoken). Free tier gives 1 tunnel.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import subprocess\nimport time\nfrom pyngrok import ngrok\n\n# --- Configure ngrok ---\n# Get your free auth token at: https://dashboard.ngrok.com/get-started/your-authtoken\nNGROK_AUTH_TOKEN = \"\"  # <-- Paste your token here\n\nif NGROK_AUTH_TOKEN:\n    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\nelse:\n    print(\"WARNING: No ngrok auth token set. Tunnel may not work.\")\n    print(\"Get one at: https://dashboard.ngrok.com/get-started/your-authtoken\")\n\n# Make sure artifacts are in the repo dir\nLOCAL_ARTIFACTS = \"/content/AI-stock-investment-tool/artifacts\"\nassert os.path.exists(os.path.join(LOCAL_ARTIFACTS, \"model.pkl\")), \\\n    \"No model.pkl found! Run sections 9-10 first to train and export.\"\n\n# Install uvicorn if not present\n!pip install -q uvicorn fastapi pydantic python-multipart\n\n# Start FastAPI in background\nos.chdir(\"/content/AI-stock-investment-tool\")\nserver_proc = subprocess.Popen(\n    [\"python\", \"-m\", \"uvicorn\", \"api.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"],\n    stdout=subprocess.PIPE,\n    stderr=subprocess.PIPE,\n)\ntime.sleep(3)\n\n# Check if server started successfully\nif server_proc.poll() is not None:\n    print(\"ERROR: Server failed to start!\")\n    print(server_proc.stderr.read().decode())\nelse:\n    print(\"FastAPI server started (PID: %d)\" % server_proc.pid)\n\n    # Open ngrok tunnel\n    public_url = ngrok.connect(8000)\n    print(\"\\n\" + \"=\" * 60)\n    print(\"PUBLIC API URL: %s\" % public_url)\n    print(\"=\" * 60)\n    print(\"\\nAPI Docs:  %s/docs\" % public_url)\n    print(\"Health:    %s/api/v1/health\" % public_url)\n    print(\"Predict:   %s/api/v1/predict\" % public_url)\n    print(\"\\n--- To connect your local frontend ---\")\n    print(\"Option A (PowerShell): set env var before npm run dev:\")\n    print('  $env:NEXT_PUBLIC_API_URL=\"%s\"' % public_url)\n    print(\"  cd frontend; npm run dev\")\n    print(\"\\nOption B: Create frontend/.env.local with:\")\n    print(\"  NEXT_PUBLIC_API_URL=%s\" % public_url)\n    print(\"\\nKeep this cell running! The tunnel closes when the runtime stops.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Quick test: verify API is responding\nimport urllib.request\nimport json\n\ntest_url = \"http://localhost:8000/api/v1/health\"\ntry:\n    resp = urllib.request.urlopen(test_url, timeout=5)\n    data = json.loads(resp.read().decode())\n    print(\"Health check OK:\")\n    for k, v in data.items():\n        print(\"  %s: %s\" % (k, v))\nexcept Exception as e:\n    print(\"Health check failed: %s\" % e)\n    print(\"Check server logs:\")\n    if server_proc.poll() is not None:\n        print(server_proc.stderr.read().decode()[-500:])",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 11. Download Artifacts to Local Machine\n\nIf you prefer to run the API locally instead of via ngrok, download the trained artifacts.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Zip artifacts and download to your local machine\n!cd {ARTIFACT_DIR} && zip -r /content/artifacts.zip .\n\nprint(\"Artifact contents:\")\n!ls -lh {ARTIFACT_DIR}\n\nprint(\"\\nTotal zip size:\")\n!ls -lh /content/artifacts.zip\n\n# Download via browser\nfrom google.colab import files\nfiles.download(\"/content/artifacts.zip\")\n\nprint(\"\\nAfter downloading, on your local machine:\")\nprint(\"  1. Unzip into your project root:\")\nprint(\"     unzip artifacts.zip -d artifacts/\")\nprint(\"  2. Start the API:\")\nprint(\"     python -m api.main\")\nprint(\"  3. Start the frontend:\")\nprint(\"     cd frontend && npm run dev\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 12. Cleanup\n\nStop the API server and ngrok tunnel when done.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Stop server and tunnel\ntry:\n    ngrok.disconnect(public_url)\n    ngrok.kill()\n    print(\"ngrok tunnel closed\")\nexcept Exception:\n    pass\n\ntry:\n    server_proc.terminate()\n    server_proc.wait(timeout=5)\n    print(\"FastAPI server stopped\")\nexcept Exception:\n    pass",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}